{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "C0851113_200neurons.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "cWACPRL869I4"
      },
      "cell_type": "code",
      "source": [
        "!pip install gym >/dev/null"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Os6feRY6ec_"
      },
      "cell_type": "code",
      "source": [
        "!pip install JSAnimation >/dev/null"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wotUOa_e6edP"
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    HTML(anim.to_jshtml())"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R66_INeZ9nYX"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 2: Playing Pong"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ngMhg3fB9aA",
        "outputId": "a84e2fa4-f04b-4ab8-f09c-2527985c19dc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=c246fa52e22a63d42bd92b6ecc8b7b83845b6b40c5f02980c4435cc1c3eca197\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "MtT2GyK_6edc",
        "outputId": "8b51d8df-6b19-4890-e188-64f056e5abca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "env = gym.make('Pong-v0')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "oRE6WmXQJ1Z0",
        "outputId": "2ede98a0-d842-4a71-aedb-c4fbc63e4e89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "env.action_space"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "yl_9d4HFJ31W",
        "outputId": "749b2f95-5339-445a-ffd9-8b91dcd581f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "env.observation_space"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "trwRXI-h6eeI",
        "outputId": "54e5ff25-0497-48cc-9c02-550839ae8a6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# Run a demo of the environment\n",
        "observation = env.reset()\n",
        "cumulated_reward = 0\n",
        "\n",
        "frames = []\n",
        "for t in range(1000):\n",
        "#     print(observation)\n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    # very stupid agent, just makes a random action within the allowd action space\n",
        "    action = env.action_space.sample()\n",
        "#     print(\"Action: {}\".format(t+1))    \n",
        "    observation, reward, done, info = env.step(action)\n",
        "#     print(reward)\n",
        "    cumulated_reward += reward\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "        break\n",
        "print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "\n",
        "env.close()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -11.0\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "3zZTecVWLLes"
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(x): \n",
        "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
        "\n",
        "def prepro(I):\n",
        "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "  I = I[35:195] # crop\n",
        "  I = I[::2,::2,0] # downsample by factor of 2\n",
        "  I[I == 144] = 0 # erase background (background type 1)\n",
        "  I[I == 109] = 0 # erase background (background type 2)\n",
        "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  return I.astype(np.float).ravel()\n",
        "\n",
        "def policy_forward(x):\n",
        "  h = np.dot(model['W1'], x)\n",
        "  h[h<0] = 0 # ReLU nonlinearity\n",
        "  logp = np.dot(model['W2'], h)\n",
        "  p = sigmoid(logp)\n",
        "  return p, h # return probability of taking action 2, and hidden state\n",
        "\n",
        "def model_step(model, observation, prev_x):\n",
        "  # preprocess the observation, set input to network to be difference image\n",
        "  cur_x = prepro(observation)\n",
        "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "  prev_x = cur_x\n",
        "  \n",
        "  # forward the policy network and sample an action from the returned probability\n",
        "  aprob, _ = policy_forward(x)\n",
        "  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n",
        "  \n",
        "  return action, prev_x\n",
        "\n",
        "def play_game(env, model):\n",
        "  observation = env.reset()\n",
        "\n",
        "  frames = []\n",
        "  cumulated_reward = 0\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "\n",
        "  for t in range(1000):\n",
        "      frames.append(env.render(mode = 'rgb_array'))\n",
        "      action, prev_x = model_step(model, observation, prev_x)\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      cumulated_reward += reward\n",
        "      if done:\n",
        "          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "          break\n",
        "  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "  display_frames_as_gif(frames)\n",
        "  env.close()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6gWvZQ7AQLQt"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 3: Policy Gradient from Scratch"
      ]
    },
    {
      "metadata": {
        "id": "eqFm7hqcItWl"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# model initialization\n",
        "H = 200 # number of hidden layer neurons\n",
        "D = 80 * 80 # input dimensionality: 80x80 grid\n",
        "model = {}\n",
        "model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
        "model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
        "\n",
        "# import pickle\n",
        "# model = pickle.load(open('model.pkl', 'rb'))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TwjiwKisQM19"
      },
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 10 # every how many episodes to do a param update?\n",
        "# learning_rate = 1e-4\n",
        "learning_rate = 1e-4\n",
        " \n",
        "gamma = 0.99 # discount factor for reward\n",
        "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
        "  \n",
        "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
        "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
        "\n",
        "def discount_rewards(r):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "    running_add = running_add * gamma + r[t]\n",
        "    discounted_r[t] = running_add\n",
        "  return discounted_r\n",
        "\n",
        "def policy_backward(epx, eph, epdlogp):\n",
        "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
        "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
        "  dh = np.outer(epdlogp, model['W2'])\n",
        "  dh[eph <= 0] = 0 # backpro prelu\n",
        "  dW1 = np.dot(dh.T, epx)\n",
        "  return {'W1':dW1, 'W2':dW2}\n",
        "\n",
        "def train_model(env, model, total_episodes = 100):\n",
        "  hist = []\n",
        "  observation = env.reset()\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "  xs,hs,dlogps,drs = [],[],[],[]\n",
        "  running_reward = None\n",
        "  reward_sum = 0\n",
        "  episode_number = 0\n",
        "\n",
        "  while True:\n",
        "    # preprocess the observation, set input to network to be difference image\n",
        "    cur_x = prepro(observation)\n",
        "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "    prev_x = cur_x\n",
        "\n",
        "    # forward the policy network and sample an action from the returned probability\n",
        "    aprob, h = policy_forward(x)\n",
        "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
        "\n",
        "    # record various intermediates (needed later for backprop)\n",
        "    xs.append(x) # observation\n",
        "    hs.append(h) # hidden state\n",
        "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
        "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
        "\n",
        "    # step the environment and get new measurements\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    reward_sum += reward\n",
        "\n",
        "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
        "\n",
        "    if done: # an episode finished\n",
        "      episode_number += 1\n",
        "\n",
        "      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
        "      epx = np.vstack(xs)\n",
        "      eph = np.vstack(hs)\n",
        "      epdlogp = np.vstack(dlogps)\n",
        "      epr = np.vstack(drs)\n",
        "      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
        "\n",
        "      # compute the discounted reward backwards through time\n",
        "      discounted_epr = discount_rewards(epr)\n",
        "      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "      discounted_epr -= np.mean(discounted_epr)\n",
        "      discounted_epr /= np.std(discounted_epr)\n",
        "\n",
        "      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
        "      grad = policy_backward(epx, eph, epdlogp)\n",
        "      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
        "\n",
        "      # perform rmsprop parameter update every batch_size episodes\n",
        "      if episode_number % batch_size == 0:\n",
        "        for k,v in model.items():\n",
        "          g = grad_buffer[k] # gradient\n",
        "          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
        "          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
        "\n",
        "      # boring book-keeping\n",
        "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "      hist.append((episode_number, reward_sum, running_reward))\n",
        "      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
        "      reward_sum = 0\n",
        "      observation = env.reset() # reset env\n",
        "      prev_x = None\n",
        "      if episode_number == total_episodes: return hist\n",
        "\n",
        "      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
        "        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G6Ka_5Vl9Orm",
        "outputId": "41a48ef4-386e-46d4-e099-9a55eeb4d461",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist1 = train_model(env, model, total_episodes=500)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -20.010000\n",
            "resetting env. episode 3.000000, reward total was -20.000000. running mean: -20.009900\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.019801\n",
            "resetting env. episode 5.000000, reward total was -18.000000. running mean: -19.999603\n",
            "resetting env. episode 6.000000, reward total was -20.000000. running mean: -19.999607\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.009611\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.019515\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.029320\n",
            "resetting env. episode 10.000000, reward total was -20.000000. running mean: -20.029026\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.038736\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.048349\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.057865\n",
            "resetting env. episode 14.000000, reward total was -19.000000. running mean: -20.047287\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.056814\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.066246\n",
            "resetting env. episode 17.000000, reward total was -20.000000. running mean: -20.065583\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.074927\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -20.074178\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.083436\n",
            "resetting env. episode 21.000000, reward total was -19.000000. running mean: -20.072602\n",
            "resetting env. episode 22.000000, reward total was -20.000000. running mean: -20.071876\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.081157\n",
            "resetting env. episode 24.000000, reward total was -20.000000. running mean: -20.080346\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.089542\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.098647\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.107660\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.116584\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.125418\n",
            "resetting env. episode 30.000000, reward total was -18.000000. running mean: -20.104164\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.113122\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -20.121991\n",
            "resetting env. episode 33.000000, reward total was -20.000000. running mean: -20.120771\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.129563\n",
            "resetting env. episode 35.000000, reward total was -20.000000. running mean: -20.128268\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.136985\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.145615\n",
            "resetting env. episode 38.000000, reward total was -20.000000. running mean: -20.144159\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -20.152717\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.161190\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.169578\n",
            "resetting env. episode 42.000000, reward total was -20.000000. running mean: -20.167882\n",
            "resetting env. episode 43.000000, reward total was -20.000000. running mean: -20.166204\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.174542\n",
            "resetting env. episode 45.000000, reward total was -19.000000. running mean: -20.162796\n",
            "resetting env. episode 46.000000, reward total was -20.000000. running mean: -20.161168\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.169557\n",
            "resetting env. episode 48.000000, reward total was -20.000000. running mean: -20.167861\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -20.176182\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.184421\n",
            "resetting env. episode 51.000000, reward total was -19.000000. running mean: -20.172576\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.180851\n",
            "resetting env. episode 53.000000, reward total was -20.000000. running mean: -20.179042\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.187252\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.195379\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.203425\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.211391\n",
            "resetting env. episode 58.000000, reward total was -20.000000. running mean: -20.209277\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -20.217184\n",
            "resetting env. episode 60.000000, reward total was -20.000000. running mean: -20.215013\n",
            "resetting env. episode 61.000000, reward total was -19.000000. running mean: -20.202862\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.210834\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.218725\n",
            "resetting env. episode 64.000000, reward total was -20.000000. running mean: -20.216538\n",
            "resetting env. episode 65.000000, reward total was -20.000000. running mean: -20.214373\n",
            "resetting env. episode 66.000000, reward total was -20.000000. running mean: -20.212229\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.220107\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.227906\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.235627\n",
            "resetting env. episode 70.000000, reward total was -20.000000. running mean: -20.233270\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.240938\n",
            "resetting env. episode 72.000000, reward total was -20.000000. running mean: -20.238528\n",
            "resetting env. episode 73.000000, reward total was -20.000000. running mean: -20.236143\n",
            "resetting env. episode 74.000000, reward total was -19.000000. running mean: -20.223782\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.231544\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.239228\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.246836\n",
            "resetting env. episode 78.000000, reward total was -20.000000. running mean: -20.244368\n",
            "resetting env. episode 79.000000, reward total was -19.000000. running mean: -20.231924\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.239605\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.247209\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.254737\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.262189\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.269567\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.276872\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.284103\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.291262\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.298349\n",
            "resetting env. episode 89.000000, reward total was -20.000000. running mean: -20.295366\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.302412\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.309388\n",
            "resetting env. episode 92.000000, reward total was -20.000000. running mean: -20.306294\n",
            "resetting env. episode 93.000000, reward total was -20.000000. running mean: -20.303231\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.310199\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.317097\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.323926\n",
            "resetting env. episode 97.000000, reward total was -19.000000. running mean: -20.310687\n",
            "resetting env. episode 98.000000, reward total was -19.000000. running mean: -20.297580\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.304604\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.311558\n",
            "resetting env. episode 101.000000, reward total was -18.000000. running mean: -20.288442\n",
            "resetting env. episode 102.000000, reward total was -20.000000. running mean: -20.285558\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -20.292702\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.299775\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.306778\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -20.313710\n",
            "resetting env. episode 107.000000, reward total was -19.000000. running mean: -20.300573\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -20.297567\n",
            "resetting env. episode 109.000000, reward total was -20.000000. running mean: -20.294591\n",
            "resetting env. episode 110.000000, reward total was -19.000000. running mean: -20.281645\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.288829\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -20.295941\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -20.302981\n",
            "resetting env. episode 114.000000, reward total was -20.000000. running mean: -20.299951\n",
            "resetting env. episode 115.000000, reward total was -19.000000. running mean: -20.286952\n",
            "resetting env. episode 116.000000, reward total was -20.000000. running mean: -20.284082\n",
            "resetting env. episode 117.000000, reward total was -20.000000. running mean: -20.281242\n",
            "resetting env. episode 118.000000, reward total was -20.000000. running mean: -20.278429\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.285645\n",
            "resetting env. episode 120.000000, reward total was -20.000000. running mean: -20.282788\n",
            "resetting env. episode 121.000000, reward total was -20.000000. running mean: -20.279961\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.287161\n",
            "resetting env. episode 123.000000, reward total was -20.000000. running mean: -20.284289\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.291446\n",
            "resetting env. episode 125.000000, reward total was -19.000000. running mean: -20.278532\n",
            "resetting env. episode 126.000000, reward total was -20.000000. running mean: -20.275747\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.282989\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.290159\n",
            "resetting env. episode 129.000000, reward total was -19.000000. running mean: -20.277258\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.284485\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.291640\n",
            "resetting env. episode 132.000000, reward total was -20.000000. running mean: -20.288724\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.295837\n",
            "resetting env. episode 134.000000, reward total was -20.000000. running mean: -20.292878\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.299950\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.306950\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.313881\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.320742\n",
            "resetting env. episode 139.000000, reward total was -20.000000. running mean: -20.317534\n",
            "resetting env. episode 140.000000, reward total was -20.000000. running mean: -20.314359\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.321215\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.328003\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.334723\n",
            "resetting env. episode 144.000000, reward total was -20.000000. running mean: -20.331376\n",
            "resetting env. episode 145.000000, reward total was -20.000000. running mean: -20.328062\n",
            "resetting env. episode 146.000000, reward total was -20.000000. running mean: -20.324782\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -20.321534\n",
            "resetting env. episode 148.000000, reward total was -19.000000. running mean: -20.308318\n",
            "resetting env. episode 149.000000, reward total was -20.000000. running mean: -20.305235\n",
            "resetting env. episode 150.000000, reward total was -20.000000. running mean: -20.302183\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -20.299161\n",
            "resetting env. episode 152.000000, reward total was -20.000000. running mean: -20.296169\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.303208\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.310176\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.317074\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.323903\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.330664\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.337358\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.343984\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.350544\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.357039\n",
            "resetting env. episode 162.000000, reward total was -20.000000. running mean: -20.353468\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.359934\n",
            "resetting env. episode 164.000000, reward total was -20.000000. running mean: -20.356334\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.362771\n",
            "resetting env. episode 166.000000, reward total was -20.000000. running mean: -20.359143\n",
            "resetting env. episode 167.000000, reward total was -19.000000. running mean: -20.345552\n",
            "resetting env. episode 168.000000, reward total was -20.000000. running mean: -20.342096\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.348675\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.355189\n",
            "resetting env. episode 171.000000, reward total was -20.000000. running mean: -20.351637\n",
            "resetting env. episode 172.000000, reward total was -19.000000. running mean: -20.338120\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.344739\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.351292\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.357779\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.364201\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.370559\n",
            "resetting env. episode 178.000000, reward total was -20.000000. running mean: -20.366853\n",
            "resetting env. episode 179.000000, reward total was -20.000000. running mean: -20.363185\n",
            "resetting env. episode 180.000000, reward total was -20.000000. running mean: -20.359553\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.365957\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.372298\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.378575\n",
            "resetting env. episode 184.000000, reward total was -20.000000. running mean: -20.374789\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.381041\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.387231\n",
            "resetting env. episode 187.000000, reward total was -20.000000. running mean: -20.383359\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -20.379525\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.385730\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.391872\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.397954\n",
            "resetting env. episode 192.000000, reward total was -20.000000. running mean: -20.393974\n",
            "resetting env. episode 193.000000, reward total was -19.000000. running mean: -20.380034\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.386234\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.392372\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.398448\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.404464\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.410419\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.416315\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.422152\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.427930\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.433651\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.439314\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.444921\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.450472\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.455967\n",
            "resetting env. episode 207.000000, reward total was -20.000000. running mean: -20.451407\n",
            "resetting env. episode 208.000000, reward total was -19.000000. running mean: -20.436893\n",
            "resetting env. episode 209.000000, reward total was -20.000000. running mean: -20.432524\n",
            "resetting env. episode 210.000000, reward total was -20.000000. running mean: -20.428199\n",
            "resetting env. episode 211.000000, reward total was -20.000000. running mean: -20.423917\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.429678\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.435381\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.441027\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.446617\n",
            "resetting env. episode 216.000000, reward total was -19.000000. running mean: -20.432151\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.437830\n",
            "resetting env. episode 218.000000, reward total was -20.000000. running mean: -20.433451\n",
            "resetting env. episode 219.000000, reward total was -20.000000. running mean: -20.429117\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.434826\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.430477\n",
            "resetting env. episode 222.000000, reward total was -20.000000. running mean: -20.426173\n",
            "resetting env. episode 223.000000, reward total was -20.000000. running mean: -20.421911\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.427692\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.433415\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.439081\n",
            "resetting env. episode 227.000000, reward total was -20.000000. running mean: -20.434690\n",
            "resetting env. episode 228.000000, reward total was -18.000000. running mean: -20.410343\n",
            "resetting env. episode 229.000000, reward total was -20.000000. running mean: -20.406239\n",
            "resetting env. episode 230.000000, reward total was -20.000000. running mean: -20.402177\n",
            "resetting env. episode 231.000000, reward total was -20.000000. running mean: -20.398155\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.404174\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.410132\n",
            "resetting env. episode 234.000000, reward total was -20.000000. running mean: -20.406031\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.411970\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.417851\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.423672\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.429435\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.435141\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.440790\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.446382\n",
            "resetting env. episode 242.000000, reward total was -20.000000. running mean: -20.441918\n",
            "resetting env. episode 243.000000, reward total was -20.000000. running mean: -20.437499\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.443124\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -20.438693\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.444306\n",
            "resetting env. episode 247.000000, reward total was -20.000000. running mean: -20.439863\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.445464\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.451009\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.456499\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.461934\n",
            "resetting env. episode 252.000000, reward total was -19.000000. running mean: -20.447315\n",
            "resetting env. episode 253.000000, reward total was -20.000000. running mean: -20.442842\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.448413\n",
            "resetting env. episode 255.000000, reward total was -20.000000. running mean: -20.443929\n",
            "resetting env. episode 256.000000, reward total was -20.000000. running mean: -20.439490\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.445095\n",
            "resetting env. episode 258.000000, reward total was -19.000000. running mean: -20.430644\n",
            "resetting env. episode 259.000000, reward total was -20.000000. running mean: -20.426338\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.432074\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.437754\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.443376\n",
            "resetting env. episode 263.000000, reward total was -20.000000. running mean: -20.438942\n",
            "resetting env. episode 264.000000, reward total was -19.000000. running mean: -20.424553\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.430307\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.436004\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.441644\n",
            "resetting env. episode 268.000000, reward total was -19.000000. running mean: -20.427228\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.432955\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.438626\n",
            "resetting env. episode 271.000000, reward total was -18.000000. running mean: -20.414240\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.420097\n",
            "resetting env. episode 273.000000, reward total was -19.000000. running mean: -20.405896\n",
            "resetting env. episode 274.000000, reward total was -20.000000. running mean: -20.401837\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.407819\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.413741\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.419603\n",
            "resetting env. episode 278.000000, reward total was -20.000000. running mean: -20.415407\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.421253\n",
            "resetting env. episode 280.000000, reward total was -20.000000. running mean: -20.417041\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.422870\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.428642\n",
            "resetting env. episode 283.000000, reward total was -20.000000. running mean: -20.424355\n",
            "resetting env. episode 284.000000, reward total was -17.000000. running mean: -20.390112\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.396210\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.402248\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.408226\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.414144\n",
            "resetting env. episode 289.000000, reward total was -20.000000. running mean: -20.410002\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.415902\n",
            "resetting env. episode 291.000000, reward total was -19.000000. running mean: -20.401743\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.407726\n",
            "resetting env. episode 293.000000, reward total was -20.000000. running mean: -20.403648\n",
            "resetting env. episode 294.000000, reward total was -20.000000. running mean: -20.399612\n",
            "resetting env. episode 295.000000, reward total was -20.000000. running mean: -20.395616\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.401660\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.407643\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.403567\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.409531\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.415436\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.421281\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.427069\n",
            "resetting env. episode 303.000000, reward total was -20.000000. running mean: -20.422798\n",
            "resetting env. episode 304.000000, reward total was -20.000000. running mean: -20.418570\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.424384\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.430140\n",
            "resetting env. episode 307.000000, reward total was -20.000000. running mean: -20.425839\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.431581\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.437265\n",
            "resetting env. episode 310.000000, reward total was -19.000000. running mean: -20.422892\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.428663\n",
            "resetting env. episode 312.000000, reward total was -20.000000. running mean: -20.424377\n",
            "resetting env. episode 313.000000, reward total was -18.000000. running mean: -20.400133\n",
            "resetting env. episode 314.000000, reward total was -20.000000. running mean: -20.396131\n",
            "resetting env. episode 315.000000, reward total was -19.000000. running mean: -20.382170\n",
            "resetting env. episode 316.000000, reward total was -19.000000. running mean: -20.368348\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.374665\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.380918\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.387109\n",
            "resetting env. episode 320.000000, reward total was -19.000000. running mean: -20.373238\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.379506\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.385711\n",
            "resetting env. episode 323.000000, reward total was -19.000000. running mean: -20.371853\n",
            "resetting env. episode 324.000000, reward total was -20.000000. running mean: -20.368135\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.374454\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.380709\n",
            "resetting env. episode 327.000000, reward total was -20.000000. running mean: -20.376902\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.383133\n",
            "resetting env. episode 329.000000, reward total was -19.000000. running mean: -20.369302\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.375609\n",
            "resetting env. episode 331.000000, reward total was -20.000000. running mean: -20.371853\n",
            "resetting env. episode 332.000000, reward total was -20.000000. running mean: -20.368134\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.374453\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.380708\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.386901\n",
            "resetting env. episode 336.000000, reward total was -19.000000. running mean: -20.373032\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.379302\n",
            "resetting env. episode 338.000000, reward total was -19.000000. running mean: -20.365509\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.371854\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.378135\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.384354\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.390510\n",
            "resetting env. episode 343.000000, reward total was -20.000000. running mean: -20.386605\n",
            "resetting env. episode 344.000000, reward total was -20.000000. running mean: -20.382739\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.388912\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.395023\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.401072\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.407062\n",
            "resetting env. episode 349.000000, reward total was -20.000000. running mean: -20.402991\n",
            "resetting env. episode 350.000000, reward total was -20.000000. running mean: -20.398961\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.404971\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.410922\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.416813\n",
            "resetting env. episode 354.000000, reward total was -19.000000. running mean: -20.402644\n",
            "resetting env. episode 355.000000, reward total was -20.000000. running mean: -20.398618\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.404632\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.410585\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.416480\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.422315\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.428092\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.433811\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -20.429473\n",
            "resetting env. episode 363.000000, reward total was -20.000000. running mean: -20.425178\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -20.420926\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.426717\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.432450\n",
            "resetting env. episode 367.000000, reward total was -19.000000. running mean: -20.418125\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.423944\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.429704\n",
            "resetting env. episode 370.000000, reward total was -18.000000. running mean: -20.405407\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.411353\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.417240\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.423067\n",
            "resetting env. episode 374.000000, reward total was -20.000000. running mean: -20.418837\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.424648\n",
            "resetting env. episode 376.000000, reward total was -20.000000. running mean: -20.420402\n",
            "resetting env. episode 377.000000, reward total was -19.000000. running mean: -20.406198\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.412136\n",
            "resetting env. episode 379.000000, reward total was -19.000000. running mean: -20.398015\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.404034\n",
            "resetting env. episode 381.000000, reward total was -19.000000. running mean: -20.389994\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.396094\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.402133\n",
            "resetting env. episode 384.000000, reward total was -20.000000. running mean: -20.398112\n",
            "resetting env. episode 385.000000, reward total was -20.000000. running mean: -20.394131\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.400189\n",
            "resetting env. episode 387.000000, reward total was -20.000000. running mean: -20.396188\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.402226\n",
            "resetting env. episode 389.000000, reward total was -19.000000. running mean: -20.388203\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.394321\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.400378\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.406374\n",
            "resetting env. episode 393.000000, reward total was -18.000000. running mean: -20.382311\n",
            "resetting env. episode 394.000000, reward total was -20.000000. running mean: -20.378488\n",
            "resetting env. episode 395.000000, reward total was -20.000000. running mean: -20.374703\n",
            "resetting env. episode 396.000000, reward total was -19.000000. running mean: -20.360956\n",
            "resetting env. episode 397.000000, reward total was -20.000000. running mean: -20.357346\n",
            "resetting env. episode 398.000000, reward total was -19.000000. running mean: -20.343773\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.350335\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.356832\n",
            "resetting env. episode 401.000000, reward total was -20.000000. running mean: -20.353263\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.359731\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.366133\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.372472\n",
            "resetting env. episode 405.000000, reward total was -20.000000. running mean: -20.368747\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.375060\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.381309\n",
            "resetting env. episode 408.000000, reward total was -20.000000. running mean: -20.377496\n",
            "resetting env. episode 409.000000, reward total was -20.000000. running mean: -20.373721\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.379984\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.386184\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.392322\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.398399\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.404415\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.410371\n",
            "resetting env. episode 416.000000, reward total was -19.000000. running mean: -20.396267\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.402304\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.408281\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.414199\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.420057\n",
            "resetting env. episode 421.000000, reward total was -19.000000. running mean: -20.405856\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.411797\n",
            "resetting env. episode 423.000000, reward total was -20.000000. running mean: -20.407680\n",
            "resetting env. episode 424.000000, reward total was -20.000000. running mean: -20.403603\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.409567\n",
            "resetting env. episode 426.000000, reward total was -20.000000. running mean: -20.405471\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.411416\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.417302\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.423129\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.428898\n",
            "resetting env. episode 431.000000, reward total was -20.000000. running mean: -20.424609\n",
            "resetting env. episode 432.000000, reward total was -20.000000. running mean: -20.420363\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.426159\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.431898\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -20.427579\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.433303\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.438970\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.444580\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.450134\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.455633\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.461077\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.466466\n",
            "resetting env. episode 443.000000, reward total was -20.000000. running mean: -20.461801\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.467183\n",
            "resetting env. episode 445.000000, reward total was -20.000000. running mean: -20.462511\n",
            "resetting env. episode 446.000000, reward total was -20.000000. running mean: -20.457886\n",
            "resetting env. episode 447.000000, reward total was -20.000000. running mean: -20.453307\n",
            "resetting env. episode 448.000000, reward total was -19.000000. running mean: -20.438774\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.444387\n",
            "resetting env. episode 450.000000, reward total was -19.000000. running mean: -20.429943\n",
            "resetting env. episode 451.000000, reward total was -20.000000. running mean: -20.425643\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.431387\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.437073\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.442702\n",
            "resetting env. episode 455.000000, reward total was -20.000000. running mean: -20.438275\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.443892\n",
            "resetting env. episode 457.000000, reward total was -20.000000. running mean: -20.439454\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.445059\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -20.440608\n",
            "resetting env. episode 460.000000, reward total was -20.000000. running mean: -20.436202\n",
            "resetting env. episode 461.000000, reward total was -20.000000. running mean: -20.431840\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.437522\n",
            "resetting env. episode 463.000000, reward total was -20.000000. running mean: -20.433147\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.438815\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.444427\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.449983\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.455483\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.460928\n",
            "resetting env. episode 469.000000, reward total was -20.000000. running mean: -20.456319\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.461756\n",
            "resetting env. episode 471.000000, reward total was -17.000000. running mean: -20.427138\n",
            "resetting env. episode 472.000000, reward total was -20.000000. running mean: -20.422867\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -20.418638\n",
            "resetting env. episode 474.000000, reward total was -19.000000. running mean: -20.404452\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.410407\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.416303\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.422140\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.417919\n",
            "resetting env. episode 479.000000, reward total was -19.000000. running mean: -20.403739\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.409702\n",
            "resetting env. episode 481.000000, reward total was -20.000000. running mean: -20.405605\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.411549\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.417434\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.423259\n",
            "resetting env. episode 485.000000, reward total was -18.000000. running mean: -20.399027\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.405036\n",
            "resetting env. episode 487.000000, reward total was -20.000000. running mean: -20.400986\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.406976\n",
            "resetting env. episode 489.000000, reward total was -19.000000. running mean: -20.392906\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.398977\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.404987\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.410938\n",
            "resetting env. episode 493.000000, reward total was -20.000000. running mean: -20.406828\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.412760\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.418632\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.424446\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.430202\n",
            "resetting env. episode 498.000000, reward total was -20.000000. running mean: -20.425900\n",
            "resetting env. episode 499.000000, reward total was -20.000000. running mean: -20.421641\n",
            "resetting env. episode 500.000000, reward total was -20.000000. running mean: -20.417424\n",
            "CPU times: user 23min 14s, sys: 10min 38s, total: 33min 53s\n",
            "Wall time: 17min 34s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "cHYCDYwhlVLV",
        "outputId": "e1be83ac-3740-4a70-c50b-2705776588b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist2 = train_model(env, model, total_episodes=500)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -19.000000. running mean: -19.000000\n",
            "resetting env. episode 2.000000, reward total was -18.000000. running mean: -18.990000\n",
            "resetting env. episode 3.000000, reward total was -20.000000. running mean: -19.000100\n",
            "resetting env. episode 4.000000, reward total was -19.000000. running mean: -19.000099\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -19.020098\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -19.039897\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -19.059498\n",
            "resetting env. episode 8.000000, reward total was -20.000000. running mean: -19.068903\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -19.088214\n",
            "resetting env. episode 10.000000, reward total was -19.000000. running mean: -19.087332\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -19.106459\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -19.125394\n",
            "resetting env. episode 13.000000, reward total was -20.000000. running mean: -19.134140\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -19.152799\n",
            "resetting env. episode 15.000000, reward total was -20.000000. running mean: -19.161271\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -19.179658\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -19.197861\n",
            "resetting env. episode 18.000000, reward total was -20.000000. running mean: -19.205883\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -19.223824\n",
            "resetting env. episode 20.000000, reward total was -19.000000. running mean: -19.221586\n",
            "resetting env. episode 21.000000, reward total was -20.000000. running mean: -19.229370\n",
            "resetting env. episode 22.000000, reward total was -17.000000. running mean: -19.207076\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -19.225005\n",
            "resetting env. episode 24.000000, reward total was -20.000000. running mean: -19.232755\n",
            "resetting env. episode 25.000000, reward total was -20.000000. running mean: -19.240428\n",
            "resetting env. episode 26.000000, reward total was -20.000000. running mean: -19.248024\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -19.265543\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -19.282888\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -19.300059\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -19.317058\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -19.333888\n",
            "resetting env. episode 32.000000, reward total was -20.000000. running mean: -19.340549\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -19.357143\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -19.373572\n",
            "resetting env. episode 35.000000, reward total was -19.000000. running mean: -19.369836\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -19.386138\n",
            "resetting env. episode 37.000000, reward total was -20.000000. running mean: -19.392277\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -19.408354\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -19.424270\n",
            "resetting env. episode 40.000000, reward total was -20.000000. running mean: -19.430028\n",
            "resetting env. episode 41.000000, reward total was -20.000000. running mean: -19.435727\n",
            "resetting env. episode 42.000000, reward total was -19.000000. running mean: -19.431370\n",
            "resetting env. episode 43.000000, reward total was -20.000000. running mean: -19.437056\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -19.452686\n",
            "resetting env. episode 45.000000, reward total was -20.000000. running mean: -19.458159\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -19.473577\n",
            "resetting env. episode 47.000000, reward total was -20.000000. running mean: -19.478841\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -19.494053\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -19.509113\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -19.524021\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -19.538781\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -19.553393\n",
            "resetting env. episode 53.000000, reward total was -19.000000. running mean: -19.547859\n",
            "resetting env. episode 54.000000, reward total was -20.000000. running mean: -19.552381\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -19.566857\n",
            "resetting env. episode 56.000000, reward total was -20.000000. running mean: -19.571188\n",
            "resetting env. episode 57.000000, reward total was -20.000000. running mean: -19.575477\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -19.589722\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -19.603825\n",
            "resetting env. episode 60.000000, reward total was -20.000000. running mean: -19.607786\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -19.621709\n",
            "resetting env. episode 62.000000, reward total was -20.000000. running mean: -19.625491\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -19.639237\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -19.652844\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -19.666316\n",
            "resetting env. episode 66.000000, reward total was -19.000000. running mean: -19.659653\n",
            "resetting env. episode 67.000000, reward total was -19.000000. running mean: -19.653056\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -19.666525\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -19.679860\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -19.693062\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -19.706131\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -19.719070\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -19.731879\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -19.744560\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -19.757115\n",
            "resetting env. episode 76.000000, reward total was -20.000000. running mean: -19.759543\n",
            "resetting env. episode 77.000000, reward total was -20.000000. running mean: -19.761948\n",
            "resetting env. episode 78.000000, reward total was -20.000000. running mean: -19.764329\n",
            "resetting env. episode 79.000000, reward total was -19.000000. running mean: -19.756685\n",
            "resetting env. episode 80.000000, reward total was -20.000000. running mean: -19.759118\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -19.771527\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -19.783812\n",
            "resetting env. episode 83.000000, reward total was -20.000000. running mean: -19.785974\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -19.798114\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -19.810133\n",
            "resetting env. episode 86.000000, reward total was -17.000000. running mean: -19.782032\n",
            "resetting env. episode 87.000000, reward total was -20.000000. running mean: -19.784211\n",
            "resetting env. episode 88.000000, reward total was -20.000000. running mean: -19.786369\n",
            "resetting env. episode 89.000000, reward total was -20.000000. running mean: -19.788505\n",
            "resetting env. episode 90.000000, reward total was -17.000000. running mean: -19.760620\n",
            "resetting env. episode 91.000000, reward total was -20.000000. running mean: -19.763014\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -19.775384\n",
            "resetting env. episode 93.000000, reward total was -20.000000. running mean: -19.777630\n",
            "resetting env. episode 94.000000, reward total was -20.000000. running mean: -19.779854\n",
            "resetting env. episode 95.000000, reward total was -18.000000. running mean: -19.762055\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -19.774435\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -19.786690\n",
            "resetting env. episode 98.000000, reward total was -19.000000. running mean: -19.778824\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -19.791035\n",
            "resetting env. episode 100.000000, reward total was -18.000000. running mean: -19.773125\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -19.785394\n",
            "resetting env. episode 102.000000, reward total was -19.000000. running mean: -19.777540\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -19.789764\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -19.801867\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -19.813848\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -19.825710\n",
            "resetting env. episode 107.000000, reward total was -20.000000. running mean: -19.827453\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -19.839178\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -19.850786\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -19.862278\n",
            "resetting env. episode 111.000000, reward total was -20.000000. running mean: -19.863656\n",
            "resetting env. episode 112.000000, reward total was -20.000000. running mean: -19.865019\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -19.876369\n",
            "resetting env. episode 114.000000, reward total was -20.000000. running mean: -19.877605\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -19.888829\n",
            "resetting env. episode 116.000000, reward total was -20.000000. running mean: -19.889941\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -19.901041\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -19.912031\n",
            "resetting env. episode 119.000000, reward total was -20.000000. running mean: -19.912911\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -19.923782\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -19.934544\n",
            "resetting env. episode 122.000000, reward total was -19.000000. running mean: -19.925198\n",
            "resetting env. episode 123.000000, reward total was -19.000000. running mean: -19.915946\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -19.926787\n",
            "resetting env. episode 125.000000, reward total was -20.000000. running mean: -19.927519\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -19.938244\n",
            "resetting env. episode 127.000000, reward total was -19.000000. running mean: -19.928861\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -19.939573\n",
            "resetting env. episode 129.000000, reward total was -19.000000. running mean: -19.930177\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -19.940875\n",
            "resetting env. episode 131.000000, reward total was -20.000000. running mean: -19.941467\n",
            "resetting env. episode 132.000000, reward total was -20.000000. running mean: -19.942052\n",
            "resetting env. episode 133.000000, reward total was -20.000000. running mean: -19.942631\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -19.953205\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -19.963673\n",
            "resetting env. episode 136.000000, reward total was -19.000000. running mean: -19.954036\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -19.964496\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -19.974851\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -19.985102\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -19.995251\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.005299\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.015246\n",
            "resetting env. episode 143.000000, reward total was -20.000000. running mean: -20.015093\n",
            "resetting env. episode 144.000000, reward total was -20.000000. running mean: -20.014942\n",
            "resetting env. episode 145.000000, reward total was -20.000000. running mean: -20.014793\n",
            "resetting env. episode 146.000000, reward total was -19.000000. running mean: -20.004645\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.014599\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.024453\n",
            "resetting env. episode 149.000000, reward total was -20.000000. running mean: -20.024208\n",
            "resetting env. episode 150.000000, reward total was -20.000000. running mean: -20.023966\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -20.023726\n",
            "resetting env. episode 152.000000, reward total was -20.000000. running mean: -20.023489\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.033254\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.042922\n",
            "resetting env. episode 155.000000, reward total was -20.000000. running mean: -20.042492\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.052068\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.061547\n",
            "resetting env. episode 158.000000, reward total was -19.000000. running mean: -20.050931\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.060422\n",
            "resetting env. episode 160.000000, reward total was -20.000000. running mean: -20.059818\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.069220\n",
            "resetting env. episode 162.000000, reward total was -20.000000. running mean: -20.068528\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.077842\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.087064\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.096193\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.105231\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.114179\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.123037\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.131807\n",
            "resetting env. episode 170.000000, reward total was -20.000000. running mean: -20.130489\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.139184\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.147792\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.156314\n",
            "resetting env. episode 174.000000, reward total was -20.000000. running mean: -20.154751\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.163203\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.171571\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.179856\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.188057\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.196177\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.204215\n",
            "resetting env. episode 181.000000, reward total was -20.000000. running mean: -20.202173\n",
            "resetting env. episode 182.000000, reward total was -19.000000. running mean: -20.190151\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.198249\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.206267\n",
            "resetting env. episode 185.000000, reward total was -20.000000. running mean: -20.204204\n",
            "resetting env. episode 186.000000, reward total was -20.000000. running mean: -20.202162\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.210141\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.218039\n",
            "resetting env. episode 189.000000, reward total was -20.000000. running mean: -20.215859\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.223700\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.231463\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.239149\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.246757\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.254289\n",
            "resetting env. episode 195.000000, reward total was -19.000000. running mean: -20.241747\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.249329\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.256836\n",
            "resetting env. episode 198.000000, reward total was -19.000000. running mean: -20.244267\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.251825\n",
            "resetting env. episode 200.000000, reward total was -20.000000. running mean: -20.249307\n",
            "resetting env. episode 201.000000, reward total was -20.000000. running mean: -20.246813\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.254345\n",
            "resetting env. episode 203.000000, reward total was -18.000000. running mean: -20.231802\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.239484\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.247089\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.254618\n",
            "resetting env. episode 207.000000, reward total was -20.000000. running mean: -20.252072\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.259551\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.266956\n",
            "resetting env. episode 210.000000, reward total was -18.000000. running mean: -20.244286\n",
            "resetting env. episode 211.000000, reward total was -20.000000. running mean: -20.241843\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.249425\n",
            "resetting env. episode 213.000000, reward total was -20.000000. running mean: -20.246931\n",
            "resetting env. episode 214.000000, reward total was -19.000000. running mean: -20.234461\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.242117\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.249696\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.257199\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.264627\n",
            "resetting env. episode 219.000000, reward total was -20.000000. running mean: -20.261980\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.269361\n",
            "resetting env. episode 221.000000, reward total was -19.000000. running mean: -20.256667\n",
            "resetting env. episode 222.000000, reward total was -19.000000. running mean: -20.244100\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.251659\n",
            "resetting env. episode 224.000000, reward total was -18.000000. running mean: -20.229143\n",
            "resetting env. episode 225.000000, reward total was -18.000000. running mean: -20.206851\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.214783\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.222635\n",
            "resetting env. episode 228.000000, reward total was -20.000000. running mean: -20.220409\n",
            "resetting env. episode 229.000000, reward total was -20.000000. running mean: -20.218204\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.226022\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.233762\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.241425\n",
            "resetting env. episode 233.000000, reward total was -19.000000. running mean: -20.229010\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.236720\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.244353\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.251909\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.259390\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.266796\n",
            "resetting env. episode 239.000000, reward total was -20.000000. running mean: -20.264129\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.271487\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.278772\n",
            "resetting env. episode 242.000000, reward total was -20.000000. running mean: -20.275985\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.283225\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.290393\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.297489\n",
            "resetting env. episode 246.000000, reward total was -20.000000. running mean: -20.294514\n",
            "resetting env. episode 247.000000, reward total was -19.000000. running mean: -20.281569\n",
            "resetting env. episode 248.000000, reward total was -20.000000. running mean: -20.278753\n",
            "resetting env. episode 249.000000, reward total was -19.000000. running mean: -20.265965\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.273306\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.280573\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.287767\n",
            "resetting env. episode 253.000000, reward total was -20.000000. running mean: -20.284889\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.292040\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.299120\n",
            "resetting env. episode 256.000000, reward total was -20.000000. running mean: -20.296129\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.303167\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.310136\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.317034\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.323864\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.330625\n",
            "resetting env. episode 262.000000, reward total was -20.000000. running mean: -20.327319\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.334046\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.340706\n",
            "resetting env. episode 265.000000, reward total was -20.000000. running mean: -20.337299\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.343926\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.350486\n",
            "resetting env. episode 268.000000, reward total was -20.000000. running mean: -20.346981\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.353512\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.359976\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.366377\n",
            "resetting env. episode 272.000000, reward total was -19.000000. running mean: -20.352713\n",
            "resetting env. episode 273.000000, reward total was -20.000000. running mean: -20.349186\n",
            "resetting env. episode 274.000000, reward total was -19.000000. running mean: -20.335694\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.342337\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.348914\n",
            "resetting env. episode 277.000000, reward total was -19.000000. running mean: -20.335425\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.342070\n",
            "resetting env. episode 279.000000, reward total was -20.000000. running mean: -20.338650\n",
            "resetting env. episode 280.000000, reward total was -18.000000. running mean: -20.315263\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.322110\n",
            "resetting env. episode 282.000000, reward total was -20.000000. running mean: -20.318889\n",
            "resetting env. episode 283.000000, reward total was -20.000000. running mean: -20.315700\n",
            "resetting env. episode 284.000000, reward total was -19.000000. running mean: -20.302543\n",
            "resetting env. episode 285.000000, reward total was -20.000000. running mean: -20.299518\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.306523\n",
            "resetting env. episode 287.000000, reward total was -20.000000. running mean: -20.303458\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.310423\n",
            "resetting env. episode 289.000000, reward total was -19.000000. running mean: -20.297319\n",
            "resetting env. episode 290.000000, reward total was -20.000000. running mean: -20.294346\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.301402\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.308388\n",
            "resetting env. episode 293.000000, reward total was -20.000000. running mean: -20.305304\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.312251\n",
            "resetting env. episode 295.000000, reward total was -20.000000. running mean: -20.309129\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.316037\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.322877\n",
            "resetting env. episode 298.000000, reward total was -18.000000. running mean: -20.299648\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.306652\n",
            "resetting env. episode 300.000000, reward total was -19.000000. running mean: -20.293585\n",
            "resetting env. episode 301.000000, reward total was -18.000000. running mean: -20.270649\n",
            "resetting env. episode 302.000000, reward total was -20.000000. running mean: -20.267943\n",
            "resetting env. episode 303.000000, reward total was -20.000000. running mean: -20.265263\n",
            "resetting env. episode 304.000000, reward total was -19.000000. running mean: -20.252611\n",
            "resetting env. episode 305.000000, reward total was -19.000000. running mean: -20.240085\n",
            "resetting env. episode 306.000000, reward total was -20.000000. running mean: -20.237684\n",
            "resetting env. episode 307.000000, reward total was -20.000000. running mean: -20.235307\n",
            "resetting env. episode 308.000000, reward total was -20.000000. running mean: -20.232954\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.240624\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.248218\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.255736\n",
            "resetting env. episode 312.000000, reward total was -20.000000. running mean: -20.253179\n",
            "resetting env. episode 313.000000, reward total was -20.000000. running mean: -20.250647\n",
            "resetting env. episode 314.000000, reward total was -20.000000. running mean: -20.248140\n",
            "resetting env. episode 315.000000, reward total was -18.000000. running mean: -20.225659\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.233402\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.241068\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.248658\n",
            "resetting env. episode 319.000000, reward total was -20.000000. running mean: -20.246171\n",
            "resetting env. episode 320.000000, reward total was -19.000000. running mean: -20.233709\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.241372\n",
            "resetting env. episode 322.000000, reward total was -20.000000. running mean: -20.238959\n",
            "resetting env. episode 323.000000, reward total was -20.000000. running mean: -20.236569\n",
            "resetting env. episode 324.000000, reward total was -20.000000. running mean: -20.234203\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.241861\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.249443\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.256948\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -20.254379\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.261835\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.269217\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.276524\n",
            "resetting env. episode 332.000000, reward total was -20.000000. running mean: -20.273759\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.281022\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.288211\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.295329\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.302376\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.309352\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.316259\n",
            "resetting env. episode 339.000000, reward total was -20.000000. running mean: -20.313096\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.319965\n",
            "resetting env. episode 341.000000, reward total was -20.000000. running mean: -20.316766\n",
            "resetting env. episode 342.000000, reward total was -20.000000. running mean: -20.313598\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.320462\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.327257\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.333985\n",
            "resetting env. episode 346.000000, reward total was -20.000000. running mean: -20.330645\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.337338\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.343965\n",
            "resetting env. episode 349.000000, reward total was -20.000000. running mean: -20.340525\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.347120\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.353649\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.360112\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -20.356511\n",
            "resetting env. episode 354.000000, reward total was -20.000000. running mean: -20.352946\n",
            "resetting env. episode 355.000000, reward total was -20.000000. running mean: -20.349417\n",
            "resetting env. episode 356.000000, reward total was -20.000000. running mean: -20.345923\n",
            "resetting env. episode 357.000000, reward total was -20.000000. running mean: -20.342463\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.349039\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.355548\n",
            "resetting env. episode 360.000000, reward total was -19.000000. running mean: -20.341993\n",
            "resetting env. episode 361.000000, reward total was -20.000000. running mean: -20.338573\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -20.335187\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.341835\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.348417\n",
            "resetting env. episode 365.000000, reward total was -19.000000. running mean: -20.334933\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.341583\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.348168\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.354686\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.361139\n",
            "resetting env. episode 370.000000, reward total was -19.000000. running mean: -20.347528\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.354052\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.360512\n",
            "resetting env. episode 373.000000, reward total was -20.000000. running mean: -20.356907\n",
            "resetting env. episode 374.000000, reward total was -20.000000. running mean: -20.353338\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -20.349804\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.356306\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.362743\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.369116\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.375425\n",
            "resetting env. episode 380.000000, reward total was -19.000000. running mean: -20.361670\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.368054\n",
            "resetting env. episode 382.000000, reward total was -20.000000. running mean: -20.364373\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.370729\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.377022\n",
            "resetting env. episode 385.000000, reward total was -20.000000. running mean: -20.373252\n",
            "resetting env. episode 386.000000, reward total was -20.000000. running mean: -20.369519\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.375824\n",
            "resetting env. episode 388.000000, reward total was -20.000000. running mean: -20.372066\n",
            "resetting env. episode 389.000000, reward total was -20.000000. running mean: -20.368345\n",
            "resetting env. episode 390.000000, reward total was -19.000000. running mean: -20.354662\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.361115\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.367504\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.373829\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.380091\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.386290\n",
            "resetting env. episode 396.000000, reward total was -20.000000. running mean: -20.382427\n",
            "resetting env. episode 397.000000, reward total was -19.000000. running mean: -20.368603\n",
            "resetting env. episode 398.000000, reward total was -20.000000. running mean: -20.364917\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.371267\n",
            "resetting env. episode 400.000000, reward total was -20.000000. running mean: -20.367555\n",
            "resetting env. episode 401.000000, reward total was -20.000000. running mean: -20.363879\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.370240\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.376538\n",
            "resetting env. episode 404.000000, reward total was -20.000000. running mean: -20.372773\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.379045\n",
            "resetting env. episode 406.000000, reward total was -20.000000. running mean: -20.375254\n",
            "resetting env. episode 407.000000, reward total was -20.000000. running mean: -20.371502\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.377787\n",
            "resetting env. episode 409.000000, reward total was -20.000000. running mean: -20.374009\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.380269\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.386466\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -20.382602\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.388776\n",
            "resetting env. episode 414.000000, reward total was -20.000000. running mean: -20.384888\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.391039\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.397129\n",
            "resetting env. episode 417.000000, reward total was -20.000000. running mean: -20.393157\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.399226\n",
            "resetting env. episode 419.000000, reward total was -20.000000. running mean: -20.395233\n",
            "resetting env. episode 420.000000, reward total was -20.000000. running mean: -20.391281\n",
            "resetting env. episode 421.000000, reward total was -20.000000. running mean: -20.387368\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.393495\n",
            "resetting env. episode 423.000000, reward total was -20.000000. running mean: -20.389560\n",
            "resetting env. episode 424.000000, reward total was -19.000000. running mean: -20.375664\n",
            "resetting env. episode 425.000000, reward total was -18.000000. running mean: -20.351907\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.358388\n",
            "resetting env. episode 427.000000, reward total was -20.000000. running mean: -20.354804\n",
            "resetting env. episode 428.000000, reward total was -18.000000. running mean: -20.331256\n",
            "resetting env. episode 429.000000, reward total was -20.000000. running mean: -20.327944\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.334664\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.341318\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.347905\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.354426\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.360881\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.367272\n",
            "resetting env. episode 436.000000, reward total was -20.000000. running mean: -20.363600\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.369964\n",
            "resetting env. episode 438.000000, reward total was -20.000000. running mean: -20.366264\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.372601\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.378875\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.385087\n",
            "resetting env. episode 442.000000, reward total was -19.000000. running mean: -20.371236\n",
            "resetting env. episode 443.000000, reward total was -19.000000. running mean: -20.357523\n",
            "resetting env. episode 444.000000, reward total was -20.000000. running mean: -20.353948\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.360409\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.366805\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.373137\n",
            "resetting env. episode 448.000000, reward total was -20.000000. running mean: -20.369405\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.375711\n",
            "resetting env. episode 450.000000, reward total was -20.000000. running mean: -20.371954\n",
            "resetting env. episode 451.000000, reward total was -19.000000. running mean: -20.358235\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.364652\n",
            "resetting env. episode 453.000000, reward total was -19.000000. running mean: -20.351006\n",
            "resetting env. episode 454.000000, reward total was -19.000000. running mean: -20.337496\n",
            "resetting env. episode 455.000000, reward total was -20.000000. running mean: -20.334121\n",
            "resetting env. episode 456.000000, reward total was -19.000000. running mean: -20.320779\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.327572\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.334296\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.340953\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.347543\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.354068\n",
            "resetting env. episode 462.000000, reward total was -18.000000. running mean: -20.330527\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.337222\n",
            "resetting env. episode 464.000000, reward total was -20.000000. running mean: -20.333850\n",
            "resetting env. episode 465.000000, reward total was -20.000000. running mean: -20.330511\n",
            "resetting env. episode 466.000000, reward total was -20.000000. running mean: -20.327206\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.333934\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.340595\n",
            "resetting env. episode 469.000000, reward total was -20.000000. running mean: -20.337189\n",
            "resetting env. episode 470.000000, reward total was -20.000000. running mean: -20.333817\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.340479\n",
            "resetting env. episode 472.000000, reward total was -20.000000. running mean: -20.337074\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.343703\n",
            "resetting env. episode 474.000000, reward total was -20.000000. running mean: -20.340266\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.346864\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.353395\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.359861\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.356262\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.362700\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.369073\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.375382\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.381628\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.387812\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.393934\n",
            "resetting env. episode 485.000000, reward total was -20.000000. running mean: -20.389995\n",
            "resetting env. episode 486.000000, reward total was -20.000000. running mean: -20.386095\n",
            "resetting env. episode 487.000000, reward total was -20.000000. running mean: -20.382234\n",
            "resetting env. episode 488.000000, reward total was -20.000000. running mean: -20.378411\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.384627\n",
            "resetting env. episode 490.000000, reward total was -20.000000. running mean: -20.380781\n",
            "resetting env. episode 491.000000, reward total was -18.000000. running mean: -20.356973\n",
            "resetting env. episode 492.000000, reward total was -20.000000. running mean: -20.353403\n",
            "resetting env. episode 493.000000, reward total was -19.000000. running mean: -20.339869\n",
            "resetting env. episode 494.000000, reward total was -19.000000. running mean: -20.326471\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.333206\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.339874\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.346475\n",
            "resetting env. episode 498.000000, reward total was -20.000000. running mean: -20.343010\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.349580\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.356084\n",
            "CPU times: user 23min 41s, sys: 10min 51s, total: 34min 33s\n",
            "Wall time: 17min 49s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "8fheN9DRlWXQ",
        "outputId": "5acd5e10-14d0-4835-b102-ada7c3f3d326",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "cell_type": "code",
      "source": [
        "play_game(env, model)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -17.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "9AxOcQhIsKow",
        "outputId": "045c66e2-ad31-4485-fa8e-fc5202eb33a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist3 = train_model(env, model, total_episodes=1500)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -19.000000. running mean: -19.000000\n",
            "resetting env. episode 2.000000, reward total was -19.000000. running mean: -19.000000\n",
            "resetting env. episode 3.000000, reward total was -20.000000. running mean: -19.010000\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -19.029900\n",
            "resetting env. episode 5.000000, reward total was -20.000000. running mean: -19.039601\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -19.059205\n",
            "resetting env. episode 7.000000, reward total was -17.000000. running mean: -19.038613\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -19.058227\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -19.077645\n",
            "resetting env. episode 10.000000, reward total was -20.000000. running mean: -19.086868\n",
            "resetting env. episode 11.000000, reward total was -19.000000. running mean: -19.085999\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -19.105139\n",
            "resetting env. episode 13.000000, reward total was -20.000000. running mean: -19.114088\n",
            "resetting env. episode 14.000000, reward total was -20.000000. running mean: -19.122947\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -19.141718\n",
            "resetting env. episode 16.000000, reward total was -20.000000. running mean: -19.150300\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -19.168797\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -19.187110\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -19.195238\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -19.213286\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -19.231153\n",
            "resetting env. episode 22.000000, reward total was -19.000000. running mean: -19.228842\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -19.246553\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -19.264088\n",
            "resetting env. episode 25.000000, reward total was -19.000000. running mean: -19.261447\n",
            "resetting env. episode 26.000000, reward total was -20.000000. running mean: -19.268832\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -19.286144\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -19.303283\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -19.320250\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -19.337047\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -19.343677\n",
            "resetting env. episode 32.000000, reward total was -20.000000. running mean: -19.350240\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -19.366738\n",
            "resetting env. episode 34.000000, reward total was -16.000000. running mean: -19.333070\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -19.349740\n",
            "resetting env. episode 36.000000, reward total was -19.000000. running mean: -19.346242\n",
            "resetting env. episode 37.000000, reward total was -18.000000. running mean: -19.332780\n",
            "resetting env. episode 38.000000, reward total was -20.000000. running mean: -19.339452\n",
            "resetting env. episode 39.000000, reward total was -20.000000. running mean: -19.346057\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -19.362597\n",
            "resetting env. episode 41.000000, reward total was -20.000000. running mean: -19.368971\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -19.385281\n",
            "resetting env. episode 43.000000, reward total was -20.000000. running mean: -19.391428\n",
            "resetting env. episode 44.000000, reward total was -20.000000. running mean: -19.397514\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -19.413539\n",
            "resetting env. episode 46.000000, reward total was -20.000000. running mean: -19.419404\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -19.435210\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -19.450857\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -19.466349\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -19.481685\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -19.496868\n",
            "resetting env. episode 52.000000, reward total was -19.000000. running mean: -19.491900\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -19.506981\n",
            "resetting env. episode 54.000000, reward total was -20.000000. running mean: -19.511911\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -19.526792\n",
            "resetting env. episode 56.000000, reward total was -19.000000. running mean: -19.521524\n",
            "resetting env. episode 57.000000, reward total was -18.000000. running mean: -19.506309\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -19.521246\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -19.536033\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -19.550673\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -19.565166\n",
            "resetting env. episode 62.000000, reward total was -20.000000. running mean: -19.569514\n",
            "resetting env. episode 63.000000, reward total was -20.000000. running mean: -19.573819\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -19.588081\n",
            "resetting env. episode 65.000000, reward total was -20.000000. running mean: -19.592200\n",
            "resetting env. episode 66.000000, reward total was -19.000000. running mean: -19.586278\n",
            "resetting env. episode 67.000000, reward total was -19.000000. running mean: -19.580416\n",
            "resetting env. episode 68.000000, reward total was -20.000000. running mean: -19.584611\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -19.598765\n",
            "resetting env. episode 70.000000, reward total was -20.000000. running mean: -19.602778\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -19.606750\n",
            "resetting env. episode 72.000000, reward total was -20.000000. running mean: -19.610682\n",
            "resetting env. episode 73.000000, reward total was -20.000000. running mean: -19.614576\n",
            "resetting env. episode 74.000000, reward total was -20.000000. running mean: -19.618430\n",
            "resetting env. episode 75.000000, reward total was -19.000000. running mean: -19.612245\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -19.626123\n",
            "resetting env. episode 77.000000, reward total was -20.000000. running mean: -19.629862\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -19.643563\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -19.657128\n",
            "resetting env. episode 80.000000, reward total was -20.000000. running mean: -19.660556\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -19.673951\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -19.687211\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -19.700339\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -19.713336\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -19.726202\n",
            "resetting env. episode 86.000000, reward total was -19.000000. running mean: -19.718940\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -19.731751\n",
            "resetting env. episode 88.000000, reward total was -20.000000. running mean: -19.734433\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -19.747089\n",
            "resetting env. episode 90.000000, reward total was -20.000000. running mean: -19.749618\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -19.762122\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -19.774501\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -19.786756\n",
            "resetting env. episode 94.000000, reward total was -20.000000. running mean: -19.788888\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -19.800999\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -19.812989\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -19.824859\n",
            "resetting env. episode 98.000000, reward total was -20.000000. running mean: -19.826611\n",
            "resetting env. episode 99.000000, reward total was -17.000000. running mean: -19.798345\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -19.810361\n",
            "resetting env. episode 101.000000, reward total was -19.000000. running mean: -19.802258\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -19.814235\n",
            "resetting env. episode 103.000000, reward total was -20.000000. running mean: -19.816093\n",
            "resetting env. episode 104.000000, reward total was -20.000000. running mean: -19.817932\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -19.829752\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -19.841455\n",
            "resetting env. episode 107.000000, reward total was -20.000000. running mean: -19.843040\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -19.854610\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -19.866064\n",
            "resetting env. episode 110.000000, reward total was -20.000000. running mean: -19.867403\n",
            "resetting env. episode 111.000000, reward total was -18.000000. running mean: -19.848729\n",
            "resetting env. episode 112.000000, reward total was -19.000000. running mean: -19.840242\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -19.841840\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -19.853421\n",
            "resetting env. episode 115.000000, reward total was -19.000000. running mean: -19.844887\n",
            "resetting env. episode 116.000000, reward total was -20.000000. running mean: -19.846438\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -19.857974\n",
            "resetting env. episode 118.000000, reward total was -18.000000. running mean: -19.839394\n",
            "resetting env. episode 119.000000, reward total was -20.000000. running mean: -19.841000\n",
            "resetting env. episode 120.000000, reward total was -20.000000. running mean: -19.842590\n",
            "resetting env. episode 121.000000, reward total was -20.000000. running mean: -19.844164\n",
            "resetting env. episode 122.000000, reward total was -20.000000. running mean: -19.845722\n",
            "resetting env. episode 123.000000, reward total was -20.000000. running mean: -19.847265\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -19.858793\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -19.870205\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -19.881503\n",
            "resetting env. episode 127.000000, reward total was -19.000000. running mean: -19.872688\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -19.883961\n",
            "resetting env. episode 129.000000, reward total was -20.000000. running mean: -19.885121\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -19.896270\n",
            "resetting env. episode 131.000000, reward total was -18.000000. running mean: -19.877307\n",
            "resetting env. episode 132.000000, reward total was -20.000000. running mean: -19.878534\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -19.889749\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -19.900851\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -19.911843\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -19.922724\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -19.933497\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -19.944162\n",
            "resetting env. episode 139.000000, reward total was -19.000000. running mean: -19.934721\n",
            "resetting env. episode 140.000000, reward total was -19.000000. running mean: -19.925373\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -19.936120\n",
            "resetting env. episode 142.000000, reward total was -20.000000. running mean: -19.936758\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -19.947391\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -19.957917\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -19.968338\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -19.978654\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -19.978868\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -19.989079\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -19.999188\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.009196\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -20.009104\n",
            "resetting env. episode 152.000000, reward total was -20.000000. running mean: -20.009013\n",
            "resetting env. episode 153.000000, reward total was -20.000000. running mean: -20.008923\n",
            "resetting env. episode 154.000000, reward total was -20.000000. running mean: -20.008834\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.018746\n",
            "resetting env. episode 156.000000, reward total was -20.000000. running mean: -20.018558\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.028373\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.038089\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.047708\n",
            "resetting env. episode 160.000000, reward total was -20.000000. running mean: -20.047231\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.056759\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.066191\n",
            "resetting env. episode 163.000000, reward total was -19.000000. running mean: -20.055529\n",
            "resetting env. episode 164.000000, reward total was -20.000000. running mean: -20.054974\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.064424\n",
            "resetting env. episode 166.000000, reward total was -19.000000. running mean: -20.053780\n",
            "resetting env. episode 167.000000, reward total was -19.000000. running mean: -20.043242\n",
            "resetting env. episode 168.000000, reward total was -20.000000. running mean: -20.042810\n",
            "resetting env. episode 169.000000, reward total was -18.000000. running mean: -20.022382\n",
            "resetting env. episode 170.000000, reward total was -19.000000. running mean: -20.012158\n",
            "resetting env. episode 171.000000, reward total was -20.000000. running mean: -20.012036\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.021916\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.031697\n",
            "resetting env. episode 174.000000, reward total was -20.000000. running mean: -20.031380\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.041066\n",
            "resetting env. episode 176.000000, reward total was -19.000000. running mean: -20.030655\n",
            "resetting env. episode 177.000000, reward total was -19.000000. running mean: -20.020349\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.030145\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.039844\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.049445\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.058951\n",
            "resetting env. episode 182.000000, reward total was -20.000000. running mean: -20.058361\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.067778\n",
            "resetting env. episode 184.000000, reward total was -20.000000. running mean: -20.067100\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.076429\n",
            "resetting env. episode 186.000000, reward total was -20.000000. running mean: -20.075665\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.084908\n",
            "resetting env. episode 188.000000, reward total was -17.000000. running mean: -20.054059\n",
            "resetting env. episode 189.000000, reward total was -20.000000. running mean: -20.053518\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -20.052983\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.062453\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.071829\n",
            "resetting env. episode 193.000000, reward total was -20.000000. running mean: -20.071111\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.080399\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.089595\n",
            "resetting env. episode 196.000000, reward total was -20.000000. running mean: -20.088699\n",
            "resetting env. episode 197.000000, reward total was -20.000000. running mean: -20.087812\n",
            "resetting env. episode 198.000000, reward total was -20.000000. running mean: -20.086934\n",
            "resetting env. episode 199.000000, reward total was -20.000000. running mean: -20.086065\n",
            "resetting env. episode 200.000000, reward total was -20.000000. running mean: -20.085204\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.094352\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.103409\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.112375\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.121251\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.130038\n",
            "resetting env. episode 206.000000, reward total was -18.000000. running mean: -20.108738\n",
            "resetting env. episode 207.000000, reward total was -20.000000. running mean: -20.107651\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.116574\n",
            "resetting env. episode 209.000000, reward total was -20.000000. running mean: -20.115408\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.124254\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.133012\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.141682\n",
            "resetting env. episode 213.000000, reward total was -20.000000. running mean: -20.140265\n",
            "resetting env. episode 214.000000, reward total was -18.000000. running mean: -20.118862\n",
            "resetting env. episode 215.000000, reward total was -19.000000. running mean: -20.107674\n",
            "resetting env. episode 216.000000, reward total was -20.000000. running mean: -20.106597\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.115531\n",
            "resetting env. episode 218.000000, reward total was -20.000000. running mean: -20.114376\n",
            "resetting env. episode 219.000000, reward total was -18.000000. running mean: -20.093232\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.102300\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.111277\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.120164\n",
            "resetting env. episode 223.000000, reward total was -20.000000. running mean: -20.118962\n",
            "resetting env. episode 224.000000, reward total was -20.000000. running mean: -20.117772\n",
            "resetting env. episode 225.000000, reward total was -20.000000. running mean: -20.116595\n",
            "resetting env. episode 226.000000, reward total was -19.000000. running mean: -20.105429\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.114375\n",
            "resetting env. episode 228.000000, reward total was -20.000000. running mean: -20.113231\n",
            "resetting env. episode 229.000000, reward total was -20.000000. running mean: -20.112098\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.120977\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.129768\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.138470\n",
            "resetting env. episode 233.000000, reward total was -20.000000. running mean: -20.137085\n",
            "resetting env. episode 234.000000, reward total was -20.000000. running mean: -20.135714\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.144357\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.152914\n",
            "resetting env. episode 237.000000, reward total was -19.000000. running mean: -20.141385\n",
            "resetting env. episode 238.000000, reward total was -20.000000. running mean: -20.139971\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.148571\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.157085\n",
            "resetting env. episode 241.000000, reward total was -19.000000. running mean: -20.145515\n",
            "resetting env. episode 242.000000, reward total was -20.000000. running mean: -20.144059\n",
            "resetting env. episode 243.000000, reward total was -18.000000. running mean: -20.122619\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.131393\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.140079\n",
            "resetting env. episode 246.000000, reward total was -20.000000. running mean: -20.138678\n",
            "resetting env. episode 247.000000, reward total was -20.000000. running mean: -20.137291\n",
            "resetting env. episode 248.000000, reward total was -20.000000. running mean: -20.135918\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.144559\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.153113\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.161582\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.169966\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.178267\n",
            "resetting env. episode 254.000000, reward total was -18.000000. running mean: -20.156484\n",
            "resetting env. episode 255.000000, reward total was -19.000000. running mean: -20.144919\n",
            "resetting env. episode 256.000000, reward total was -19.000000. running mean: -20.133470\n",
            "resetting env. episode 257.000000, reward total was -19.000000. running mean: -20.122135\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.130914\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.139605\n",
            "resetting env. episode 260.000000, reward total was -19.000000. running mean: -20.128209\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.136927\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.145557\n",
            "resetting env. episode 263.000000, reward total was -20.000000. running mean: -20.144102\n",
            "resetting env. episode 264.000000, reward total was -20.000000. running mean: -20.142661\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.151234\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.159722\n",
            "resetting env. episode 267.000000, reward total was -19.000000. running mean: -20.148125\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.156643\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.165077\n",
            "resetting env. episode 270.000000, reward total was -20.000000. running mean: -20.163426\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.171792\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.180074\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.188273\n",
            "resetting env. episode 274.000000, reward total was -20.000000. running mean: -20.186391\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.194527\n",
            "resetting env. episode 276.000000, reward total was -20.000000. running mean: -20.192581\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.200656\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.208649\n",
            "resetting env. episode 279.000000, reward total was -19.000000. running mean: -20.196563\n",
            "resetting env. episode 280.000000, reward total was -19.000000. running mean: -20.184597\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.192751\n",
            "resetting env. episode 282.000000, reward total was -20.000000. running mean: -20.190823\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.198915\n",
            "resetting env. episode 284.000000, reward total was -18.000000. running mean: -20.176926\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.185157\n",
            "resetting env. episode 286.000000, reward total was -20.000000. running mean: -20.183305\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.191472\n",
            "resetting env. episode 288.000000, reward total was -20.000000. running mean: -20.189557\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.197662\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.205685\n",
            "resetting env. episode 291.000000, reward total was -20.000000. running mean: -20.203628\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.211592\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.219476\n",
            "resetting env. episode 294.000000, reward total was -20.000000. running mean: -20.217281\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.225109\n",
            "resetting env. episode 296.000000, reward total was -19.000000. running mean: -20.212858\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.220729\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.218522\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -20.216336\n",
            "resetting env. episode 300.000000, reward total was -20.000000. running mean: -20.214173\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.222031\n",
            "resetting env. episode 302.000000, reward total was -20.000000. running mean: -20.219811\n",
            "resetting env. episode 303.000000, reward total was -20.000000. running mean: -20.217613\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.225437\n",
            "resetting env. episode 305.000000, reward total was -17.000000. running mean: -20.193182\n",
            "resetting env. episode 306.000000, reward total was -20.000000. running mean: -20.191251\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.199338\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.207345\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.215271\n",
            "resetting env. episode 310.000000, reward total was -20.000000. running mean: -20.213119\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.220987\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.228778\n",
            "resetting env. episode 313.000000, reward total was -20.000000. running mean: -20.226490\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.234225\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.241883\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.249464\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.256969\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.264399\n",
            "resetting env. episode 319.000000, reward total was -18.000000. running mean: -20.241755\n",
            "resetting env. episode 320.000000, reward total was -20.000000. running mean: -20.239338\n",
            "resetting env. episode 321.000000, reward total was -20.000000. running mean: -20.236945\n",
            "resetting env. episode 322.000000, reward total was -19.000000. running mean: -20.224575\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.232329\n",
            "resetting env. episode 324.000000, reward total was -20.000000. running mean: -20.230006\n",
            "resetting env. episode 325.000000, reward total was -19.000000. running mean: -20.217706\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.225529\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.233274\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.240941\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.248531\n",
            "resetting env. episode 330.000000, reward total was -19.000000. running mean: -20.236046\n",
            "resetting env. episode 331.000000, reward total was -17.000000. running mean: -20.203686\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.211649\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.219532\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.227337\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.235064\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.242713\n",
            "resetting env. episode 337.000000, reward total was -20.000000. running mean: -20.240286\n",
            "resetting env. episode 338.000000, reward total was -19.000000. running mean: -20.227883\n",
            "resetting env. episode 339.000000, reward total was -20.000000. running mean: -20.225604\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.233348\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.241015\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.248605\n",
            "resetting env. episode 343.000000, reward total was -19.000000. running mean: -20.236118\n",
            "resetting env. episode 344.000000, reward total was -20.000000. running mean: -20.233757\n",
            "resetting env. episode 345.000000, reward total was -20.000000. running mean: -20.231420\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.239106\n",
            "resetting env. episode 347.000000, reward total was -19.000000. running mean: -20.226714\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.234447\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.242103\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.249682\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -20.247185\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.254713\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.262166\n",
            "resetting env. episode 354.000000, reward total was -20.000000. running mean: -20.259544\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.266949\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.274279\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.281537\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -20.278721\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.285934\n",
            "resetting env. episode 360.000000, reward total was -19.000000. running mean: -20.273075\n",
            "resetting env. episode 361.000000, reward total was -20.000000. running mean: -20.270344\n",
            "resetting env. episode 362.000000, reward total was -19.000000. running mean: -20.257641\n",
            "resetting env. episode 363.000000, reward total was -20.000000. running mean: -20.255064\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.262513\n",
            "resetting env. episode 365.000000, reward total was -20.000000. running mean: -20.259888\n",
            "resetting env. episode 366.000000, reward total was -20.000000. running mean: -20.257289\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.264717\n",
            "resetting env. episode 368.000000, reward total was -20.000000. running mean: -20.262069\n",
            "resetting env. episode 369.000000, reward total was -20.000000. running mean: -20.259449\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.266854\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.274186\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.281444\n",
            "resetting env. episode 373.000000, reward total was -19.000000. running mean: -20.268629\n",
            "resetting env. episode 374.000000, reward total was -19.000000. running mean: -20.255943\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.263384\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.270750\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.278042\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.285262\n",
            "resetting env. episode 379.000000, reward total was -20.000000. running mean: -20.282409\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.289585\n",
            "resetting env. episode 381.000000, reward total was -19.000000. running mean: -20.276689\n",
            "resetting env. episode 382.000000, reward total was -19.000000. running mean: -20.263922\n",
            "resetting env. episode 383.000000, reward total was -20.000000. running mean: -20.261283\n",
            "resetting env. episode 384.000000, reward total was -20.000000. running mean: -20.258670\n",
            "resetting env. episode 385.000000, reward total was -19.000000. running mean: -20.246084\n",
            "resetting env. episode 386.000000, reward total was -19.000000. running mean: -20.233623\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.241287\n",
            "resetting env. episode 388.000000, reward total was -20.000000. running mean: -20.238874\n",
            "resetting env. episode 389.000000, reward total was -20.000000. running mean: -20.236485\n",
            "resetting env. episode 390.000000, reward total was -20.000000. running mean: -20.234120\n",
            "resetting env. episode 391.000000, reward total was -19.000000. running mean: -20.221779\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.229561\n",
            "resetting env. episode 393.000000, reward total was -20.000000. running mean: -20.227266\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.234993\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.242643\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.250217\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.257714\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.265137\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.272486\n",
            "resetting env. episode 400.000000, reward total was -20.000000. running mean: -20.269761\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.277063\n",
            "resetting env. episode 402.000000, reward total was -19.000000. running mean: -20.264293\n",
            "resetting env. episode 403.000000, reward total was -20.000000. running mean: -20.261650\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.269033\n",
            "resetting env. episode 405.000000, reward total was -20.000000. running mean: -20.266343\n",
            "resetting env. episode 406.000000, reward total was -20.000000. running mean: -20.263680\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.271043\n",
            "resetting env. episode 408.000000, reward total was -20.000000. running mean: -20.268332\n",
            "resetting env. episode 409.000000, reward total was -20.000000. running mean: -20.265649\n",
            "resetting env. episode 410.000000, reward total was -20.000000. running mean: -20.262993\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -20.260363\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -20.257759\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.265181\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.272530\n",
            "resetting env. episode 415.000000, reward total was -17.000000. running mean: -20.239804\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.247406\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.254932\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.262383\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.269759\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.277061\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.284291\n",
            "resetting env. episode 422.000000, reward total was -20.000000. running mean: -20.281448\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.288633\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.295747\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.302790\n",
            "resetting env. episode 426.000000, reward total was -20.000000. running mean: -20.299762\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.306764\n",
            "resetting env. episode 428.000000, reward total was -20.000000. running mean: -20.303696\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.310660\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.317553\n",
            "resetting env. episode 431.000000, reward total was -19.000000. running mean: -20.304377\n",
            "resetting env. episode 432.000000, reward total was -20.000000. running mean: -20.301334\n",
            "resetting env. episode 433.000000, reward total was -18.000000. running mean: -20.278320\n",
            "resetting env. episode 434.000000, reward total was -20.000000. running mean: -20.275537\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -20.272782\n",
            "resetting env. episode 436.000000, reward total was -20.000000. running mean: -20.270054\n",
            "resetting env. episode 437.000000, reward total was -20.000000. running mean: -20.267353\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.274680\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.281933\n",
            "resetting env. episode 440.000000, reward total was -20.000000. running mean: -20.279114\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.286323\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.293459\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.300525\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.307519\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.314444\n",
            "resetting env. episode 446.000000, reward total was -19.000000. running mean: -20.301300\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.308287\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.315204\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.322052\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.328831\n",
            "resetting env. episode 451.000000, reward total was -19.000000. running mean: -20.315543\n",
            "resetting env. episode 452.000000, reward total was -20.000000. running mean: -20.312388\n",
            "resetting env. episode 453.000000, reward total was -19.000000. running mean: -20.299264\n",
            "resetting env. episode 454.000000, reward total was -19.000000. running mean: -20.286271\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.293408\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.300474\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.307470\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.314395\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.321251\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.328038\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.334758\n",
            "resetting env. episode 462.000000, reward total was -20.000000. running mean: -20.331411\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.338096\n",
            "resetting env. episode 464.000000, reward total was -20.000000. running mean: -20.334715\n",
            "resetting env. episode 465.000000, reward total was -20.000000. running mean: -20.331368\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.338055\n",
            "resetting env. episode 467.000000, reward total was -19.000000. running mean: -20.324674\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.331427\n",
            "resetting env. episode 469.000000, reward total was -20.000000. running mean: -20.328113\n",
            "resetting env. episode 470.000000, reward total was -20.000000. running mean: -20.324832\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.331584\n",
            "resetting env. episode 472.000000, reward total was -18.000000. running mean: -20.308268\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -20.305185\n",
            "resetting env. episode 474.000000, reward total was -20.000000. running mean: -20.302133\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.309112\n",
            "resetting env. episode 476.000000, reward total was -19.000000. running mean: -20.296021\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.303061\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.300030\n",
            "resetting env. episode 479.000000, reward total was -20.000000. running mean: -20.297030\n",
            "resetting env. episode 480.000000, reward total was -19.000000. running mean: -20.284059\n",
            "resetting env. episode 481.000000, reward total was -18.000000. running mean: -20.261219\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.268607\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.275921\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.283161\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.290330\n",
            "resetting env. episode 486.000000, reward total was -20.000000. running mean: -20.287426\n",
            "resetting env. episode 487.000000, reward total was -20.000000. running mean: -20.284552\n",
            "resetting env. episode 488.000000, reward total was -20.000000. running mean: -20.281707\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.288890\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.296001\n",
            "resetting env. episode 491.000000, reward total was -20.000000. running mean: -20.293041\n",
            "resetting env. episode 492.000000, reward total was -20.000000. running mean: -20.290110\n",
            "resetting env. episode 493.000000, reward total was -20.000000. running mean: -20.287209\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.294337\n",
            "resetting env. episode 495.000000, reward total was -20.000000. running mean: -20.291394\n",
            "resetting env. episode 496.000000, reward total was -20.000000. running mean: -20.288480\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.295595\n",
            "resetting env. episode 498.000000, reward total was -20.000000. running mean: -20.292639\n",
            "resetting env. episode 499.000000, reward total was -20.000000. running mean: -20.289713\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.296815\n",
            "resetting env. episode 501.000000, reward total was -21.000000. running mean: -20.303847\n",
            "resetting env. episode 502.000000, reward total was -21.000000. running mean: -20.310809\n",
            "resetting env. episode 503.000000, reward total was -18.000000. running mean: -20.287701\n",
            "resetting env. episode 504.000000, reward total was -20.000000. running mean: -20.284824\n",
            "resetting env. episode 505.000000, reward total was -21.000000. running mean: -20.291976\n",
            "resetting env. episode 506.000000, reward total was -20.000000. running mean: -20.289056\n",
            "resetting env. episode 507.000000, reward total was -21.000000. running mean: -20.296165\n",
            "resetting env. episode 508.000000, reward total was -21.000000. running mean: -20.303204\n",
            "resetting env. episode 509.000000, reward total was -21.000000. running mean: -20.310172\n",
            "resetting env. episode 510.000000, reward total was -20.000000. running mean: -20.307070\n",
            "resetting env. episode 511.000000, reward total was -21.000000. running mean: -20.313999\n",
            "resetting env. episode 512.000000, reward total was -20.000000. running mean: -20.310859\n",
            "resetting env. episode 513.000000, reward total was -21.000000. running mean: -20.317751\n",
            "resetting env. episode 514.000000, reward total was -19.000000. running mean: -20.304573\n",
            "resetting env. episode 515.000000, reward total was -21.000000. running mean: -20.311527\n",
            "resetting env. episode 516.000000, reward total was -18.000000. running mean: -20.288412\n",
            "resetting env. episode 517.000000, reward total was -20.000000. running mean: -20.285528\n",
            "resetting env. episode 518.000000, reward total was -21.000000. running mean: -20.292673\n",
            "resetting env. episode 519.000000, reward total was -20.000000. running mean: -20.289746\n",
            "resetting env. episode 520.000000, reward total was -21.000000. running mean: -20.296848\n",
            "resetting env. episode 521.000000, reward total was -21.000000. running mean: -20.303880\n",
            "resetting env. episode 522.000000, reward total was -20.000000. running mean: -20.300841\n",
            "resetting env. episode 523.000000, reward total was -21.000000. running mean: -20.307833\n",
            "resetting env. episode 524.000000, reward total was -21.000000. running mean: -20.314754\n",
            "resetting env. episode 525.000000, reward total was -21.000000. running mean: -20.321607\n",
            "resetting env. episode 526.000000, reward total was -21.000000. running mean: -20.328391\n",
            "resetting env. episode 527.000000, reward total was -20.000000. running mean: -20.325107\n",
            "resetting env. episode 528.000000, reward total was -19.000000. running mean: -20.311856\n",
            "resetting env. episode 529.000000, reward total was -21.000000. running mean: -20.318737\n",
            "resetting env. episode 530.000000, reward total was -20.000000. running mean: -20.315550\n",
            "resetting env. episode 531.000000, reward total was -20.000000. running mean: -20.312394\n",
            "resetting env. episode 532.000000, reward total was -21.000000. running mean: -20.319270\n",
            "resetting env. episode 533.000000, reward total was -20.000000. running mean: -20.316078\n",
            "resetting env. episode 534.000000, reward total was -21.000000. running mean: -20.322917\n",
            "resetting env. episode 535.000000, reward total was -21.000000. running mean: -20.329688\n",
            "resetting env. episode 536.000000, reward total was -21.000000. running mean: -20.336391\n",
            "resetting env. episode 537.000000, reward total was -21.000000. running mean: -20.343027\n",
            "resetting env. episode 538.000000, reward total was -20.000000. running mean: -20.339597\n",
            "resetting env. episode 539.000000, reward total was -21.000000. running mean: -20.346201\n",
            "resetting env. episode 540.000000, reward total was -19.000000. running mean: -20.332739\n",
            "resetting env. episode 541.000000, reward total was -19.000000. running mean: -20.319411\n",
            "resetting env. episode 542.000000, reward total was -21.000000. running mean: -20.326217\n",
            "resetting env. episode 543.000000, reward total was -18.000000. running mean: -20.302955\n",
            "resetting env. episode 544.000000, reward total was -20.000000. running mean: -20.299926\n",
            "resetting env. episode 545.000000, reward total was -21.000000. running mean: -20.306926\n",
            "resetting env. episode 546.000000, reward total was -20.000000. running mean: -20.303857\n",
            "resetting env. episode 547.000000, reward total was -21.000000. running mean: -20.310818\n",
            "resetting env. episode 548.000000, reward total was -19.000000. running mean: -20.297710\n",
            "resetting env. episode 549.000000, reward total was -20.000000. running mean: -20.294733\n",
            "resetting env. episode 550.000000, reward total was -21.000000. running mean: -20.301786\n",
            "resetting env. episode 551.000000, reward total was -20.000000. running mean: -20.298768\n",
            "resetting env. episode 552.000000, reward total was -21.000000. running mean: -20.305780\n",
            "resetting env. episode 553.000000, reward total was -20.000000. running mean: -20.302722\n",
            "resetting env. episode 554.000000, reward total was -20.000000. running mean: -20.299695\n",
            "resetting env. episode 555.000000, reward total was -19.000000. running mean: -20.286698\n",
            "resetting env. episode 556.000000, reward total was -18.000000. running mean: -20.263831\n",
            "resetting env. episode 557.000000, reward total was -20.000000. running mean: -20.261193\n",
            "resetting env. episode 558.000000, reward total was -20.000000. running mean: -20.258581\n",
            "resetting env. episode 559.000000, reward total was -21.000000. running mean: -20.265995\n",
            "resetting env. episode 560.000000, reward total was -20.000000. running mean: -20.263335\n",
            "resetting env. episode 561.000000, reward total was -19.000000. running mean: -20.250702\n",
            "resetting env. episode 562.000000, reward total was -20.000000. running mean: -20.248195\n",
            "resetting env. episode 563.000000, reward total was -21.000000. running mean: -20.255713\n",
            "resetting env. episode 564.000000, reward total was -20.000000. running mean: -20.253156\n",
            "resetting env. episode 565.000000, reward total was -20.000000. running mean: -20.250624\n",
            "resetting env. episode 566.000000, reward total was -21.000000. running mean: -20.258118\n",
            "resetting env. episode 567.000000, reward total was -21.000000. running mean: -20.265537\n",
            "resetting env. episode 568.000000, reward total was -21.000000. running mean: -20.272882\n",
            "resetting env. episode 569.000000, reward total was -21.000000. running mean: -20.280153\n",
            "resetting env. episode 570.000000, reward total was -20.000000. running mean: -20.277351\n",
            "resetting env. episode 571.000000, reward total was -19.000000. running mean: -20.264578\n",
            "resetting env. episode 572.000000, reward total was -21.000000. running mean: -20.271932\n",
            "resetting env. episode 573.000000, reward total was -19.000000. running mean: -20.259213\n",
            "resetting env. episode 574.000000, reward total was -19.000000. running mean: -20.246620\n",
            "resetting env. episode 575.000000, reward total was -20.000000. running mean: -20.244154\n",
            "resetting env. episode 576.000000, reward total was -21.000000. running mean: -20.251713\n",
            "resetting env. episode 577.000000, reward total was -21.000000. running mean: -20.259196\n",
            "resetting env. episode 578.000000, reward total was -19.000000. running mean: -20.246604\n",
            "resetting env. episode 579.000000, reward total was -20.000000. running mean: -20.244138\n",
            "resetting env. episode 580.000000, reward total was -20.000000. running mean: -20.241696\n",
            "resetting env. episode 581.000000, reward total was -20.000000. running mean: -20.239279\n",
            "resetting env. episode 582.000000, reward total was -20.000000. running mean: -20.236886\n",
            "resetting env. episode 583.000000, reward total was -21.000000. running mean: -20.244518\n",
            "resetting env. episode 584.000000, reward total was -20.000000. running mean: -20.242072\n",
            "resetting env. episode 585.000000, reward total was -18.000000. running mean: -20.219652\n",
            "resetting env. episode 586.000000, reward total was -21.000000. running mean: -20.227455\n",
            "resetting env. episode 587.000000, reward total was -21.000000. running mean: -20.235181\n",
            "resetting env. episode 588.000000, reward total was -21.000000. running mean: -20.242829\n",
            "resetting env. episode 589.000000, reward total was -21.000000. running mean: -20.250401\n",
            "resetting env. episode 590.000000, reward total was -21.000000. running mean: -20.257897\n",
            "resetting env. episode 591.000000, reward total was -20.000000. running mean: -20.255318\n",
            "resetting env. episode 592.000000, reward total was -21.000000. running mean: -20.262764\n",
            "resetting env. episode 593.000000, reward total was -20.000000. running mean: -20.260137\n",
            "resetting env. episode 594.000000, reward total was -20.000000. running mean: -20.257535\n",
            "resetting env. episode 595.000000, reward total was -20.000000. running mean: -20.254960\n",
            "resetting env. episode 596.000000, reward total was -21.000000. running mean: -20.262410\n",
            "resetting env. episode 597.000000, reward total was -21.000000. running mean: -20.269786\n",
            "resetting env. episode 598.000000, reward total was -20.000000. running mean: -20.267088\n",
            "resetting env. episode 599.000000, reward total was -21.000000. running mean: -20.274418\n",
            "resetting env. episode 600.000000, reward total was -21.000000. running mean: -20.281673\n",
            "resetting env. episode 601.000000, reward total was -20.000000. running mean: -20.278857\n",
            "resetting env. episode 602.000000, reward total was -17.000000. running mean: -20.246068\n",
            "resetting env. episode 603.000000, reward total was -20.000000. running mean: -20.243607\n",
            "resetting env. episode 604.000000, reward total was -19.000000. running mean: -20.231171\n",
            "resetting env. episode 605.000000, reward total was -20.000000. running mean: -20.228860\n",
            "resetting env. episode 606.000000, reward total was -21.000000. running mean: -20.236571\n",
            "resetting env. episode 607.000000, reward total was -21.000000. running mean: -20.244205\n",
            "resetting env. episode 608.000000, reward total was -20.000000. running mean: -20.241763\n",
            "resetting env. episode 609.000000, reward total was -21.000000. running mean: -20.249346\n",
            "resetting env. episode 610.000000, reward total was -21.000000. running mean: -20.256852\n",
            "resetting env. episode 611.000000, reward total was -21.000000. running mean: -20.264284\n",
            "resetting env. episode 612.000000, reward total was -18.000000. running mean: -20.241641\n",
            "resetting env. episode 613.000000, reward total was -20.000000. running mean: -20.239224\n",
            "resetting env. episode 614.000000, reward total was -21.000000. running mean: -20.246832\n",
            "resetting env. episode 615.000000, reward total was -21.000000. running mean: -20.254364\n",
            "resetting env. episode 616.000000, reward total was -20.000000. running mean: -20.251820\n",
            "resetting env. episode 617.000000, reward total was -21.000000. running mean: -20.259302\n",
            "resetting env. episode 618.000000, reward total was -19.000000. running mean: -20.246709\n",
            "resetting env. episode 619.000000, reward total was -20.000000. running mean: -20.244242\n",
            "resetting env. episode 620.000000, reward total was -20.000000. running mean: -20.241799\n",
            "resetting env. episode 621.000000, reward total was -21.000000. running mean: -20.249381\n",
            "resetting env. episode 622.000000, reward total was -21.000000. running mean: -20.256888\n",
            "resetting env. episode 623.000000, reward total was -19.000000. running mean: -20.244319\n",
            "resetting env. episode 624.000000, reward total was -21.000000. running mean: -20.251876\n",
            "resetting env. episode 625.000000, reward total was -19.000000. running mean: -20.239357\n",
            "resetting env. episode 626.000000, reward total was -20.000000. running mean: -20.236963\n",
            "resetting env. episode 627.000000, reward total was -20.000000. running mean: -20.234594\n",
            "resetting env. episode 628.000000, reward total was -21.000000. running mean: -20.242248\n",
            "resetting env. episode 629.000000, reward total was -21.000000. running mean: -20.249825\n",
            "resetting env. episode 630.000000, reward total was -21.000000. running mean: -20.257327\n",
            "resetting env. episode 631.000000, reward total was -19.000000. running mean: -20.244754\n",
            "resetting env. episode 632.000000, reward total was -21.000000. running mean: -20.252306\n",
            "resetting env. episode 633.000000, reward total was -20.000000. running mean: -20.249783\n",
            "resetting env. episode 634.000000, reward total was -20.000000. running mean: -20.247285\n",
            "resetting env. episode 635.000000, reward total was -21.000000. running mean: -20.254812\n",
            "resetting env. episode 636.000000, reward total was -20.000000. running mean: -20.252264\n",
            "resetting env. episode 637.000000, reward total was -20.000000. running mean: -20.249742\n",
            "resetting env. episode 638.000000, reward total was -21.000000. running mean: -20.257244\n",
            "resetting env. episode 639.000000, reward total was -19.000000. running mean: -20.244672\n",
            "resetting env. episode 640.000000, reward total was -21.000000. running mean: -20.252225\n",
            "resetting env. episode 641.000000, reward total was -21.000000. running mean: -20.259703\n",
            "resetting env. episode 642.000000, reward total was -21.000000. running mean: -20.267106\n",
            "resetting env. episode 643.000000, reward total was -19.000000. running mean: -20.254435\n",
            "resetting env. episode 644.000000, reward total was -21.000000. running mean: -20.261890\n",
            "resetting env. episode 645.000000, reward total was -19.000000. running mean: -20.249271\n",
            "resetting env. episode 646.000000, reward total was -20.000000. running mean: -20.246779\n",
            "resetting env. episode 647.000000, reward total was -21.000000. running mean: -20.254311\n",
            "resetting env. episode 648.000000, reward total was -21.000000. running mean: -20.261768\n",
            "resetting env. episode 649.000000, reward total was -20.000000. running mean: -20.259150\n",
            "resetting env. episode 650.000000, reward total was -21.000000. running mean: -20.266559\n",
            "resetting env. episode 651.000000, reward total was -21.000000. running mean: -20.273893\n",
            "resetting env. episode 652.000000, reward total was -19.000000. running mean: -20.261154\n",
            "resetting env. episode 653.000000, reward total was -21.000000. running mean: -20.268543\n",
            "resetting env. episode 654.000000, reward total was -21.000000. running mean: -20.275857\n",
            "resetting env. episode 655.000000, reward total was -20.000000. running mean: -20.273099\n",
            "resetting env. episode 656.000000, reward total was -20.000000. running mean: -20.270368\n",
            "resetting env. episode 657.000000, reward total was -21.000000. running mean: -20.277664\n",
            "resetting env. episode 658.000000, reward total was -21.000000. running mean: -20.284887\n",
            "resetting env. episode 659.000000, reward total was -21.000000. running mean: -20.292038\n",
            "resetting env. episode 660.000000, reward total was -20.000000. running mean: -20.289118\n",
            "resetting env. episode 661.000000, reward total was -21.000000. running mean: -20.296227\n",
            "resetting env. episode 662.000000, reward total was -20.000000. running mean: -20.293265\n",
            "resetting env. episode 663.000000, reward total was -20.000000. running mean: -20.290332\n",
            "resetting env. episode 664.000000, reward total was -21.000000. running mean: -20.297429\n",
            "resetting env. episode 665.000000, reward total was -18.000000. running mean: -20.274454\n",
            "resetting env. episode 666.000000, reward total was -21.000000. running mean: -20.281710\n",
            "resetting env. episode 667.000000, reward total was -18.000000. running mean: -20.258893\n",
            "resetting env. episode 668.000000, reward total was -21.000000. running mean: -20.266304\n",
            "resetting env. episode 669.000000, reward total was -21.000000. running mean: -20.273641\n",
            "resetting env. episode 670.000000, reward total was -21.000000. running mean: -20.280904\n",
            "resetting env. episode 671.000000, reward total was -20.000000. running mean: -20.278095\n",
            "resetting env. episode 672.000000, reward total was -20.000000. running mean: -20.275314\n",
            "resetting env. episode 673.000000, reward total was -20.000000. running mean: -20.272561\n",
            "resetting env. episode 674.000000, reward total was -20.000000. running mean: -20.269836\n",
            "resetting env. episode 675.000000, reward total was -21.000000. running mean: -20.277137\n",
            "resetting env. episode 676.000000, reward total was -20.000000. running mean: -20.274366\n",
            "resetting env. episode 677.000000, reward total was -21.000000. running mean: -20.281622\n",
            "resetting env. episode 678.000000, reward total was -21.000000. running mean: -20.288806\n",
            "resetting env. episode 679.000000, reward total was -21.000000. running mean: -20.295918\n",
            "resetting env. episode 680.000000, reward total was -20.000000. running mean: -20.292959\n",
            "resetting env. episode 681.000000, reward total was -19.000000. running mean: -20.280029\n",
            "resetting env. episode 682.000000, reward total was -21.000000. running mean: -20.287229\n",
            "resetting env. episode 683.000000, reward total was -21.000000. running mean: -20.294357\n",
            "resetting env. episode 684.000000, reward total was -20.000000. running mean: -20.291413\n",
            "resetting env. episode 685.000000, reward total was -20.000000. running mean: -20.288499\n",
            "resetting env. episode 686.000000, reward total was -20.000000. running mean: -20.285614\n",
            "resetting env. episode 687.000000, reward total was -19.000000. running mean: -20.272758\n",
            "resetting env. episode 688.000000, reward total was -21.000000. running mean: -20.280030\n",
            "resetting env. episode 689.000000, reward total was -19.000000. running mean: -20.267230\n",
            "resetting env. episode 690.000000, reward total was -20.000000. running mean: -20.264558\n",
            "resetting env. episode 691.000000, reward total was -19.000000. running mean: -20.251912\n",
            "resetting env. episode 692.000000, reward total was -19.000000. running mean: -20.239393\n",
            "resetting env. episode 693.000000, reward total was -20.000000. running mean: -20.236999\n",
            "resetting env. episode 694.000000, reward total was -21.000000. running mean: -20.244629\n",
            "resetting env. episode 695.000000, reward total was -19.000000. running mean: -20.232183\n",
            "resetting env. episode 696.000000, reward total was -18.000000. running mean: -20.209861\n",
            "resetting env. episode 697.000000, reward total was -20.000000. running mean: -20.207762\n",
            "resetting env. episode 698.000000, reward total was -21.000000. running mean: -20.215685\n",
            "resetting env. episode 699.000000, reward total was -19.000000. running mean: -20.203528\n",
            "resetting env. episode 700.000000, reward total was -21.000000. running mean: -20.211492\n",
            "resetting env. episode 701.000000, reward total was -21.000000. running mean: -20.219378\n",
            "resetting env. episode 702.000000, reward total was -21.000000. running mean: -20.227184\n",
            "resetting env. episode 703.000000, reward total was -20.000000. running mean: -20.224912\n",
            "resetting env. episode 704.000000, reward total was -21.000000. running mean: -20.232663\n",
            "resetting env. episode 705.000000, reward total was -21.000000. running mean: -20.240336\n",
            "resetting env. episode 706.000000, reward total was -19.000000. running mean: -20.227933\n",
            "resetting env. episode 707.000000, reward total was -20.000000. running mean: -20.225654\n",
            "resetting env. episode 708.000000, reward total was -20.000000. running mean: -20.223397\n",
            "resetting env. episode 709.000000, reward total was -20.000000. running mean: -20.221163\n",
            "resetting env. episode 710.000000, reward total was -21.000000. running mean: -20.228951\n",
            "resetting env. episode 711.000000, reward total was -17.000000. running mean: -20.196662\n",
            "resetting env. episode 712.000000, reward total was -17.000000. running mean: -20.164695\n",
            "resetting env. episode 713.000000, reward total was -21.000000. running mean: -20.173048\n",
            "resetting env. episode 714.000000, reward total was -20.000000. running mean: -20.171318\n",
            "resetting env. episode 715.000000, reward total was -20.000000. running mean: -20.169605\n",
            "resetting env. episode 716.000000, reward total was -20.000000. running mean: -20.167909\n",
            "resetting env. episode 717.000000, reward total was -20.000000. running mean: -20.166230\n",
            "resetting env. episode 718.000000, reward total was -21.000000. running mean: -20.174567\n",
            "resetting env. episode 719.000000, reward total was -20.000000. running mean: -20.172822\n",
            "resetting env. episode 720.000000, reward total was -19.000000. running mean: -20.161093\n",
            "resetting env. episode 721.000000, reward total was -20.000000. running mean: -20.159482\n",
            "resetting env. episode 722.000000, reward total was -21.000000. running mean: -20.167888\n",
            "resetting env. episode 723.000000, reward total was -20.000000. running mean: -20.166209\n",
            "resetting env. episode 724.000000, reward total was -20.000000. running mean: -20.164547\n",
            "resetting env. episode 725.000000, reward total was -21.000000. running mean: -20.172901\n",
            "resetting env. episode 726.000000, reward total was -21.000000. running mean: -20.181172\n",
            "resetting env. episode 727.000000, reward total was -21.000000. running mean: -20.189360\n",
            "resetting env. episode 728.000000, reward total was -20.000000. running mean: -20.187467\n",
            "resetting env. episode 729.000000, reward total was -21.000000. running mean: -20.195592\n",
            "resetting env. episode 730.000000, reward total was -20.000000. running mean: -20.193636\n",
            "resetting env. episode 731.000000, reward total was -21.000000. running mean: -20.201700\n",
            "resetting env. episode 732.000000, reward total was -21.000000. running mean: -20.209683\n",
            "resetting env. episode 733.000000, reward total was -21.000000. running mean: -20.217586\n",
            "resetting env. episode 734.000000, reward total was -21.000000. running mean: -20.225410\n",
            "resetting env. episode 735.000000, reward total was -20.000000. running mean: -20.223156\n",
            "resetting env. episode 736.000000, reward total was -20.000000. running mean: -20.220925\n",
            "resetting env. episode 737.000000, reward total was -19.000000. running mean: -20.208715\n",
            "resetting env. episode 738.000000, reward total was -20.000000. running mean: -20.206628\n",
            "resetting env. episode 739.000000, reward total was -21.000000. running mean: -20.214562\n",
            "resetting env. episode 740.000000, reward total was -20.000000. running mean: -20.212416\n",
            "resetting env. episode 741.000000, reward total was -19.000000. running mean: -20.200292\n",
            "resetting env. episode 742.000000, reward total was -20.000000. running mean: -20.198289\n",
            "resetting env. episode 743.000000, reward total was -20.000000. running mean: -20.196306\n",
            "resetting env. episode 744.000000, reward total was -19.000000. running mean: -20.184343\n",
            "resetting env. episode 745.000000, reward total was -20.000000. running mean: -20.182500\n",
            "resetting env. episode 746.000000, reward total was -20.000000. running mean: -20.180675\n",
            "resetting env. episode 747.000000, reward total was -19.000000. running mean: -20.168868\n",
            "resetting env. episode 748.000000, reward total was -21.000000. running mean: -20.177179\n",
            "resetting env. episode 749.000000, reward total was -21.000000. running mean: -20.185408\n",
            "resetting env. episode 750.000000, reward total was -18.000000. running mean: -20.163553\n",
            "resetting env. episode 751.000000, reward total was -21.000000. running mean: -20.171918\n",
            "resetting env. episode 752.000000, reward total was -20.000000. running mean: -20.170199\n",
            "resetting env. episode 753.000000, reward total was -21.000000. running mean: -20.178497\n",
            "resetting env. episode 754.000000, reward total was -21.000000. running mean: -20.186712\n",
            "resetting env. episode 755.000000, reward total was -20.000000. running mean: -20.184845\n",
            "resetting env. episode 756.000000, reward total was -20.000000. running mean: -20.182996\n",
            "resetting env. episode 757.000000, reward total was -21.000000. running mean: -20.191166\n",
            "resetting env. episode 758.000000, reward total was -21.000000. running mean: -20.199255\n",
            "resetting env. episode 759.000000, reward total was -21.000000. running mean: -20.207262\n",
            "resetting env. episode 760.000000, reward total was -20.000000. running mean: -20.205189\n",
            "resetting env. episode 761.000000, reward total was -19.000000. running mean: -20.193138\n",
            "resetting env. episode 762.000000, reward total was -21.000000. running mean: -20.201206\n",
            "resetting env. episode 763.000000, reward total was -18.000000. running mean: -20.179194\n",
            "resetting env. episode 764.000000, reward total was -21.000000. running mean: -20.187402\n",
            "resetting env. episode 765.000000, reward total was -20.000000. running mean: -20.185528\n",
            "resetting env. episode 766.000000, reward total was -21.000000. running mean: -20.193673\n",
            "resetting env. episode 767.000000, reward total was -20.000000. running mean: -20.191736\n",
            "resetting env. episode 768.000000, reward total was -21.000000. running mean: -20.199819\n",
            "resetting env. episode 769.000000, reward total was -21.000000. running mean: -20.207821\n",
            "resetting env. episode 770.000000, reward total was -21.000000. running mean: -20.215742\n",
            "resetting env. episode 771.000000, reward total was -21.000000. running mean: -20.223585\n",
            "resetting env. episode 772.000000, reward total was -21.000000. running mean: -20.231349\n",
            "resetting env. episode 773.000000, reward total was -20.000000. running mean: -20.229036\n",
            "resetting env. episode 774.000000, reward total was -21.000000. running mean: -20.236745\n",
            "resetting env. episode 775.000000, reward total was -20.000000. running mean: -20.234378\n",
            "resetting env. episode 776.000000, reward total was -21.000000. running mean: -20.242034\n",
            "resetting env. episode 777.000000, reward total was -20.000000. running mean: -20.239614\n",
            "resetting env. episode 778.000000, reward total was -20.000000. running mean: -20.237218\n",
            "resetting env. episode 779.000000, reward total was -21.000000. running mean: -20.244845\n",
            "resetting env. episode 780.000000, reward total was -19.000000. running mean: -20.232397\n",
            "resetting env. episode 781.000000, reward total was -21.000000. running mean: -20.240073\n",
            "resetting env. episode 782.000000, reward total was -19.000000. running mean: -20.227672\n",
            "resetting env. episode 783.000000, reward total was -20.000000. running mean: -20.225395\n",
            "resetting env. episode 784.000000, reward total was -20.000000. running mean: -20.223142\n",
            "resetting env. episode 785.000000, reward total was -21.000000. running mean: -20.230910\n",
            "resetting env. episode 786.000000, reward total was -19.000000. running mean: -20.218601\n",
            "resetting env. episode 787.000000, reward total was -21.000000. running mean: -20.226415\n",
            "resetting env. episode 788.000000, reward total was -21.000000. running mean: -20.234151\n",
            "resetting env. episode 789.000000, reward total was -21.000000. running mean: -20.241809\n",
            "resetting env. episode 790.000000, reward total was -19.000000. running mean: -20.229391\n",
            "resetting env. episode 791.000000, reward total was -20.000000. running mean: -20.227097\n",
            "resetting env. episode 792.000000, reward total was -20.000000. running mean: -20.224826\n",
            "resetting env. episode 793.000000, reward total was -21.000000. running mean: -20.232578\n",
            "resetting env. episode 794.000000, reward total was -21.000000. running mean: -20.240252\n",
            "resetting env. episode 795.000000, reward total was -19.000000. running mean: -20.227850\n",
            "resetting env. episode 796.000000, reward total was -19.000000. running mean: -20.215571\n",
            "resetting env. episode 797.000000, reward total was -21.000000. running mean: -20.223416\n",
            "resetting env. episode 798.000000, reward total was -21.000000. running mean: -20.231181\n",
            "resetting env. episode 799.000000, reward total was -18.000000. running mean: -20.208870\n",
            "resetting env. episode 800.000000, reward total was -19.000000. running mean: -20.196781\n",
            "resetting env. episode 801.000000, reward total was -21.000000. running mean: -20.204813\n",
            "resetting env. episode 802.000000, reward total was -21.000000. running mean: -20.212765\n",
            "resetting env. episode 803.000000, reward total was -21.000000. running mean: -20.220637\n",
            "resetting env. episode 804.000000, reward total was -20.000000. running mean: -20.218431\n",
            "resetting env. episode 805.000000, reward total was -21.000000. running mean: -20.226247\n",
            "resetting env. episode 806.000000, reward total was -21.000000. running mean: -20.233984\n",
            "resetting env. episode 807.000000, reward total was -20.000000. running mean: -20.231644\n",
            "resetting env. episode 808.000000, reward total was -21.000000. running mean: -20.239328\n",
            "resetting env. episode 809.000000, reward total was -19.000000. running mean: -20.226935\n",
            "resetting env. episode 810.000000, reward total was -21.000000. running mean: -20.234665\n",
            "resetting env. episode 811.000000, reward total was -21.000000. running mean: -20.242319\n",
            "resetting env. episode 812.000000, reward total was -20.000000. running mean: -20.239895\n",
            "resetting env. episode 813.000000, reward total was -21.000000. running mean: -20.247496\n",
            "resetting env. episode 814.000000, reward total was -19.000000. running mean: -20.235022\n",
            "resetting env. episode 815.000000, reward total was -19.000000. running mean: -20.222671\n",
            "resetting env. episode 816.000000, reward total was -21.000000. running mean: -20.230445\n",
            "resetting env. episode 817.000000, reward total was -20.000000. running mean: -20.228140\n",
            "resetting env. episode 818.000000, reward total was -20.000000. running mean: -20.225859\n",
            "resetting env. episode 819.000000, reward total was -21.000000. running mean: -20.233600\n",
            "resetting env. episode 820.000000, reward total was -21.000000. running mean: -20.241264\n",
            "resetting env. episode 821.000000, reward total was -20.000000. running mean: -20.238852\n",
            "resetting env. episode 822.000000, reward total was -20.000000. running mean: -20.236463\n",
            "resetting env. episode 823.000000, reward total was -18.000000. running mean: -20.214098\n",
            "resetting env. episode 824.000000, reward total was -21.000000. running mean: -20.221957\n",
            "resetting env. episode 825.000000, reward total was -21.000000. running mean: -20.229738\n",
            "resetting env. episode 826.000000, reward total was -21.000000. running mean: -20.237440\n",
            "resetting env. episode 827.000000, reward total was -20.000000. running mean: -20.235066\n",
            "resetting env. episode 828.000000, reward total was -21.000000. running mean: -20.242715\n",
            "resetting env. episode 829.000000, reward total was -21.000000. running mean: -20.250288\n",
            "resetting env. episode 830.000000, reward total was -21.000000. running mean: -20.257785\n",
            "resetting env. episode 831.000000, reward total was -20.000000. running mean: -20.255207\n",
            "resetting env. episode 832.000000, reward total was -19.000000. running mean: -20.242655\n",
            "resetting env. episode 833.000000, reward total was -21.000000. running mean: -20.250229\n",
            "resetting env. episode 834.000000, reward total was -20.000000. running mean: -20.247727\n",
            "resetting env. episode 835.000000, reward total was -21.000000. running mean: -20.255249\n",
            "resetting env. episode 836.000000, reward total was -20.000000. running mean: -20.252697\n",
            "resetting env. episode 837.000000, reward total was -20.000000. running mean: -20.250170\n",
            "resetting env. episode 838.000000, reward total was -21.000000. running mean: -20.257668\n",
            "resetting env. episode 839.000000, reward total was -21.000000. running mean: -20.265091\n",
            "resetting env. episode 840.000000, reward total was -20.000000. running mean: -20.262441\n",
            "resetting env. episode 841.000000, reward total was -21.000000. running mean: -20.269816\n",
            "resetting env. episode 842.000000, reward total was -19.000000. running mean: -20.257118\n",
            "resetting env. episode 843.000000, reward total was -21.000000. running mean: -20.264547\n",
            "resetting env. episode 844.000000, reward total was -21.000000. running mean: -20.271901\n",
            "resetting env. episode 845.000000, reward total was -21.000000. running mean: -20.279182\n",
            "resetting env. episode 846.000000, reward total was -19.000000. running mean: -20.266390\n",
            "resetting env. episode 847.000000, reward total was -19.000000. running mean: -20.253727\n",
            "resetting env. episode 848.000000, reward total was -21.000000. running mean: -20.261189\n",
            "resetting env. episode 849.000000, reward total was -21.000000. running mean: -20.268577\n",
            "resetting env. episode 850.000000, reward total was -20.000000. running mean: -20.265892\n",
            "resetting env. episode 851.000000, reward total was -20.000000. running mean: -20.263233\n",
            "resetting env. episode 852.000000, reward total was -20.000000. running mean: -20.260600\n",
            "resetting env. episode 853.000000, reward total was -20.000000. running mean: -20.257994\n",
            "resetting env. episode 854.000000, reward total was -19.000000. running mean: -20.245414\n",
            "resetting env. episode 855.000000, reward total was -21.000000. running mean: -20.252960\n",
            "resetting env. episode 856.000000, reward total was -20.000000. running mean: -20.250431\n",
            "resetting env. episode 857.000000, reward total was -20.000000. running mean: -20.247926\n",
            "resetting env. episode 858.000000, reward total was -21.000000. running mean: -20.255447\n",
            "resetting env. episode 859.000000, reward total was -20.000000. running mean: -20.252893\n",
            "resetting env. episode 860.000000, reward total was -21.000000. running mean: -20.260364\n",
            "resetting env. episode 861.000000, reward total was -19.000000. running mean: -20.247760\n",
            "resetting env. episode 862.000000, reward total was -20.000000. running mean: -20.245283\n",
            "resetting env. episode 863.000000, reward total was -19.000000. running mean: -20.232830\n",
            "resetting env. episode 864.000000, reward total was -21.000000. running mean: -20.240501\n",
            "resetting env. episode 865.000000, reward total was -21.000000. running mean: -20.248096\n",
            "resetting env. episode 866.000000, reward total was -19.000000. running mean: -20.235615\n",
            "resetting env. episode 867.000000, reward total was -21.000000. running mean: -20.243259\n",
            "resetting env. episode 868.000000, reward total was -17.000000. running mean: -20.210827\n",
            "resetting env. episode 869.000000, reward total was -21.000000. running mean: -20.218718\n",
            "resetting env. episode 870.000000, reward total was -18.000000. running mean: -20.196531\n",
            "resetting env. episode 871.000000, reward total was -19.000000. running mean: -20.184566\n",
            "resetting env. episode 872.000000, reward total was -19.000000. running mean: -20.172720\n",
            "resetting env. episode 873.000000, reward total was -21.000000. running mean: -20.180993\n",
            "resetting env. episode 874.000000, reward total was -21.000000. running mean: -20.189183\n",
            "resetting env. episode 875.000000, reward total was -21.000000. running mean: -20.197291\n",
            "resetting env. episode 876.000000, reward total was -20.000000. running mean: -20.195318\n",
            "resetting env. episode 877.000000, reward total was -21.000000. running mean: -20.203365\n",
            "resetting env. episode 878.000000, reward total was -20.000000. running mean: -20.201332\n",
            "resetting env. episode 879.000000, reward total was -19.000000. running mean: -20.189318\n",
            "resetting env. episode 880.000000, reward total was -18.000000. running mean: -20.167425\n",
            "resetting env. episode 881.000000, reward total was -20.000000. running mean: -20.165751\n",
            "resetting env. episode 882.000000, reward total was -20.000000. running mean: -20.164093\n",
            "resetting env. episode 883.000000, reward total was -20.000000. running mean: -20.162452\n",
            "resetting env. episode 884.000000, reward total was -20.000000. running mean: -20.160828\n",
            "resetting env. episode 885.000000, reward total was -21.000000. running mean: -20.169220\n",
            "resetting env. episode 886.000000, reward total was -21.000000. running mean: -20.177527\n",
            "resetting env. episode 887.000000, reward total was -21.000000. running mean: -20.185752\n",
            "resetting env. episode 888.000000, reward total was -21.000000. running mean: -20.193895\n",
            "resetting env. episode 889.000000, reward total was -19.000000. running mean: -20.181956\n",
            "resetting env. episode 890.000000, reward total was -20.000000. running mean: -20.180136\n",
            "resetting env. episode 891.000000, reward total was -21.000000. running mean: -20.188335\n",
            "resetting env. episode 892.000000, reward total was -21.000000. running mean: -20.196451\n",
            "resetting env. episode 893.000000, reward total was -20.000000. running mean: -20.194487\n",
            "resetting env. episode 894.000000, reward total was -19.000000. running mean: -20.182542\n",
            "resetting env. episode 895.000000, reward total was -19.000000. running mean: -20.170717\n",
            "resetting env. episode 896.000000, reward total was -21.000000. running mean: -20.179009\n",
            "resetting env. episode 897.000000, reward total was -21.000000. running mean: -20.187219\n",
            "resetting env. episode 898.000000, reward total was -20.000000. running mean: -20.185347\n",
            "resetting env. episode 899.000000, reward total was -20.000000. running mean: -20.183494\n",
            "resetting env. episode 900.000000, reward total was -20.000000. running mean: -20.181659\n",
            "resetting env. episode 901.000000, reward total was -21.000000. running mean: -20.189842\n",
            "resetting env. episode 902.000000, reward total was -20.000000. running mean: -20.187944\n",
            "resetting env. episode 903.000000, reward total was -20.000000. running mean: -20.186064\n",
            "resetting env. episode 904.000000, reward total was -21.000000. running mean: -20.194204\n",
            "resetting env. episode 905.000000, reward total was -21.000000. running mean: -20.202262\n",
            "resetting env. episode 906.000000, reward total was -21.000000. running mean: -20.210239\n",
            "resetting env. episode 907.000000, reward total was -21.000000. running mean: -20.218137\n",
            "resetting env. episode 908.000000, reward total was -21.000000. running mean: -20.225955\n",
            "resetting env. episode 909.000000, reward total was -17.000000. running mean: -20.193696\n",
            "resetting env. episode 910.000000, reward total was -19.000000. running mean: -20.181759\n",
            "resetting env. episode 911.000000, reward total was -21.000000. running mean: -20.189941\n",
            "resetting env. episode 912.000000, reward total was -21.000000. running mean: -20.198042\n",
            "resetting env. episode 913.000000, reward total was -19.000000. running mean: -20.186061\n",
            "resetting env. episode 914.000000, reward total was -21.000000. running mean: -20.194201\n",
            "resetting env. episode 915.000000, reward total was -20.000000. running mean: -20.192259\n",
            "resetting env. episode 916.000000, reward total was -21.000000. running mean: -20.200336\n",
            "resetting env. episode 917.000000, reward total was -21.000000. running mean: -20.208333\n",
            "resetting env. episode 918.000000, reward total was -20.000000. running mean: -20.206249\n",
            "resetting env. episode 919.000000, reward total was -21.000000. running mean: -20.214187\n",
            "resetting env. episode 920.000000, reward total was -18.000000. running mean: -20.192045\n",
            "resetting env. episode 921.000000, reward total was -20.000000. running mean: -20.190125\n",
            "resetting env. episode 922.000000, reward total was -20.000000. running mean: -20.188223\n",
            "resetting env. episode 923.000000, reward total was -21.000000. running mean: -20.196341\n",
            "resetting env. episode 924.000000, reward total was -20.000000. running mean: -20.194378\n",
            "resetting env. episode 925.000000, reward total was -19.000000. running mean: -20.182434\n",
            "resetting env. episode 926.000000, reward total was -21.000000. running mean: -20.190610\n",
            "resetting env. episode 927.000000, reward total was -21.000000. running mean: -20.198703\n",
            "resetting env. episode 928.000000, reward total was -19.000000. running mean: -20.186716\n",
            "resetting env. episode 929.000000, reward total was -20.000000. running mean: -20.184849\n",
            "resetting env. episode 930.000000, reward total was -21.000000. running mean: -20.193001\n",
            "resetting env. episode 931.000000, reward total was -18.000000. running mean: -20.171071\n",
            "resetting env. episode 932.000000, reward total was -21.000000. running mean: -20.179360\n",
            "resetting env. episode 933.000000, reward total was -21.000000. running mean: -20.187566\n",
            "resetting env. episode 934.000000, reward total was -21.000000. running mean: -20.195691\n",
            "resetting env. episode 935.000000, reward total was -21.000000. running mean: -20.203734\n",
            "resetting env. episode 936.000000, reward total was -20.000000. running mean: -20.201697\n",
            "resetting env. episode 937.000000, reward total was -20.000000. running mean: -20.199680\n",
            "resetting env. episode 938.000000, reward total was -19.000000. running mean: -20.187683\n",
            "resetting env. episode 939.000000, reward total was -20.000000. running mean: -20.185806\n",
            "resetting env. episode 940.000000, reward total was -21.000000. running mean: -20.193948\n",
            "resetting env. episode 941.000000, reward total was -20.000000. running mean: -20.192008\n",
            "resetting env. episode 942.000000, reward total was -20.000000. running mean: -20.190088\n",
            "resetting env. episode 943.000000, reward total was -19.000000. running mean: -20.178187\n",
            "resetting env. episode 944.000000, reward total was -21.000000. running mean: -20.186406\n",
            "resetting env. episode 945.000000, reward total was -19.000000. running mean: -20.174542\n",
            "resetting env. episode 946.000000, reward total was -20.000000. running mean: -20.172796\n",
            "resetting env. episode 947.000000, reward total was -21.000000. running mean: -20.181068\n",
            "resetting env. episode 948.000000, reward total was -20.000000. running mean: -20.179257\n",
            "resetting env. episode 949.000000, reward total was -21.000000. running mean: -20.187465\n",
            "resetting env. episode 950.000000, reward total was -19.000000. running mean: -20.175590\n",
            "resetting env. episode 951.000000, reward total was -21.000000. running mean: -20.183834\n",
            "resetting env. episode 952.000000, reward total was -21.000000. running mean: -20.191996\n",
            "resetting env. episode 953.000000, reward total was -21.000000. running mean: -20.200076\n",
            "resetting env. episode 954.000000, reward total was -19.000000. running mean: -20.188075\n",
            "resetting env. episode 955.000000, reward total was -20.000000. running mean: -20.186195\n",
            "resetting env. episode 956.000000, reward total was -18.000000. running mean: -20.164333\n",
            "resetting env. episode 957.000000, reward total was -21.000000. running mean: -20.172689\n",
            "resetting env. episode 958.000000, reward total was -21.000000. running mean: -20.180962\n",
            "resetting env. episode 959.000000, reward total was -21.000000. running mean: -20.189153\n",
            "resetting env. episode 960.000000, reward total was -19.000000. running mean: -20.177261\n",
            "resetting env. episode 961.000000, reward total was -21.000000. running mean: -20.185489\n",
            "resetting env. episode 962.000000, reward total was -19.000000. running mean: -20.173634\n",
            "resetting env. episode 963.000000, reward total was -21.000000. running mean: -20.181897\n",
            "resetting env. episode 964.000000, reward total was -21.000000. running mean: -20.190078\n",
            "resetting env. episode 965.000000, reward total was -20.000000. running mean: -20.188178\n",
            "resetting env. episode 966.000000, reward total was -20.000000. running mean: -20.186296\n",
            "resetting env. episode 967.000000, reward total was -19.000000. running mean: -20.174433\n",
            "resetting env. episode 968.000000, reward total was -21.000000. running mean: -20.182689\n",
            "resetting env. episode 969.000000, reward total was -20.000000. running mean: -20.180862\n",
            "resetting env. episode 970.000000, reward total was -21.000000. running mean: -20.189053\n",
            "resetting env. episode 971.000000, reward total was -21.000000. running mean: -20.197163\n",
            "resetting env. episode 972.000000, reward total was -20.000000. running mean: -20.195191\n",
            "resetting env. episode 973.000000, reward total was -20.000000. running mean: -20.193239\n",
            "resetting env. episode 974.000000, reward total was -21.000000. running mean: -20.201307\n",
            "resetting env. episode 975.000000, reward total was -21.000000. running mean: -20.209294\n",
            "resetting env. episode 976.000000, reward total was -21.000000. running mean: -20.217201\n",
            "resetting env. episode 977.000000, reward total was -19.000000. running mean: -20.205029\n",
            "resetting env. episode 978.000000, reward total was -21.000000. running mean: -20.212978\n",
            "resetting env. episode 979.000000, reward total was -21.000000. running mean: -20.220849\n",
            "resetting env. episode 980.000000, reward total was -20.000000. running mean: -20.218640\n",
            "resetting env. episode 981.000000, reward total was -21.000000. running mean: -20.226454\n",
            "resetting env. episode 982.000000, reward total was -21.000000. running mean: -20.234189\n",
            "resetting env. episode 983.000000, reward total was -21.000000. running mean: -20.241847\n",
            "resetting env. episode 984.000000, reward total was -20.000000. running mean: -20.239429\n",
            "resetting env. episode 985.000000, reward total was -21.000000. running mean: -20.247034\n",
            "resetting env. episode 986.000000, reward total was -21.000000. running mean: -20.254564\n",
            "resetting env. episode 987.000000, reward total was -20.000000. running mean: -20.252018\n",
            "resetting env. episode 988.000000, reward total was -21.000000. running mean: -20.259498\n",
            "resetting env. episode 989.000000, reward total was -20.000000. running mean: -20.256903\n",
            "resetting env. episode 990.000000, reward total was -19.000000. running mean: -20.244334\n",
            "resetting env. episode 991.000000, reward total was -19.000000. running mean: -20.231891\n",
            "resetting env. episode 992.000000, reward total was -20.000000. running mean: -20.229572\n",
            "resetting env. episode 993.000000, reward total was -21.000000. running mean: -20.237276\n",
            "resetting env. episode 994.000000, reward total was -21.000000. running mean: -20.244904\n",
            "resetting env. episode 995.000000, reward total was -21.000000. running mean: -20.252454\n",
            "resetting env. episode 996.000000, reward total was -20.000000. running mean: -20.249930\n",
            "resetting env. episode 997.000000, reward total was -18.000000. running mean: -20.227431\n",
            "resetting env. episode 998.000000, reward total was -20.000000. running mean: -20.225156\n",
            "resetting env. episode 999.000000, reward total was -18.000000. running mean: -20.202905\n",
            "resetting env. episode 1000.000000, reward total was -21.000000. running mean: -20.210876\n",
            "resetting env. episode 1001.000000, reward total was -20.000000. running mean: -20.208767\n",
            "resetting env. episode 1002.000000, reward total was -17.000000. running mean: -20.176679\n",
            "resetting env. episode 1003.000000, reward total was -21.000000. running mean: -20.184913\n",
            "resetting env. episode 1004.000000, reward total was -19.000000. running mean: -20.173063\n",
            "resetting env. episode 1005.000000, reward total was -19.000000. running mean: -20.161333\n",
            "resetting env. episode 1006.000000, reward total was -20.000000. running mean: -20.159719\n",
            "resetting env. episode 1007.000000, reward total was -20.000000. running mean: -20.158122\n",
            "resetting env. episode 1008.000000, reward total was -21.000000. running mean: -20.166541\n",
            "resetting env. episode 1009.000000, reward total was -20.000000. running mean: -20.164876\n",
            "resetting env. episode 1010.000000, reward total was -21.000000. running mean: -20.173227\n",
            "resetting env. episode 1011.000000, reward total was -21.000000. running mean: -20.181495\n",
            "resetting env. episode 1012.000000, reward total was -20.000000. running mean: -20.179680\n",
            "resetting env. episode 1013.000000, reward total was -21.000000. running mean: -20.187883\n",
            "resetting env. episode 1014.000000, reward total was -21.000000. running mean: -20.196004\n",
            "resetting env. episode 1015.000000, reward total was -20.000000. running mean: -20.194044\n",
            "resetting env. episode 1016.000000, reward total was -21.000000. running mean: -20.202104\n",
            "resetting env. episode 1017.000000, reward total was -21.000000. running mean: -20.210082\n",
            "resetting env. episode 1018.000000, reward total was -19.000000. running mean: -20.197982\n",
            "resetting env. episode 1019.000000, reward total was -21.000000. running mean: -20.206002\n",
            "resetting env. episode 1020.000000, reward total was -21.000000. running mean: -20.213942\n",
            "resetting env. episode 1021.000000, reward total was -21.000000. running mean: -20.221802\n",
            "resetting env. episode 1022.000000, reward total was -20.000000. running mean: -20.219584\n",
            "resetting env. episode 1023.000000, reward total was -21.000000. running mean: -20.227389\n",
            "resetting env. episode 1024.000000, reward total was -21.000000. running mean: -20.235115\n",
            "resetting env. episode 1025.000000, reward total was -20.000000. running mean: -20.232764\n",
            "resetting env. episode 1026.000000, reward total was -21.000000. running mean: -20.240436\n",
            "resetting env. episode 1027.000000, reward total was -21.000000. running mean: -20.248032\n",
            "resetting env. episode 1028.000000, reward total was -21.000000. running mean: -20.255551\n",
            "resetting env. episode 1029.000000, reward total was -21.000000. running mean: -20.262996\n",
            "resetting env. episode 1030.000000, reward total was -21.000000. running mean: -20.270366\n",
            "resetting env. episode 1031.000000, reward total was -18.000000. running mean: -20.247662\n",
            "resetting env. episode 1032.000000, reward total was -20.000000. running mean: -20.245185\n",
            "resetting env. episode 1033.000000, reward total was -21.000000. running mean: -20.252734\n",
            "resetting env. episode 1034.000000, reward total was -21.000000. running mean: -20.260206\n",
            "resetting env. episode 1035.000000, reward total was -21.000000. running mean: -20.267604\n",
            "resetting env. episode 1036.000000, reward total was -20.000000. running mean: -20.264928\n",
            "resetting env. episode 1037.000000, reward total was -21.000000. running mean: -20.272279\n",
            "resetting env. episode 1038.000000, reward total was -19.000000. running mean: -20.259556\n",
            "resetting env. episode 1039.000000, reward total was -20.000000. running mean: -20.256961\n",
            "resetting env. episode 1040.000000, reward total was -20.000000. running mean: -20.254391\n",
            "resetting env. episode 1041.000000, reward total was -20.000000. running mean: -20.251847\n",
            "resetting env. episode 1042.000000, reward total was -21.000000. running mean: -20.259329\n",
            "resetting env. episode 1043.000000, reward total was -21.000000. running mean: -20.266735\n",
            "resetting env. episode 1044.000000, reward total was -18.000000. running mean: -20.244068\n",
            "resetting env. episode 1045.000000, reward total was -21.000000. running mean: -20.251627\n",
            "resetting env. episode 1046.000000, reward total was -21.000000. running mean: -20.259111\n",
            "resetting env. episode 1047.000000, reward total was -21.000000. running mean: -20.266520\n",
            "resetting env. episode 1048.000000, reward total was -18.000000. running mean: -20.243855\n",
            "resetting env. episode 1049.000000, reward total was -20.000000. running mean: -20.241416\n",
            "resetting env. episode 1050.000000, reward total was -21.000000. running mean: -20.249002\n",
            "resetting env. episode 1051.000000, reward total was -18.000000. running mean: -20.226512\n",
            "resetting env. episode 1052.000000, reward total was -20.000000. running mean: -20.224247\n",
            "resetting env. episode 1053.000000, reward total was -19.000000. running mean: -20.212004\n",
            "resetting env. episode 1054.000000, reward total was -21.000000. running mean: -20.219884\n",
            "resetting env. episode 1055.000000, reward total was -21.000000. running mean: -20.227685\n",
            "resetting env. episode 1056.000000, reward total was -20.000000. running mean: -20.225409\n",
            "resetting env. episode 1057.000000, reward total was -19.000000. running mean: -20.213155\n",
            "resetting env. episode 1058.000000, reward total was -20.000000. running mean: -20.211023\n",
            "resetting env. episode 1059.000000, reward total was -20.000000. running mean: -20.208913\n",
            "resetting env. episode 1060.000000, reward total was -21.000000. running mean: -20.216824\n",
            "resetting env. episode 1061.000000, reward total was -20.000000. running mean: -20.214655\n",
            "resetting env. episode 1062.000000, reward total was -20.000000. running mean: -20.212509\n",
            "resetting env. episode 1063.000000, reward total was -20.000000. running mean: -20.210384\n",
            "resetting env. episode 1064.000000, reward total was -21.000000. running mean: -20.218280\n",
            "resetting env. episode 1065.000000, reward total was -21.000000. running mean: -20.226097\n",
            "resetting env. episode 1066.000000, reward total was -20.000000. running mean: -20.223836\n",
            "resetting env. episode 1067.000000, reward total was -18.000000. running mean: -20.201598\n",
            "resetting env. episode 1068.000000, reward total was -19.000000. running mean: -20.189582\n",
            "resetting env. episode 1069.000000, reward total was -20.000000. running mean: -20.187686\n",
            "resetting env. episode 1070.000000, reward total was -21.000000. running mean: -20.195809\n",
            "resetting env. episode 1071.000000, reward total was -20.000000. running mean: -20.193851\n",
            "resetting env. episode 1072.000000, reward total was -21.000000. running mean: -20.201912\n",
            "resetting env. episode 1073.000000, reward total was -15.000000. running mean: -20.149893\n",
            "resetting env. episode 1074.000000, reward total was -21.000000. running mean: -20.158394\n",
            "resetting env. episode 1075.000000, reward total was -21.000000. running mean: -20.166810\n",
            "resetting env. episode 1076.000000, reward total was -21.000000. running mean: -20.175142\n",
            "resetting env. episode 1077.000000, reward total was -17.000000. running mean: -20.143391\n",
            "resetting env. episode 1078.000000, reward total was -21.000000. running mean: -20.151957\n",
            "resetting env. episode 1079.000000, reward total was -21.000000. running mean: -20.160437\n",
            "resetting env. episode 1080.000000, reward total was -21.000000. running mean: -20.168833\n",
            "resetting env. episode 1081.000000, reward total was -21.000000. running mean: -20.177145\n",
            "resetting env. episode 1082.000000, reward total was -21.000000. running mean: -20.185373\n",
            "resetting env. episode 1083.000000, reward total was -21.000000. running mean: -20.193520\n",
            "resetting env. episode 1084.000000, reward total was -19.000000. running mean: -20.181584\n",
            "resetting env. episode 1085.000000, reward total was -21.000000. running mean: -20.189769\n",
            "resetting env. episode 1086.000000, reward total was -21.000000. running mean: -20.197871\n",
            "resetting env. episode 1087.000000, reward total was -20.000000. running mean: -20.195892\n",
            "resetting env. episode 1088.000000, reward total was -21.000000. running mean: -20.203933\n",
            "resetting env. episode 1089.000000, reward total was -20.000000. running mean: -20.201894\n",
            "resetting env. episode 1090.000000, reward total was -20.000000. running mean: -20.199875\n",
            "resetting env. episode 1091.000000, reward total was -21.000000. running mean: -20.207876\n",
            "resetting env. episode 1092.000000, reward total was -18.000000. running mean: -20.185797\n",
            "resetting env. episode 1093.000000, reward total was -21.000000. running mean: -20.193939\n",
            "resetting env. episode 1094.000000, reward total was -21.000000. running mean: -20.202000\n",
            "resetting env. episode 1095.000000, reward total was -20.000000. running mean: -20.199980\n",
            "resetting env. episode 1096.000000, reward total was -20.000000. running mean: -20.197980\n",
            "resetting env. episode 1097.000000, reward total was -21.000000. running mean: -20.206000\n",
            "resetting env. episode 1098.000000, reward total was -20.000000. running mean: -20.203940\n",
            "resetting env. episode 1099.000000, reward total was -21.000000. running mean: -20.211901\n",
            "resetting env. episode 1100.000000, reward total was -20.000000. running mean: -20.209782\n",
            "resetting env. episode 1101.000000, reward total was -21.000000. running mean: -20.217684\n",
            "resetting env. episode 1102.000000, reward total was -21.000000. running mean: -20.225507\n",
            "resetting env. episode 1103.000000, reward total was -21.000000. running mean: -20.233252\n",
            "resetting env. episode 1104.000000, reward total was -20.000000. running mean: -20.230920\n",
            "resetting env. episode 1105.000000, reward total was -21.000000. running mean: -20.238611\n",
            "resetting env. episode 1106.000000, reward total was -20.000000. running mean: -20.236224\n",
            "resetting env. episode 1107.000000, reward total was -21.000000. running mean: -20.243862\n",
            "resetting env. episode 1108.000000, reward total was -20.000000. running mean: -20.241424\n",
            "resetting env. episode 1109.000000, reward total was -20.000000. running mean: -20.239009\n",
            "resetting env. episode 1110.000000, reward total was -19.000000. running mean: -20.226619\n",
            "resetting env. episode 1111.000000, reward total was -21.000000. running mean: -20.234353\n",
            "resetting env. episode 1112.000000, reward total was -21.000000. running mean: -20.242010\n",
            "resetting env. episode 1113.000000, reward total was -20.000000. running mean: -20.239589\n",
            "resetting env. episode 1114.000000, reward total was -21.000000. running mean: -20.247194\n",
            "resetting env. episode 1115.000000, reward total was -20.000000. running mean: -20.244722\n",
            "resetting env. episode 1116.000000, reward total was -20.000000. running mean: -20.242274\n",
            "resetting env. episode 1117.000000, reward total was -21.000000. running mean: -20.249852\n",
            "resetting env. episode 1118.000000, reward total was -20.000000. running mean: -20.247353\n",
            "resetting env. episode 1119.000000, reward total was -21.000000. running mean: -20.254880\n",
            "resetting env. episode 1120.000000, reward total was -20.000000. running mean: -20.252331\n",
            "resetting env. episode 1121.000000, reward total was -21.000000. running mean: -20.259808\n",
            "resetting env. episode 1122.000000, reward total was -18.000000. running mean: -20.237209\n",
            "resetting env. episode 1123.000000, reward total was -21.000000. running mean: -20.244837\n",
            "resetting env. episode 1124.000000, reward total was -20.000000. running mean: -20.242389\n",
            "resetting env. episode 1125.000000, reward total was -21.000000. running mean: -20.249965\n",
            "resetting env. episode 1126.000000, reward total was -21.000000. running mean: -20.257465\n",
            "resetting env. episode 1127.000000, reward total was -20.000000. running mean: -20.254891\n",
            "resetting env. episode 1128.000000, reward total was -21.000000. running mean: -20.262342\n",
            "resetting env. episode 1129.000000, reward total was -21.000000. running mean: -20.269718\n",
            "resetting env. episode 1130.000000, reward total was -20.000000. running mean: -20.267021\n",
            "resetting env. episode 1131.000000, reward total was -21.000000. running mean: -20.274351\n",
            "resetting env. episode 1132.000000, reward total was -21.000000. running mean: -20.281608\n",
            "resetting env. episode 1133.000000, reward total was -19.000000. running mean: -20.268791\n",
            "resetting env. episode 1134.000000, reward total was -19.000000. running mean: -20.256104\n",
            "resetting env. episode 1135.000000, reward total was -21.000000. running mean: -20.263543\n",
            "resetting env. episode 1136.000000, reward total was -20.000000. running mean: -20.260907\n",
            "resetting env. episode 1137.000000, reward total was -19.000000. running mean: -20.248298\n",
            "resetting env. episode 1138.000000, reward total was -20.000000. running mean: -20.245815\n",
            "resetting env. episode 1139.000000, reward total was -20.000000. running mean: -20.243357\n",
            "resetting env. episode 1140.000000, reward total was -20.000000. running mean: -20.240923\n",
            "resetting env. episode 1141.000000, reward total was -21.000000. running mean: -20.248514\n",
            "resetting env. episode 1142.000000, reward total was -20.000000. running mean: -20.246029\n",
            "resetting env. episode 1143.000000, reward total was -20.000000. running mean: -20.243569\n",
            "resetting env. episode 1144.000000, reward total was -20.000000. running mean: -20.241133\n",
            "resetting env. episode 1145.000000, reward total was -19.000000. running mean: -20.228722\n",
            "resetting env. episode 1146.000000, reward total was -20.000000. running mean: -20.226434\n",
            "resetting env. episode 1147.000000, reward total was -20.000000. running mean: -20.224170\n",
            "resetting env. episode 1148.000000, reward total was -20.000000. running mean: -20.221928\n",
            "resetting env. episode 1149.000000, reward total was -20.000000. running mean: -20.219709\n",
            "resetting env. episode 1150.000000, reward total was -21.000000. running mean: -20.227512\n",
            "resetting env. episode 1151.000000, reward total was -19.000000. running mean: -20.215237\n",
            "resetting env. episode 1152.000000, reward total was -18.000000. running mean: -20.193085\n",
            "resetting env. episode 1153.000000, reward total was -20.000000. running mean: -20.191154\n",
            "resetting env. episode 1154.000000, reward total was -19.000000. running mean: -20.179242\n",
            "resetting env. episode 1155.000000, reward total was -20.000000. running mean: -20.177450\n",
            "resetting env. episode 1156.000000, reward total was -19.000000. running mean: -20.165675\n",
            "resetting env. episode 1157.000000, reward total was -21.000000. running mean: -20.174018\n",
            "resetting env. episode 1158.000000, reward total was -21.000000. running mean: -20.182278\n",
            "resetting env. episode 1159.000000, reward total was -20.000000. running mean: -20.180456\n",
            "resetting env. episode 1160.000000, reward total was -21.000000. running mean: -20.188651\n",
            "resetting env. episode 1161.000000, reward total was -21.000000. running mean: -20.196764\n",
            "resetting env. episode 1162.000000, reward total was -21.000000. running mean: -20.204797\n",
            "resetting env. episode 1163.000000, reward total was -20.000000. running mean: -20.202749\n",
            "resetting env. episode 1164.000000, reward total was -18.000000. running mean: -20.180721\n",
            "resetting env. episode 1165.000000, reward total was -21.000000. running mean: -20.188914\n",
            "resetting env. episode 1166.000000, reward total was -20.000000. running mean: -20.187025\n",
            "resetting env. episode 1167.000000, reward total was -19.000000. running mean: -20.175155\n",
            "resetting env. episode 1168.000000, reward total was -21.000000. running mean: -20.183403\n",
            "resetting env. episode 1169.000000, reward total was -20.000000. running mean: -20.181569\n",
            "resetting env. episode 1170.000000, reward total was -19.000000. running mean: -20.169753\n",
            "resetting env. episode 1171.000000, reward total was -20.000000. running mean: -20.168056\n",
            "resetting env. episode 1172.000000, reward total was -21.000000. running mean: -20.176375\n",
            "resetting env. episode 1173.000000, reward total was -21.000000. running mean: -20.184612\n",
            "resetting env. episode 1174.000000, reward total was -19.000000. running mean: -20.172766\n",
            "resetting env. episode 1175.000000, reward total was -20.000000. running mean: -20.171038\n",
            "resetting env. episode 1176.000000, reward total was -19.000000. running mean: -20.159327\n",
            "resetting env. episode 1177.000000, reward total was -20.000000. running mean: -20.157734\n",
            "resetting env. episode 1178.000000, reward total was -20.000000. running mean: -20.156157\n",
            "resetting env. episode 1179.000000, reward total was -21.000000. running mean: -20.164595\n",
            "resetting env. episode 1180.000000, reward total was -19.000000. running mean: -20.152949\n",
            "resetting env. episode 1181.000000, reward total was -21.000000. running mean: -20.161420\n",
            "resetting env. episode 1182.000000, reward total was -20.000000. running mean: -20.159806\n",
            "resetting env. episode 1183.000000, reward total was -19.000000. running mean: -20.148208\n",
            "resetting env. episode 1184.000000, reward total was -21.000000. running mean: -20.156726\n",
            "resetting env. episode 1185.000000, reward total was -20.000000. running mean: -20.155158\n",
            "resetting env. episode 1186.000000, reward total was -21.000000. running mean: -20.163607\n",
            "resetting env. episode 1187.000000, reward total was -21.000000. running mean: -20.171971\n",
            "resetting env. episode 1188.000000, reward total was -21.000000. running mean: -20.180251\n",
            "resetting env. episode 1189.000000, reward total was -21.000000. running mean: -20.188448\n",
            "resetting env. episode 1190.000000, reward total was -20.000000. running mean: -20.186564\n",
            "resetting env. episode 1191.000000, reward total was -21.000000. running mean: -20.194698\n",
            "resetting env. episode 1192.000000, reward total was -20.000000. running mean: -20.192751\n",
            "resetting env. episode 1193.000000, reward total was -21.000000. running mean: -20.200824\n",
            "resetting env. episode 1194.000000, reward total was -20.000000. running mean: -20.198816\n",
            "resetting env. episode 1195.000000, reward total was -21.000000. running mean: -20.206827\n",
            "resetting env. episode 1196.000000, reward total was -20.000000. running mean: -20.204759\n",
            "resetting env. episode 1197.000000, reward total was -20.000000. running mean: -20.202712\n",
            "resetting env. episode 1198.000000, reward total was -21.000000. running mean: -20.210684\n",
            "resetting env. episode 1199.000000, reward total was -20.000000. running mean: -20.208578\n",
            "resetting env. episode 1200.000000, reward total was -21.000000. running mean: -20.216492\n",
            "resetting env. episode 1201.000000, reward total was -19.000000. running mean: -20.204327\n",
            "resetting env. episode 1202.000000, reward total was -21.000000. running mean: -20.212284\n",
            "resetting env. episode 1203.000000, reward total was -19.000000. running mean: -20.200161\n",
            "resetting env. episode 1204.000000, reward total was -21.000000. running mean: -20.208159\n",
            "resetting env. episode 1205.000000, reward total was -20.000000. running mean: -20.206078\n",
            "resetting env. episode 1206.000000, reward total was -19.000000. running mean: -20.194017\n",
            "resetting env. episode 1207.000000, reward total was -21.000000. running mean: -20.202077\n",
            "resetting env. episode 1208.000000, reward total was -18.000000. running mean: -20.180056\n",
            "resetting env. episode 1209.000000, reward total was -21.000000. running mean: -20.188255\n",
            "resetting env. episode 1210.000000, reward total was -21.000000. running mean: -20.196373\n",
            "resetting env. episode 1211.000000, reward total was -20.000000. running mean: -20.194409\n",
            "resetting env. episode 1212.000000, reward total was -18.000000. running mean: -20.172465\n",
            "resetting env. episode 1213.000000, reward total was -21.000000. running mean: -20.180740\n",
            "resetting env. episode 1214.000000, reward total was -19.000000. running mean: -20.168933\n",
            "resetting env. episode 1215.000000, reward total was -20.000000. running mean: -20.167244\n",
            "resetting env. episode 1216.000000, reward total was -20.000000. running mean: -20.165571\n",
            "resetting env. episode 1217.000000, reward total was -20.000000. running mean: -20.163915\n",
            "resetting env. episode 1218.000000, reward total was -20.000000. running mean: -20.162276\n",
            "resetting env. episode 1219.000000, reward total was -21.000000. running mean: -20.170653\n",
            "resetting env. episode 1220.000000, reward total was -20.000000. running mean: -20.168947\n",
            "resetting env. episode 1221.000000, reward total was -18.000000. running mean: -20.147257\n",
            "resetting env. episode 1222.000000, reward total was -20.000000. running mean: -20.145785\n",
            "resetting env. episode 1223.000000, reward total was -20.000000. running mean: -20.144327\n",
            "resetting env. episode 1224.000000, reward total was -21.000000. running mean: -20.152884\n",
            "resetting env. episode 1225.000000, reward total was -20.000000. running mean: -20.151355\n",
            "resetting env. episode 1226.000000, reward total was -19.000000. running mean: -20.139841\n",
            "resetting env. episode 1227.000000, reward total was -21.000000. running mean: -20.148443\n",
            "resetting env. episode 1228.000000, reward total was -20.000000. running mean: -20.146959\n",
            "resetting env. episode 1229.000000, reward total was -21.000000. running mean: -20.155489\n",
            "resetting env. episode 1230.000000, reward total was -21.000000. running mean: -20.163934\n",
            "resetting env. episode 1231.000000, reward total was -20.000000. running mean: -20.162295\n",
            "resetting env. episode 1232.000000, reward total was -19.000000. running mean: -20.150672\n",
            "resetting env. episode 1233.000000, reward total was -20.000000. running mean: -20.149165\n",
            "resetting env. episode 1234.000000, reward total was -21.000000. running mean: -20.157673\n",
            "resetting env. episode 1235.000000, reward total was -20.000000. running mean: -20.156097\n",
            "resetting env. episode 1236.000000, reward total was -18.000000. running mean: -20.134536\n",
            "resetting env. episode 1237.000000, reward total was -20.000000. running mean: -20.133190\n",
            "resetting env. episode 1238.000000, reward total was -21.000000. running mean: -20.141858\n",
            "resetting env. episode 1239.000000, reward total was -20.000000. running mean: -20.140440\n",
            "resetting env. episode 1240.000000, reward total was -19.000000. running mean: -20.129035\n",
            "resetting env. episode 1241.000000, reward total was -21.000000. running mean: -20.137745\n",
            "resetting env. episode 1242.000000, reward total was -21.000000. running mean: -20.146368\n",
            "resetting env. episode 1243.000000, reward total was -20.000000. running mean: -20.144904\n",
            "resetting env. episode 1244.000000, reward total was -20.000000. running mean: -20.143455\n",
            "resetting env. episode 1245.000000, reward total was -21.000000. running mean: -20.152020\n",
            "resetting env. episode 1246.000000, reward total was -21.000000. running mean: -20.160500\n",
            "resetting env. episode 1247.000000, reward total was -21.000000. running mean: -20.168895\n",
            "resetting env. episode 1248.000000, reward total was -20.000000. running mean: -20.167206\n",
            "resetting env. episode 1249.000000, reward total was -21.000000. running mean: -20.175534\n",
            "resetting env. episode 1250.000000, reward total was -20.000000. running mean: -20.173779\n",
            "resetting env. episode 1251.000000, reward total was -18.000000. running mean: -20.152041\n",
            "resetting env. episode 1252.000000, reward total was -20.000000. running mean: -20.150521\n",
            "resetting env. episode 1253.000000, reward total was -18.000000. running mean: -20.129015\n",
            "resetting env. episode 1254.000000, reward total was -19.000000. running mean: -20.117725\n",
            "resetting env. episode 1255.000000, reward total was -19.000000. running mean: -20.106548\n",
            "resetting env. episode 1256.000000, reward total was -21.000000. running mean: -20.115483\n",
            "resetting env. episode 1257.000000, reward total was -20.000000. running mean: -20.114328\n",
            "resetting env. episode 1258.000000, reward total was -20.000000. running mean: -20.113184\n",
            "resetting env. episode 1259.000000, reward total was -21.000000. running mean: -20.122053\n",
            "resetting env. episode 1260.000000, reward total was -21.000000. running mean: -20.130832\n",
            "resetting env. episode 1261.000000, reward total was -20.000000. running mean: -20.129524\n",
            "resetting env. episode 1262.000000, reward total was -21.000000. running mean: -20.138229\n",
            "resetting env. episode 1263.000000, reward total was -19.000000. running mean: -20.126846\n",
            "resetting env. episode 1264.000000, reward total was -20.000000. running mean: -20.125578\n",
            "resetting env. episode 1265.000000, reward total was -21.000000. running mean: -20.134322\n",
            "resetting env. episode 1266.000000, reward total was -21.000000. running mean: -20.142979\n",
            "resetting env. episode 1267.000000, reward total was -20.000000. running mean: -20.141549\n",
            "resetting env. episode 1268.000000, reward total was -20.000000. running mean: -20.140133\n",
            "resetting env. episode 1269.000000, reward total was -21.000000. running mean: -20.148732\n",
            "resetting env. episode 1270.000000, reward total was -21.000000. running mean: -20.157245\n",
            "resetting env. episode 1271.000000, reward total was -20.000000. running mean: -20.155672\n",
            "resetting env. episode 1272.000000, reward total was -20.000000. running mean: -20.154116\n",
            "resetting env. episode 1273.000000, reward total was -21.000000. running mean: -20.162575\n",
            "resetting env. episode 1274.000000, reward total was -20.000000. running mean: -20.160949\n",
            "resetting env. episode 1275.000000, reward total was -21.000000. running mean: -20.169339\n",
            "resetting env. episode 1276.000000, reward total was -21.000000. running mean: -20.177646\n",
            "resetting env. episode 1277.000000, reward total was -20.000000. running mean: -20.175869\n",
            "resetting env. episode 1278.000000, reward total was -21.000000. running mean: -20.184111\n",
            "resetting env. episode 1279.000000, reward total was -20.000000. running mean: -20.182270\n",
            "resetting env. episode 1280.000000, reward total was -21.000000. running mean: -20.190447\n",
            "resetting env. episode 1281.000000, reward total was -21.000000. running mean: -20.198542\n",
            "resetting env. episode 1282.000000, reward total was -18.000000. running mean: -20.176557\n",
            "resetting env. episode 1283.000000, reward total was -21.000000. running mean: -20.184791\n",
            "resetting env. episode 1284.000000, reward total was -20.000000. running mean: -20.182944\n",
            "resetting env. episode 1285.000000, reward total was -21.000000. running mean: -20.191114\n",
            "resetting env. episode 1286.000000, reward total was -21.000000. running mean: -20.199203\n",
            "resetting env. episode 1287.000000, reward total was -21.000000. running mean: -20.207211\n",
            "resetting env. episode 1288.000000, reward total was -19.000000. running mean: -20.195139\n",
            "resetting env. episode 1289.000000, reward total was -21.000000. running mean: -20.203187\n",
            "resetting env. episode 1290.000000, reward total was -21.000000. running mean: -20.211156\n",
            "resetting env. episode 1291.000000, reward total was -21.000000. running mean: -20.219044\n",
            "resetting env. episode 1292.000000, reward total was -18.000000. running mean: -20.196854\n",
            "resetting env. episode 1293.000000, reward total was -21.000000. running mean: -20.204885\n",
            "resetting env. episode 1294.000000, reward total was -21.000000. running mean: -20.212836\n",
            "resetting env. episode 1295.000000, reward total was -21.000000. running mean: -20.220708\n",
            "resetting env. episode 1296.000000, reward total was -18.000000. running mean: -20.198501\n",
            "resetting env. episode 1297.000000, reward total was -21.000000. running mean: -20.206516\n",
            "resetting env. episode 1298.000000, reward total was -21.000000. running mean: -20.214451\n",
            "resetting env. episode 1299.000000, reward total was -20.000000. running mean: -20.212306\n",
            "resetting env. episode 1300.000000, reward total was -19.000000. running mean: -20.200183\n",
            "resetting env. episode 1301.000000, reward total was -20.000000. running mean: -20.198181\n",
            "resetting env. episode 1302.000000, reward total was -20.000000. running mean: -20.196199\n",
            "resetting env. episode 1303.000000, reward total was -20.000000. running mean: -20.194237\n",
            "resetting env. episode 1304.000000, reward total was -20.000000. running mean: -20.192295\n",
            "resetting env. episode 1305.000000, reward total was -20.000000. running mean: -20.190372\n",
            "resetting env. episode 1306.000000, reward total was -20.000000. running mean: -20.188468\n",
            "resetting env. episode 1307.000000, reward total was -20.000000. running mean: -20.186584\n",
            "resetting env. episode 1308.000000, reward total was -20.000000. running mean: -20.184718\n",
            "resetting env. episode 1309.000000, reward total was -20.000000. running mean: -20.182871\n",
            "resetting env. episode 1310.000000, reward total was -19.000000. running mean: -20.171042\n",
            "resetting env. episode 1311.000000, reward total was -20.000000. running mean: -20.169332\n",
            "resetting env. episode 1312.000000, reward total was -20.000000. running mean: -20.167638\n",
            "resetting env. episode 1313.000000, reward total was -20.000000. running mean: -20.165962\n",
            "resetting env. episode 1314.000000, reward total was -20.000000. running mean: -20.164302\n",
            "resetting env. episode 1315.000000, reward total was -21.000000. running mean: -20.172659\n",
            "resetting env. episode 1316.000000, reward total was -20.000000. running mean: -20.170933\n",
            "resetting env. episode 1317.000000, reward total was -20.000000. running mean: -20.169223\n",
            "resetting env. episode 1318.000000, reward total was -21.000000. running mean: -20.177531\n",
            "resetting env. episode 1319.000000, reward total was -21.000000. running mean: -20.185756\n",
            "resetting env. episode 1320.000000, reward total was -18.000000. running mean: -20.163898\n",
            "resetting env. episode 1321.000000, reward total was -21.000000. running mean: -20.172259\n",
            "resetting env. episode 1322.000000, reward total was -20.000000. running mean: -20.170537\n",
            "resetting env. episode 1323.000000, reward total was -21.000000. running mean: -20.178831\n",
            "resetting env. episode 1324.000000, reward total was -20.000000. running mean: -20.177043\n",
            "resetting env. episode 1325.000000, reward total was -21.000000. running mean: -20.185272\n",
            "resetting env. episode 1326.000000, reward total was -21.000000. running mean: -20.193420\n",
            "resetting env. episode 1327.000000, reward total was -21.000000. running mean: -20.201486\n",
            "resetting env. episode 1328.000000, reward total was -21.000000. running mean: -20.209471\n",
            "resetting env. episode 1329.000000, reward total was -18.000000. running mean: -20.187376\n",
            "resetting env. episode 1330.000000, reward total was -18.000000. running mean: -20.165502\n",
            "resetting env. episode 1331.000000, reward total was -21.000000. running mean: -20.173847\n",
            "resetting env. episode 1332.000000, reward total was -20.000000. running mean: -20.172109\n",
            "resetting env. episode 1333.000000, reward total was -21.000000. running mean: -20.180388\n",
            "resetting env. episode 1334.000000, reward total was -18.000000. running mean: -20.158584\n",
            "resetting env. episode 1335.000000, reward total was -21.000000. running mean: -20.166998\n",
            "resetting env. episode 1336.000000, reward total was -21.000000. running mean: -20.175328\n",
            "resetting env. episode 1337.000000, reward total was -21.000000. running mean: -20.183575\n",
            "resetting env. episode 1338.000000, reward total was -21.000000. running mean: -20.191739\n",
            "resetting env. episode 1339.000000, reward total was -20.000000. running mean: -20.189822\n",
            "resetting env. episode 1340.000000, reward total was -21.000000. running mean: -20.197923\n",
            "resetting env. episode 1341.000000, reward total was -21.000000. running mean: -20.205944\n",
            "resetting env. episode 1342.000000, reward total was -21.000000. running mean: -20.213885\n",
            "resetting env. episode 1343.000000, reward total was -20.000000. running mean: -20.211746\n",
            "resetting env. episode 1344.000000, reward total was -21.000000. running mean: -20.219628\n",
            "resetting env. episode 1345.000000, reward total was -21.000000. running mean: -20.227432\n",
            "resetting env. episode 1346.000000, reward total was -21.000000. running mean: -20.235158\n",
            "resetting env. episode 1347.000000, reward total was -20.000000. running mean: -20.232806\n",
            "resetting env. episode 1348.000000, reward total was -20.000000. running mean: -20.230478\n",
            "resetting env. episode 1349.000000, reward total was -20.000000. running mean: -20.228173\n",
            "resetting env. episode 1350.000000, reward total was -21.000000. running mean: -20.235892\n",
            "resetting env. episode 1351.000000, reward total was -20.000000. running mean: -20.233533\n",
            "resetting env. episode 1352.000000, reward total was -20.000000. running mean: -20.231197\n",
            "resetting env. episode 1353.000000, reward total was -21.000000. running mean: -20.238885\n",
            "resetting env. episode 1354.000000, reward total was -20.000000. running mean: -20.236497\n",
            "resetting env. episode 1355.000000, reward total was -21.000000. running mean: -20.244132\n",
            "resetting env. episode 1356.000000, reward total was -21.000000. running mean: -20.251690\n",
            "resetting env. episode 1357.000000, reward total was -21.000000. running mean: -20.259173\n",
            "resetting env. episode 1358.000000, reward total was -21.000000. running mean: -20.266582\n",
            "resetting env. episode 1359.000000, reward total was -20.000000. running mean: -20.263916\n",
            "resetting env. episode 1360.000000, reward total was -20.000000. running mean: -20.261277\n",
            "resetting env. episode 1361.000000, reward total was -20.000000. running mean: -20.258664\n",
            "resetting env. episode 1362.000000, reward total was -21.000000. running mean: -20.266077\n",
            "resetting env. episode 1363.000000, reward total was -19.000000. running mean: -20.253416\n",
            "resetting env. episode 1364.000000, reward total was -21.000000. running mean: -20.260882\n",
            "resetting env. episode 1365.000000, reward total was -21.000000. running mean: -20.268273\n",
            "resetting env. episode 1366.000000, reward total was -20.000000. running mean: -20.265591\n",
            "resetting env. episode 1367.000000, reward total was -20.000000. running mean: -20.262935\n",
            "resetting env. episode 1368.000000, reward total was -21.000000. running mean: -20.270305\n",
            "resetting env. episode 1369.000000, reward total was -19.000000. running mean: -20.257602\n",
            "resetting env. episode 1370.000000, reward total was -21.000000. running mean: -20.265026\n",
            "resetting env. episode 1371.000000, reward total was -20.000000. running mean: -20.262376\n",
            "resetting env. episode 1372.000000, reward total was -21.000000. running mean: -20.269752\n",
            "resetting env. episode 1373.000000, reward total was -20.000000. running mean: -20.267055\n",
            "resetting env. episode 1374.000000, reward total was -19.000000. running mean: -20.254384\n",
            "resetting env. episode 1375.000000, reward total was -21.000000. running mean: -20.261840\n",
            "resetting env. episode 1376.000000, reward total was -21.000000. running mean: -20.269222\n",
            "resetting env. episode 1377.000000, reward total was -20.000000. running mean: -20.266530\n",
            "resetting env. episode 1378.000000, reward total was -20.000000. running mean: -20.263865\n",
            "resetting env. episode 1379.000000, reward total was -19.000000. running mean: -20.251226\n",
            "resetting env. episode 1380.000000, reward total was -19.000000. running mean: -20.238714\n",
            "resetting env. episode 1381.000000, reward total was -18.000000. running mean: -20.216326\n",
            "resetting env. episode 1382.000000, reward total was -21.000000. running mean: -20.224163\n",
            "resetting env. episode 1383.000000, reward total was -21.000000. running mean: -20.231922\n",
            "resetting env. episode 1384.000000, reward total was -20.000000. running mean: -20.229602\n",
            "resetting env. episode 1385.000000, reward total was -19.000000. running mean: -20.217306\n",
            "resetting env. episode 1386.000000, reward total was -21.000000. running mean: -20.225133\n",
            "resetting env. episode 1387.000000, reward total was -20.000000. running mean: -20.222882\n",
            "resetting env. episode 1388.000000, reward total was -19.000000. running mean: -20.210653\n",
            "resetting env. episode 1389.000000, reward total was -19.000000. running mean: -20.198547\n",
            "resetting env. episode 1390.000000, reward total was -20.000000. running mean: -20.196561\n",
            "resetting env. episode 1391.000000, reward total was -21.000000. running mean: -20.204596\n",
            "resetting env. episode 1392.000000, reward total was -18.000000. running mean: -20.182550\n",
            "resetting env. episode 1393.000000, reward total was -20.000000. running mean: -20.180724\n",
            "resetting env. episode 1394.000000, reward total was -20.000000. running mean: -20.178917\n",
            "resetting env. episode 1395.000000, reward total was -21.000000. running mean: -20.187128\n",
            "resetting env. episode 1396.000000, reward total was -21.000000. running mean: -20.195256\n",
            "resetting env. episode 1397.000000, reward total was -19.000000. running mean: -20.183304\n",
            "resetting env. episode 1398.000000, reward total was -21.000000. running mean: -20.191471\n",
            "resetting env. episode 1399.000000, reward total was -19.000000. running mean: -20.179556\n",
            "resetting env. episode 1400.000000, reward total was -21.000000. running mean: -20.187761\n",
            "resetting env. episode 1401.000000, reward total was -19.000000. running mean: -20.175883\n",
            "resetting env. episode 1402.000000, reward total was -20.000000. running mean: -20.174124\n",
            "resetting env. episode 1403.000000, reward total was -21.000000. running mean: -20.182383\n",
            "resetting env. episode 1404.000000, reward total was -20.000000. running mean: -20.180559\n",
            "resetting env. episode 1405.000000, reward total was -20.000000. running mean: -20.178753\n",
            "resetting env. episode 1406.000000, reward total was -20.000000. running mean: -20.176966\n",
            "resetting env. episode 1407.000000, reward total was -21.000000. running mean: -20.185196\n",
            "resetting env. episode 1408.000000, reward total was -20.000000. running mean: -20.183344\n",
            "resetting env. episode 1409.000000, reward total was -20.000000. running mean: -20.181511\n",
            "resetting env. episode 1410.000000, reward total was -21.000000. running mean: -20.189696\n",
            "resetting env. episode 1411.000000, reward total was -20.000000. running mean: -20.187799\n",
            "resetting env. episode 1412.000000, reward total was -21.000000. running mean: -20.195921\n",
            "resetting env. episode 1413.000000, reward total was -21.000000. running mean: -20.203962\n",
            "resetting env. episode 1414.000000, reward total was -21.000000. running mean: -20.211922\n",
            "resetting env. episode 1415.000000, reward total was -21.000000. running mean: -20.219803\n",
            "resetting env. episode 1416.000000, reward total was -21.000000. running mean: -20.227605\n",
            "resetting env. episode 1417.000000, reward total was -21.000000. running mean: -20.235329\n",
            "resetting env. episode 1418.000000, reward total was -21.000000. running mean: -20.242975\n",
            "resetting env. episode 1419.000000, reward total was -20.000000. running mean: -20.240546\n",
            "resetting env. episode 1420.000000, reward total was -21.000000. running mean: -20.248140\n",
            "resetting env. episode 1421.000000, reward total was -19.000000. running mean: -20.235659\n",
            "resetting env. episode 1422.000000, reward total was -20.000000. running mean: -20.233302\n",
            "resetting env. episode 1423.000000, reward total was -19.000000. running mean: -20.220969\n",
            "resetting env. episode 1424.000000, reward total was -21.000000. running mean: -20.228759\n",
            "resetting env. episode 1425.000000, reward total was -20.000000. running mean: -20.226472\n",
            "resetting env. episode 1426.000000, reward total was -19.000000. running mean: -20.214207\n",
            "resetting env. episode 1427.000000, reward total was -20.000000. running mean: -20.212065\n",
            "resetting env. episode 1428.000000, reward total was -21.000000. running mean: -20.219944\n",
            "resetting env. episode 1429.000000, reward total was -21.000000. running mean: -20.227745\n",
            "resetting env. episode 1430.000000, reward total was -19.000000. running mean: -20.215468\n",
            "resetting env. episode 1431.000000, reward total was -20.000000. running mean: -20.213313\n",
            "resetting env. episode 1432.000000, reward total was -21.000000. running mean: -20.221180\n",
            "resetting env. episode 1433.000000, reward total was -20.000000. running mean: -20.218968\n",
            "resetting env. episode 1434.000000, reward total was -18.000000. running mean: -20.196778\n",
            "resetting env. episode 1435.000000, reward total was -21.000000. running mean: -20.204810\n",
            "resetting env. episode 1436.000000, reward total was -21.000000. running mean: -20.212762\n",
            "resetting env. episode 1437.000000, reward total was -21.000000. running mean: -20.220635\n",
            "resetting env. episode 1438.000000, reward total was -21.000000. running mean: -20.228428\n",
            "resetting env. episode 1439.000000, reward total was -21.000000. running mean: -20.236144\n",
            "resetting env. episode 1440.000000, reward total was -18.000000. running mean: -20.213783\n",
            "resetting env. episode 1441.000000, reward total was -20.000000. running mean: -20.211645\n",
            "resetting env. episode 1442.000000, reward total was -21.000000. running mean: -20.219528\n",
            "resetting env. episode 1443.000000, reward total was -21.000000. running mean: -20.227333\n",
            "resetting env. episode 1444.000000, reward total was -20.000000. running mean: -20.225060\n",
            "resetting env. episode 1445.000000, reward total was -20.000000. running mean: -20.222809\n",
            "resetting env. episode 1446.000000, reward total was -21.000000. running mean: -20.230581\n",
            "resetting env. episode 1447.000000, reward total was -18.000000. running mean: -20.208275\n",
            "resetting env. episode 1448.000000, reward total was -21.000000. running mean: -20.216193\n",
            "resetting env. episode 1449.000000, reward total was -20.000000. running mean: -20.214031\n",
            "resetting env. episode 1450.000000, reward total was -20.000000. running mean: -20.211890\n",
            "resetting env. episode 1451.000000, reward total was -19.000000. running mean: -20.199771\n",
            "resetting env. episode 1452.000000, reward total was -20.000000. running mean: -20.197774\n",
            "resetting env. episode 1453.000000, reward total was -21.000000. running mean: -20.205796\n",
            "resetting env. episode 1454.000000, reward total was -21.000000. running mean: -20.213738\n",
            "resetting env. episode 1455.000000, reward total was -20.000000. running mean: -20.211601\n",
            "resetting env. episode 1456.000000, reward total was -21.000000. running mean: -20.219485\n",
            "resetting env. episode 1457.000000, reward total was -20.000000. running mean: -20.217290\n",
            "resetting env. episode 1458.000000, reward total was -19.000000. running mean: -20.205117\n",
            "resetting env. episode 1459.000000, reward total was -20.000000. running mean: -20.203066\n",
            "resetting env. episode 1460.000000, reward total was -21.000000. running mean: -20.211035\n",
            "resetting env. episode 1461.000000, reward total was -21.000000. running mean: -20.218925\n",
            "resetting env. episode 1462.000000, reward total was -21.000000. running mean: -20.226735\n",
            "resetting env. episode 1463.000000, reward total was -19.000000. running mean: -20.214468\n",
            "resetting env. episode 1464.000000, reward total was -18.000000. running mean: -20.192323\n",
            "resetting env. episode 1465.000000, reward total was -19.000000. running mean: -20.180400\n",
            "resetting env. episode 1466.000000, reward total was -21.000000. running mean: -20.188596\n",
            "resetting env. episode 1467.000000, reward total was -20.000000. running mean: -20.186710\n",
            "resetting env. episode 1468.000000, reward total was -21.000000. running mean: -20.194843\n",
            "resetting env. episode 1469.000000, reward total was -21.000000. running mean: -20.202895\n",
            "resetting env. episode 1470.000000, reward total was -20.000000. running mean: -20.200866\n",
            "resetting env. episode 1471.000000, reward total was -21.000000. running mean: -20.208857\n",
            "resetting env. episode 1472.000000, reward total was -20.000000. running mean: -20.206768\n",
            "resetting env. episode 1473.000000, reward total was -20.000000. running mean: -20.204701\n",
            "resetting env. episode 1474.000000, reward total was -20.000000. running mean: -20.202654\n",
            "resetting env. episode 1475.000000, reward total was -20.000000. running mean: -20.200627\n",
            "resetting env. episode 1476.000000, reward total was -20.000000. running mean: -20.198621\n",
            "resetting env. episode 1477.000000, reward total was -21.000000. running mean: -20.206635\n",
            "resetting env. episode 1478.000000, reward total was -19.000000. running mean: -20.194568\n",
            "resetting env. episode 1479.000000, reward total was -21.000000. running mean: -20.202623\n",
            "resetting env. episode 1480.000000, reward total was -21.000000. running mean: -20.210597\n",
            "resetting env. episode 1481.000000, reward total was -20.000000. running mean: -20.208491\n",
            "resetting env. episode 1482.000000, reward total was -21.000000. running mean: -20.216406\n",
            "resetting env. episode 1483.000000, reward total was -20.000000. running mean: -20.214242\n",
            "resetting env. episode 1484.000000, reward total was -21.000000. running mean: -20.222099\n",
            "resetting env. episode 1485.000000, reward total was -21.000000. running mean: -20.229878\n",
            "resetting env. episode 1486.000000, reward total was -21.000000. running mean: -20.237579\n",
            "resetting env. episode 1487.000000, reward total was -21.000000. running mean: -20.245204\n",
            "resetting env. episode 1488.000000, reward total was -21.000000. running mean: -20.252752\n",
            "resetting env. episode 1489.000000, reward total was -19.000000. running mean: -20.240224\n",
            "resetting env. episode 1490.000000, reward total was -19.000000. running mean: -20.227822\n",
            "resetting env. episode 1491.000000, reward total was -21.000000. running mean: -20.235544\n",
            "resetting env. episode 1492.000000, reward total was -19.000000. running mean: -20.223188\n",
            "resetting env. episode 1493.000000, reward total was -20.000000. running mean: -20.220956\n",
            "resetting env. episode 1494.000000, reward total was -21.000000. running mean: -20.228747\n",
            "resetting env. episode 1495.000000, reward total was -17.000000. running mean: -20.196459\n",
            "resetting env. episode 1496.000000, reward total was -21.000000. running mean: -20.204495\n",
            "resetting env. episode 1497.000000, reward total was -21.000000. running mean: -20.212450\n",
            "resetting env. episode 1498.000000, reward total was -19.000000. running mean: -20.200325\n",
            "resetting env. episode 1499.000000, reward total was -20.000000. running mean: -20.198322\n",
            "resetting env. episode 1500.000000, reward total was -21.000000. running mean: -20.206339\n",
            "CPU times: user 1h 12min 34s, sys: 33min 12s, total: 1h 45min 47s\n",
            "Wall time: 54min 38s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "w2NblmwDsL3y",
        "outputId": "5ccd238f-fc07-4770-c9c3-4b3859bc6072",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "cell_type": "code",
      "source": [
        "play_game(env, model)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -10.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}
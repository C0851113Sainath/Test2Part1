{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "C0851113_400neurons.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "cWACPRL869I4"
      },
      "cell_type": "code",
      "source": [
        "!pip install gym >/dev/null"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Os6feRY6ec_"
      },
      "cell_type": "code",
      "source": [
        "!pip install JSAnimation >/dev/null"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wotUOa_e6edP"
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    HTML(anim.to_jshtml())"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R66_INeZ9nYX"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 2: Playing Pong"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ngMhg3fB9aA",
        "outputId": "af8e96da-20c7-4584-b633-0ad2d8f8b538"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Requirement already satisfied: ale-py~=0.7.5 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.7.5)\n",
            "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.4.2)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (0.4.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "MtT2GyK_6edc",
        "outputId": "c028a5a5-ef41-4f16-9083-bb83443049d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "env = gym.make('Pong-v0')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "oRE6WmXQJ1Z0",
        "outputId": "1e25f8e8-7221-4875-cdf1-dd1fd1e83c23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "env.action_space"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "yl_9d4HFJ31W",
        "outputId": "24715188-7d71-4c59-a784-4f6a192e5566",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "env.observation_space"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "trwRXI-h6eeI",
        "outputId": "efb2c4b0-1afa-45fb-a6b3-e1768848f83f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# Run a demo of the environment\n",
        "observation = env.reset()\n",
        "cumulated_reward = 0\n",
        "\n",
        "frames = []\n",
        "for t in range(1000):\n",
        "#     print(observation)\n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    # very stupid agent, just makes a random action within the allowd action space\n",
        "    action = env.action_space.sample()\n",
        "#     print(\"Action: {}\".format(t+1))    \n",
        "    observation, reward, done, info = env.step(action)\n",
        "#     print(reward)\n",
        "    cumulated_reward += reward\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "        break\n",
        "print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "\n",
        "env.close()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -15.0\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "3zZTecVWLLes"
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(x): \n",
        "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
        "\n",
        "def prepro(I):\n",
        "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "  I = I[35:195] # crop\n",
        "  I = I[::2,::2,0] # downsample by factor of 2\n",
        "  I[I == 144] = 0 # erase background (background type 1)\n",
        "  I[I == 109] = 0 # erase background (background type 2)\n",
        "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  return I.astype(np.float).ravel()\n",
        "\n",
        "def policy_forward(x):\n",
        "  h = np.dot(model['W1'], x)\n",
        "  h[h<0] = 0 # ReLU nonlinearity\n",
        "  logp = np.dot(model['W2'], h)\n",
        "  p = sigmoid(logp)\n",
        "  return p, h # return probability of taking action 2, and hidden state\n",
        "\n",
        "def model_step(model, observation, prev_x):\n",
        "  # preprocess the observation, set input to network to be difference image\n",
        "  cur_x = prepro(observation)\n",
        "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "  prev_x = cur_x\n",
        "  \n",
        "  # forward the policy network and sample an action from the returned probability\n",
        "  aprob, _ = policy_forward(x)\n",
        "  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n",
        "  \n",
        "  return action, prev_x\n",
        "\n",
        "def play_game(env, model):\n",
        "  observation = env.reset()\n",
        "\n",
        "  frames = []\n",
        "  cumulated_reward = 0\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "\n",
        "  for t in range(1000):\n",
        "      frames.append(env.render(mode = 'rgb_array'))\n",
        "      action, prev_x = model_step(model, observation, prev_x)\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      cumulated_reward += reward\n",
        "      if done:\n",
        "          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "          break\n",
        "  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "  display_frames_as_gif(frames)\n",
        "  env.close()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6gWvZQ7AQLQt"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 3: Policy Gradient from Scratch"
      ]
    },
    {
      "metadata": {
        "id": "eqFm7hqcItWl"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# model initialization\n",
        "H = 400 # number of hidden layer neurons\n",
        "D = 80 * 80 # input dimensionality: 80x80 grid\n",
        "model = {}\n",
        "model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
        "model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
        "\n",
        "# import pickle\n",
        "# model = pickle.load(open('model.pkl', 'rb'))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TwjiwKisQM19"
      },
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 10 # every how many episodes to do a param update?\n",
        "# learning_rate = 1e-4\n",
        "learning_rate = 1e-4\n",
        " \n",
        "gamma = 0.99 # discount factor for reward\n",
        "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
        "  \n",
        "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
        "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
        "\n",
        "def discount_rewards(r):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "    running_add = running_add * gamma + r[t]\n",
        "    discounted_r[t] = running_add\n",
        "  return discounted_r\n",
        "\n",
        "def policy_backward(epx, eph, epdlogp):\n",
        "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
        "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
        "  dh = np.outer(epdlogp, model['W2'])\n",
        "  dh[eph <= 0] = 0 # backpro prelu\n",
        "  dW1 = np.dot(dh.T, epx)\n",
        "  return {'W1':dW1, 'W2':dW2}\n",
        "\n",
        "def train_model(env, model, total_episodes = 100):\n",
        "  hist = []\n",
        "  observation = env.reset()\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "  xs,hs,dlogps,drs = [],[],[],[]\n",
        "  running_reward = None\n",
        "  reward_sum = 0\n",
        "  episode_number = 0\n",
        "\n",
        "  while True:\n",
        "    # preprocess the observation, set input to network to be difference image\n",
        "    cur_x = prepro(observation)\n",
        "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "    prev_x = cur_x\n",
        "\n",
        "    # forward the policy network and sample an action from the returned probability\n",
        "    aprob, h = policy_forward(x)\n",
        "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
        "\n",
        "    # record various intermediates (needed later for backprop)\n",
        "    xs.append(x) # observation\n",
        "    hs.append(h) # hidden state\n",
        "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
        "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
        "\n",
        "    # step the environment and get new measurements\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    reward_sum += reward\n",
        "\n",
        "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
        "\n",
        "    if done: # an episode finished\n",
        "      episode_number += 1\n",
        "\n",
        "      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
        "      epx = np.vstack(xs)\n",
        "      eph = np.vstack(hs)\n",
        "      epdlogp = np.vstack(dlogps)\n",
        "      epr = np.vstack(drs)\n",
        "      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
        "\n",
        "      # compute the discounted reward backwards through time\n",
        "      discounted_epr = discount_rewards(epr)\n",
        "      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "      discounted_epr -= np.mean(discounted_epr)\n",
        "      discounted_epr /= np.std(discounted_epr)\n",
        "\n",
        "      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
        "      grad = policy_backward(epx, eph, epdlogp)\n",
        "      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
        "\n",
        "      # perform rmsprop parameter update every batch_size episodes\n",
        "      if episode_number % batch_size == 0:\n",
        "        for k,v in model.items():\n",
        "          g = grad_buffer[k] # gradient\n",
        "          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
        "          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
        "\n",
        "      # boring book-keeping\n",
        "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "      hist.append((episode_number, reward_sum, running_reward))\n",
        "      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
        "      reward_sum = 0\n",
        "      observation = env.reset() # reset env\n",
        "      prev_x = None\n",
        "      if episode_number == total_episodes: return hist\n",
        "\n",
        "      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
        "        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G6Ka_5Vl9Orm",
        "outputId": "0f52612b-72ce-4ebe-8ca7-b79990eb7e06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist1 = train_model(env, model, total_episodes=500)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 3.000000, reward total was -19.000000. running mean: -20.980000\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.980200\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.980398\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.980594\n",
            "resetting env. episode 7.000000, reward total was -19.000000. running mean: -20.960788\n",
            "resetting env. episode 8.000000, reward total was -20.000000. running mean: -20.951180\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.951668\n",
            "resetting env. episode 10.000000, reward total was -19.000000. running mean: -20.932152\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.932830\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.933502\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.934167\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.934825\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.935477\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.936122\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.936761\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.937393\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.938019\n",
            "resetting env. episode 20.000000, reward total was -20.000000. running mean: -20.928639\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -20.929353\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.930059\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.930759\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.931451\n",
            "resetting env. episode 25.000000, reward total was -19.000000. running mean: -20.912137\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.913015\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.913885\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.914746\n",
            "resetting env. episode 29.000000, reward total was -19.000000. running mean: -20.895599\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.896643\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -20.887676\n",
            "resetting env. episode 32.000000, reward total was -20.000000. running mean: -20.878800\n",
            "resetting env. episode 33.000000, reward total was -20.000000. running mean: -20.870012\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.871311\n",
            "resetting env. episode 35.000000, reward total was -20.000000. running mean: -20.862598\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.863972\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.865333\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.866679\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -20.868013\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.869332\n",
            "resetting env. episode 41.000000, reward total was -20.000000. running mean: -20.860639\n",
            "resetting env. episode 42.000000, reward total was -20.000000. running mean: -20.852033\n",
            "resetting env. episode 43.000000, reward total was -20.000000. running mean: -20.843512\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.845077\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.846626\n",
            "resetting env. episode 46.000000, reward total was -20.000000. running mean: -20.838160\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.839779\n",
            "resetting env. episode 48.000000, reward total was -20.000000. running mean: -20.831381\n",
            "resetting env. episode 49.000000, reward total was -19.000000. running mean: -20.813067\n",
            "resetting env. episode 50.000000, reward total was -20.000000. running mean: -20.804936\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.806887\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.808818\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -20.810730\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.812623\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.814496\n",
            "resetting env. episode 56.000000, reward total was -19.000000. running mean: -20.796351\n",
            "resetting env. episode 57.000000, reward total was -20.000000. running mean: -20.788388\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.790504\n",
            "resetting env. episode 59.000000, reward total was -20.000000. running mean: -20.782599\n",
            "resetting env. episode 60.000000, reward total was -20.000000. running mean: -20.774773\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.777025\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.779255\n",
            "resetting env. episode 63.000000, reward total was -20.000000. running mean: -20.771462\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.773748\n",
            "resetting env. episode 65.000000, reward total was -20.000000. running mean: -20.766010\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.768350\n",
            "resetting env. episode 67.000000, reward total was -19.000000. running mean: -20.750667\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.753160\n",
            "resetting env. episode 69.000000, reward total was -19.000000. running mean: -20.735629\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.738272\n",
            "resetting env. episode 71.000000, reward total was -18.000000. running mean: -20.710890\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.713781\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.716643\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.719476\n",
            "resetting env. episode 75.000000, reward total was -20.000000. running mean: -20.712282\n",
            "resetting env. episode 76.000000, reward total was -19.000000. running mean: -20.695159\n",
            "resetting env. episode 77.000000, reward total was -20.000000. running mean: -20.688207\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.691325\n",
            "resetting env. episode 79.000000, reward total was -20.000000. running mean: -20.684412\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.687568\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.690692\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.693785\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.696847\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.699879\n",
            "resetting env. episode 85.000000, reward total was -20.000000. running mean: -20.692880\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -20.685951\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.689092\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.692201\n",
            "resetting env. episode 89.000000, reward total was -19.000000. running mean: -20.675279\n",
            "resetting env. episode 90.000000, reward total was -20.000000. running mean: -20.668526\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.671841\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.675122\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.678371\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.681587\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.684772\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.687924\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -20.691045\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.694134\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.697193\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.700221\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.703219\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.706186\n",
            "resetting env. episode 103.000000, reward total was -20.000000. running mean: -20.699125\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.702133\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.705112\n",
            "resetting env. episode 106.000000, reward total was -20.000000. running mean: -20.698061\n",
            "resetting env. episode 107.000000, reward total was -20.000000. running mean: -20.691080\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -20.694170\n",
            "resetting env. episode 109.000000, reward total was -20.000000. running mean: -20.687228\n",
            "resetting env. episode 110.000000, reward total was -19.000000. running mean: -20.670356\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.673652\n",
            "resetting env. episode 112.000000, reward total was -20.000000. running mean: -20.666915\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -20.670246\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.673544\n",
            "resetting env. episode 115.000000, reward total was -20.000000. running mean: -20.666808\n",
            "resetting env. episode 116.000000, reward total was -19.000000. running mean: -20.650140\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.653639\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.657103\n",
            "resetting env. episode 119.000000, reward total was -19.000000. running mean: -20.640532\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.644126\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.647685\n",
            "resetting env. episode 122.000000, reward total was -20.000000. running mean: -20.641208\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.644796\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.648348\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.651865\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.655346\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.658792\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.662205\n",
            "resetting env. episode 129.000000, reward total was -19.000000. running mean: -20.645582\n",
            "resetting env. episode 130.000000, reward total was -20.000000. running mean: -20.639127\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.642735\n",
            "resetting env. episode 132.000000, reward total was -20.000000. running mean: -20.636308\n",
            "resetting env. episode 133.000000, reward total was -18.000000. running mean: -20.609945\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.613846\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.617707\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.621530\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.625315\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.629062\n",
            "resetting env. episode 139.000000, reward total was -20.000000. running mean: -20.622771\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.626543\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.630278\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.633975\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.637635\n",
            "resetting env. episode 144.000000, reward total was -20.000000. running mean: -20.631259\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.634946\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.638597\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.642211\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.645789\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.649331\n",
            "resetting env. episode 150.000000, reward total was -20.000000. running mean: -20.642838\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.646409\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.649945\n",
            "resetting env. episode 153.000000, reward total was -17.000000. running mean: -20.613446\n",
            "resetting env. episode 154.000000, reward total was -19.000000. running mean: -20.597311\n",
            "resetting env. episode 155.000000, reward total was -18.000000. running mean: -20.571338\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.575625\n",
            "resetting env. episode 157.000000, reward total was -20.000000. running mean: -20.569868\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.574170\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.578428\n",
            "resetting env. episode 160.000000, reward total was -19.000000. running mean: -20.562644\n",
            "resetting env. episode 161.000000, reward total was -20.000000. running mean: -20.557017\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.561447\n",
            "resetting env. episode 163.000000, reward total was -20.000000. running mean: -20.555833\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.560274\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.564672\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.569025\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.573335\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.577601\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.581825\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.586007\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.590147\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.594246\n",
            "resetting env. episode 173.000000, reward total was -20.000000. running mean: -20.588303\n",
            "resetting env. episode 174.000000, reward total was -19.000000. running mean: -20.572420\n",
            "resetting env. episode 175.000000, reward total was -19.000000. running mean: -20.556696\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.561129\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.565518\n",
            "resetting env. episode 178.000000, reward total was -20.000000. running mean: -20.559862\n",
            "resetting env. episode 179.000000, reward total was -20.000000. running mean: -20.554264\n",
            "resetting env. episode 180.000000, reward total was -20.000000. running mean: -20.548721\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.553234\n",
            "resetting env. episode 182.000000, reward total was -20.000000. running mean: -20.547702\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.552225\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.556702\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.561135\n",
            "resetting env. episode 186.000000, reward total was -20.000000. running mean: -20.555524\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.559969\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -20.554369\n",
            "resetting env. episode 189.000000, reward total was -19.000000. running mean: -20.538825\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -20.533437\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.538103\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.542722\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.547294\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.551822\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.556303\n",
            "resetting env. episode 196.000000, reward total was -19.000000. running mean: -20.540740\n",
            "resetting env. episode 197.000000, reward total was -19.000000. running mean: -20.525333\n",
            "resetting env. episode 198.000000, reward total was -20.000000. running mean: -20.520080\n",
            "resetting env. episode 199.000000, reward total was -20.000000. running mean: -20.514879\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.519730\n",
            "resetting env. episode 201.000000, reward total was -20.000000. running mean: -20.514533\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.519387\n",
            "resetting env. episode 203.000000, reward total was -20.000000. running mean: -20.514193\n",
            "resetting env. episode 204.000000, reward total was -19.000000. running mean: -20.499052\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.504061\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.509020\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.513930\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.518791\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.523603\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.528367\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.533083\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.537752\n",
            "resetting env. episode 213.000000, reward total was -20.000000. running mean: -20.532375\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.537051\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.541681\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.546264\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.550801\n",
            "resetting env. episode 218.000000, reward total was -20.000000. running mean: -20.545293\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.549840\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.554342\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.558798\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.563210\n",
            "resetting env. episode 223.000000, reward total was -19.000000. running mean: -20.547578\n",
            "resetting env. episode 224.000000, reward total was -18.000000. running mean: -20.522103\n",
            "resetting env. episode 225.000000, reward total was -18.000000. running mean: -20.496882\n",
            "resetting env. episode 226.000000, reward total was -20.000000. running mean: -20.491913\n",
            "resetting env. episode 227.000000, reward total was -20.000000. running mean: -20.486994\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.492124\n",
            "resetting env. episode 229.000000, reward total was -19.000000. running mean: -20.477202\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.482430\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.487606\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.492730\n",
            "resetting env. episode 233.000000, reward total was -20.000000. running mean: -20.487803\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.492925\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.497995\n",
            "resetting env. episode 236.000000, reward total was -20.000000. running mean: -20.493016\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.498085\n",
            "resetting env. episode 238.000000, reward total was -20.000000. running mean: -20.493105\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.498173\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.503192\n",
            "resetting env. episode 241.000000, reward total was -20.000000. running mean: -20.498160\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.503178\n",
            "resetting env. episode 243.000000, reward total was -20.000000. running mean: -20.498146\n",
            "resetting env. episode 244.000000, reward total was -20.000000. running mean: -20.493165\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -20.488233\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.493351\n",
            "resetting env. episode 247.000000, reward total was -20.000000. running mean: -20.488417\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.493533\n",
            "resetting env. episode 249.000000, reward total was -19.000000. running mean: -20.478598\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.483812\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.488974\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.494084\n",
            "resetting env. episode 253.000000, reward total was -19.000000. running mean: -20.479143\n",
            "resetting env. episode 254.000000, reward total was -19.000000. running mean: -20.464352\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.469708\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.475011\n",
            "resetting env. episode 257.000000, reward total was -19.000000. running mean: -20.460261\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -20.455659\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.461102\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.466491\n",
            "resetting env. episode 261.000000, reward total was -20.000000. running mean: -20.461826\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.467208\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.472536\n",
            "resetting env. episode 264.000000, reward total was -20.000000. running mean: -20.467810\n",
            "resetting env. episode 265.000000, reward total was -20.000000. running mean: -20.463132\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.468501\n",
            "resetting env. episode 267.000000, reward total was -20.000000. running mean: -20.463816\n",
            "resetting env. episode 268.000000, reward total was -20.000000. running mean: -20.459178\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.464586\n",
            "resetting env. episode 270.000000, reward total was -20.000000. running mean: -20.459940\n",
            "resetting env. episode 271.000000, reward total was -20.000000. running mean: -20.455341\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.460787\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.466179\n",
            "resetting env. episode 274.000000, reward total was -20.000000. running mean: -20.461518\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.466902\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.472233\n",
            "resetting env. episode 277.000000, reward total was -19.000000. running mean: -20.457511\n",
            "resetting env. episode 278.000000, reward total was -19.000000. running mean: -20.442936\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.448507\n",
            "resetting env. episode 280.000000, reward total was -19.000000. running mean: -20.434022\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.439681\n",
            "resetting env. episode 282.000000, reward total was -19.000000. running mean: -20.425285\n",
            "resetting env. episode 283.000000, reward total was -19.000000. running mean: -20.411032\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.416921\n",
            "resetting env. episode 285.000000, reward total was -20.000000. running mean: -20.412752\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.418625\n",
            "resetting env. episode 287.000000, reward total was -18.000000. running mean: -20.394438\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.400494\n",
            "resetting env. episode 289.000000, reward total was -19.000000. running mean: -20.386489\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.392624\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.398698\n",
            "resetting env. episode 292.000000, reward total was -20.000000. running mean: -20.394711\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.400764\n",
            "resetting env. episode 294.000000, reward total was -20.000000. running mean: -20.396756\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.402789\n",
            "resetting env. episode 296.000000, reward total was -19.000000. running mean: -20.388761\n",
            "resetting env. episode 297.000000, reward total was -20.000000. running mean: -20.384873\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.391024\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -20.387114\n",
            "resetting env. episode 300.000000, reward total was -19.000000. running mean: -20.373243\n",
            "resetting env. episode 301.000000, reward total was -19.000000. running mean: -20.359511\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.365915\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.372256\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.378534\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.384748\n",
            "resetting env. episode 306.000000, reward total was -20.000000. running mean: -20.380901\n",
            "resetting env. episode 307.000000, reward total was -20.000000. running mean: -20.377092\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.383321\n",
            "resetting env. episode 309.000000, reward total was -20.000000. running mean: -20.379488\n",
            "resetting env. episode 310.000000, reward total was -19.000000. running mean: -20.365693\n",
            "resetting env. episode 311.000000, reward total was -20.000000. running mean: -20.362036\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.368416\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.374731\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.380984\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.387174\n",
            "resetting env. episode 316.000000, reward total was -20.000000. running mean: -20.383303\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.389470\n",
            "resetting env. episode 318.000000, reward total was -18.000000. running mean: -20.365575\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.371919\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.378200\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.384418\n",
            "resetting env. episode 322.000000, reward total was -19.000000. running mean: -20.370574\n",
            "resetting env. episode 323.000000, reward total was -18.000000. running mean: -20.346868\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.353399\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.359865\n",
            "resetting env. episode 326.000000, reward total was -20.000000. running mean: -20.356267\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.362704\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.369077\n",
            "resetting env. episode 329.000000, reward total was -20.000000. running mean: -20.365386\n",
            "resetting env. episode 330.000000, reward total was -19.000000. running mean: -20.351732\n",
            "resetting env. episode 331.000000, reward total was -20.000000. running mean: -20.348215\n",
            "resetting env. episode 332.000000, reward total was -19.000000. running mean: -20.334733\n",
            "resetting env. episode 333.000000, reward total was -20.000000. running mean: -20.331386\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.338072\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.344691\n",
            "resetting env. episode 336.000000, reward total was -20.000000. running mean: -20.341244\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.347832\n",
            "resetting env. episode 338.000000, reward total was -20.000000. running mean: -20.344353\n",
            "resetting env. episode 339.000000, reward total was -20.000000. running mean: -20.340910\n",
            "resetting env. episode 340.000000, reward total was -20.000000. running mean: -20.337501\n",
            "resetting env. episode 341.000000, reward total was -19.000000. running mean: -20.324126\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.330884\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.337576\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.344200\n",
            "resetting env. episode 345.000000, reward total was -20.000000. running mean: -20.340758\n",
            "resetting env. episode 346.000000, reward total was -20.000000. running mean: -20.337350\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.343977\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.350537\n",
            "resetting env. episode 349.000000, reward total was -19.000000. running mean: -20.337032\n",
            "resetting env. episode 350.000000, reward total was -20.000000. running mean: -20.333661\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.340325\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.346921\n",
            "resetting env. episode 353.000000, reward total was -18.000000. running mean: -20.323452\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.330218\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.336916\n",
            "resetting env. episode 356.000000, reward total was -20.000000. running mean: -20.333546\n",
            "resetting env. episode 357.000000, reward total was -19.000000. running mean: -20.320211\n",
            "resetting env. episode 358.000000, reward total was -19.000000. running mean: -20.307009\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.313939\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.320799\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.327591\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.334315\n",
            "resetting env. episode 363.000000, reward total was -20.000000. running mean: -20.330972\n",
            "resetting env. episode 364.000000, reward total was -17.000000. running mean: -20.297663\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.304686\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.311639\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.318523\n",
            "resetting env. episode 368.000000, reward total was -20.000000. running mean: -20.315337\n",
            "resetting env. episode 369.000000, reward total was -19.000000. running mean: -20.302184\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.309162\n",
            "resetting env. episode 371.000000, reward total was -19.000000. running mean: -20.296071\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.303110\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.310079\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.316978\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -20.313808\n",
            "resetting env. episode 376.000000, reward total was -20.000000. running mean: -20.310670\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.317563\n",
            "resetting env. episode 378.000000, reward total was -20.000000. running mean: -20.314388\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.321244\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.328031\n",
            "resetting env. episode 381.000000, reward total was -18.000000. running mean: -20.304751\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.311704\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.318587\n",
            "resetting env. episode 384.000000, reward total was -20.000000. running mean: -20.315401\n",
            "resetting env. episode 385.000000, reward total was -20.000000. running mean: -20.312247\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.319124\n",
            "resetting env. episode 387.000000, reward total was -20.000000. running mean: -20.315933\n",
            "resetting env. episode 388.000000, reward total was -20.000000. running mean: -20.312774\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.319646\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.326450\n",
            "resetting env. episode 391.000000, reward total was -20.000000. running mean: -20.323185\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -20.319953\n",
            "resetting env. episode 393.000000, reward total was -19.000000. running mean: -20.306754\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.313686\n",
            "resetting env. episode 395.000000, reward total was -20.000000. running mean: -20.310549\n",
            "resetting env. episode 396.000000, reward total was -19.000000. running mean: -20.297444\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.304469\n",
            "resetting env. episode 398.000000, reward total was -20.000000. running mean: -20.301425\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.308410\n",
            "resetting env. episode 400.000000, reward total was -20.000000. running mean: -20.305326\n",
            "resetting env. episode 401.000000, reward total was -19.000000. running mean: -20.292273\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.299350\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.306357\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.313293\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.320160\n",
            "resetting env. episode 406.000000, reward total was -20.000000. running mean: -20.316959\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.323789\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.330551\n",
            "resetting env. episode 409.000000, reward total was -19.000000. running mean: -20.317246\n",
            "resetting env. episode 410.000000, reward total was -20.000000. running mean: -20.314073\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.320932\n",
            "resetting env. episode 412.000000, reward total was -19.000000. running mean: -20.307723\n",
            "resetting env. episode 413.000000, reward total was -19.000000. running mean: -20.294646\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.301699\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.308682\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.315596\n",
            "resetting env. episode 417.000000, reward total was -20.000000. running mean: -20.312440\n",
            "resetting env. episode 418.000000, reward total was -19.000000. running mean: -20.299315\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.306322\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.313259\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.320126\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.326925\n",
            "resetting env. episode 423.000000, reward total was -20.000000. running mean: -20.323656\n",
            "resetting env. episode 424.000000, reward total was -20.000000. running mean: -20.320419\n",
            "resetting env. episode 425.000000, reward total was -20.000000. running mean: -20.317215\n",
            "resetting env. episode 426.000000, reward total was -20.000000. running mean: -20.314043\n",
            "resetting env. episode 427.000000, reward total was -19.000000. running mean: -20.300902\n",
            "resetting env. episode 428.000000, reward total was -20.000000. running mean: -20.297893\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.304915\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.311865\n",
            "resetting env. episode 431.000000, reward total was -20.000000. running mean: -20.308747\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.315659\n",
            "resetting env. episode 433.000000, reward total was -19.000000. running mean: -20.302503\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.309478\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -20.306383\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.313319\n",
            "resetting env. episode 437.000000, reward total was -20.000000. running mean: -20.310186\n",
            "resetting env. episode 438.000000, reward total was -19.000000. running mean: -20.297084\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.294113\n",
            "resetting env. episode 440.000000, reward total was -19.000000. running mean: -20.281172\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -20.278360\n",
            "resetting env. episode 442.000000, reward total was -20.000000. running mean: -20.275577\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.282821\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.289993\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.297093\n",
            "resetting env. episode 446.000000, reward total was -19.000000. running mean: -20.284122\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.291281\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.298368\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.305384\n",
            "resetting env. episode 450.000000, reward total was -19.000000. running mean: -20.292330\n",
            "resetting env. episode 451.000000, reward total was -20.000000. running mean: -20.289407\n",
            "resetting env. episode 452.000000, reward total was -20.000000. running mean: -20.286513\n",
            "resetting env. episode 453.000000, reward total was -20.000000. running mean: -20.283648\n",
            "resetting env. episode 454.000000, reward total was -20.000000. running mean: -20.280811\n",
            "resetting env. episode 455.000000, reward total was -20.000000. running mean: -20.278003\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.285223\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.292371\n",
            "resetting env. episode 458.000000, reward total was -20.000000. running mean: -20.289447\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.296553\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.303587\n",
            "resetting env. episode 461.000000, reward total was -20.000000. running mean: -20.300551\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.307546\n",
            "resetting env. episode 463.000000, reward total was -17.000000. running mean: -20.274470\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.281726\n",
            "resetting env. episode 465.000000, reward total was -20.000000. running mean: -20.278908\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.286119\n",
            "resetting env. episode 467.000000, reward total was -20.000000. running mean: -20.283258\n",
            "resetting env. episode 468.000000, reward total was -20.000000. running mean: -20.280426\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.287621\n",
            "resetting env. episode 470.000000, reward total was -20.000000. running mean: -20.284745\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.291898\n",
            "resetting env. episode 472.000000, reward total was -20.000000. running mean: -20.288979\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.296089\n",
            "resetting env. episode 474.000000, reward total was -19.000000. running mean: -20.283128\n",
            "resetting env. episode 475.000000, reward total was -20.000000. running mean: -20.280297\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.287494\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.294619\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.291673\n",
            "resetting env. episode 479.000000, reward total was -19.000000. running mean: -20.278756\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.285968\n",
            "resetting env. episode 481.000000, reward total was -20.000000. running mean: -20.283109\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.290278\n",
            "resetting env. episode 483.000000, reward total was -20.000000. running mean: -20.287375\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.294501\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.301556\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.308540\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.315455\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.322301\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.329078\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.335787\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.342429\n",
            "resetting env. episode 492.000000, reward total was -20.000000. running mean: -20.339005\n",
            "resetting env. episode 493.000000, reward total was -20.000000. running mean: -20.335615\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.342258\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.348836\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.355347\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.361794\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.368176\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.374494\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.380749\n",
            "CPU times: user 34min 2s, sys: 10min 49s, total: 44min 52s\n",
            "Wall time: 23min 22s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "cHYCDYwhlVLV",
        "outputId": "0a7e6b26-ed4d-4fef-9de1-11044e0cb78c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist2 = train_model(env, model, total_episodes=500)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 5.000000, reward total was -19.000000. running mean: -20.980000\n",
            "resetting env. episode 6.000000, reward total was -20.000000. running mean: -20.970200\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.970498\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.970793\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.971085\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.971374\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.971660\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.971944\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.972224\n",
            "resetting env. episode 14.000000, reward total was -20.000000. running mean: -20.962502\n",
            "resetting env. episode 15.000000, reward total was -19.000000. running mean: -20.942877\n",
            "resetting env. episode 16.000000, reward total was -19.000000. running mean: -20.923448\n",
            "resetting env. episode 17.000000, reward total was -20.000000. running mean: -20.914214\n",
            "resetting env. episode 18.000000, reward total was -18.000000. running mean: -20.885072\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.886221\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.887359\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -20.888485\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.889600\n",
            "resetting env. episode 23.000000, reward total was -20.000000. running mean: -20.880704\n",
            "resetting env. episode 24.000000, reward total was -20.000000. running mean: -20.871897\n",
            "resetting env. episode 25.000000, reward total was -20.000000. running mean: -20.863178\n",
            "resetting env. episode 26.000000, reward total was -20.000000. running mean: -20.854547\n",
            "resetting env. episode 27.000000, reward total was -20.000000. running mean: -20.846001\n",
            "resetting env. episode 28.000000, reward total was -20.000000. running mean: -20.837541\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.839166\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.840774\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -20.832366\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -20.834043\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.835702\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.837345\n",
            "resetting env. episode 35.000000, reward total was -19.000000. running mean: -20.818972\n",
            "resetting env. episode 36.000000, reward total was -20.000000. running mean: -20.810782\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.812674\n",
            "resetting env. episode 38.000000, reward total was -20.000000. running mean: -20.804547\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -20.806502\n",
            "resetting env. episode 40.000000, reward total was -20.000000. running mean: -20.798437\n",
            "resetting env. episode 41.000000, reward total was -20.000000. running mean: -20.790453\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.792548\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.794623\n",
            "resetting env. episode 44.000000, reward total was -20.000000. running mean: -20.786676\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.788810\n",
            "resetting env. episode 46.000000, reward total was -20.000000. running mean: -20.780922\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.783112\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.785281\n",
            "resetting env. episode 49.000000, reward total was -20.000000. running mean: -20.777428\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.779654\n",
            "resetting env. episode 51.000000, reward total was -20.000000. running mean: -20.771858\n",
            "resetting env. episode 52.000000, reward total was -17.000000. running mean: -20.734139\n",
            "resetting env. episode 53.000000, reward total was -20.000000. running mean: -20.726798\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.729530\n",
            "resetting env. episode 55.000000, reward total was -20.000000. running mean: -20.722234\n",
            "resetting env. episode 56.000000, reward total was -20.000000. running mean: -20.715012\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.717862\n",
            "resetting env. episode 58.000000, reward total was -20.000000. running mean: -20.710683\n",
            "resetting env. episode 59.000000, reward total was -20.000000. running mean: -20.703576\n",
            "resetting env. episode 60.000000, reward total was -20.000000. running mean: -20.696541\n",
            "resetting env. episode 61.000000, reward total was -19.000000. running mean: -20.679575\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.682779\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.685952\n",
            "resetting env. episode 64.000000, reward total was -20.000000. running mean: -20.679092\n",
            "resetting env. episode 65.000000, reward total was -19.000000. running mean: -20.662301\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.665678\n",
            "resetting env. episode 67.000000, reward total was -20.000000. running mean: -20.659021\n",
            "resetting env. episode 68.000000, reward total was -19.000000. running mean: -20.642431\n",
            "resetting env. episode 69.000000, reward total was -19.000000. running mean: -20.626007\n",
            "resetting env. episode 70.000000, reward total was -19.000000. running mean: -20.609747\n",
            "resetting env. episode 71.000000, reward total was -18.000000. running mean: -20.583649\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.587813\n",
            "resetting env. episode 73.000000, reward total was -19.000000. running mean: -20.571935\n",
            "resetting env. episode 74.000000, reward total was -20.000000. running mean: -20.566215\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.570553\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.574848\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.579099\n",
            "resetting env. episode 78.000000, reward total was -20.000000. running mean: -20.573308\n",
            "resetting env. episode 79.000000, reward total was -20.000000. running mean: -20.567575\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.571899\n",
            "resetting env. episode 81.000000, reward total was -20.000000. running mean: -20.566180\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.570519\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.574813\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.579065\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.583275\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -20.577442\n",
            "resetting env. episode 87.000000, reward total was -19.000000. running mean: -20.561667\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.566051\n",
            "resetting env. episode 89.000000, reward total was -20.000000. running mean: -20.560390\n",
            "resetting env. episode 90.000000, reward total was -20.000000. running mean: -20.554786\n",
            "resetting env. episode 91.000000, reward total was -20.000000. running mean: -20.549239\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.553746\n",
            "resetting env. episode 93.000000, reward total was -20.000000. running mean: -20.548209\n",
            "resetting env. episode 94.000000, reward total was -20.000000. running mean: -20.542727\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.547299\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.551826\n",
            "resetting env. episode 97.000000, reward total was -19.000000. running mean: -20.536308\n",
            "resetting env. episode 98.000000, reward total was -19.000000. running mean: -20.520945\n",
            "resetting env. episode 99.000000, reward total was -20.000000. running mean: -20.515736\n",
            "resetting env. episode 100.000000, reward total was -20.000000. running mean: -20.510578\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.515472\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.520318\n",
            "resetting env. episode 103.000000, reward total was -20.000000. running mean: -20.515115\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.519963\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.524764\n",
            "resetting env. episode 106.000000, reward total was -20.000000. running mean: -20.519516\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.524321\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -20.519078\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.523887\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.528648\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.533362\n",
            "resetting env. episode 112.000000, reward total was -20.000000. running mean: -20.528028\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -20.532748\n",
            "resetting env. episode 114.000000, reward total was -19.000000. running mean: -20.517420\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.522246\n",
            "resetting env. episode 116.000000, reward total was -20.000000. running mean: -20.517024\n",
            "resetting env. episode 117.000000, reward total was -20.000000. running mean: -20.511853\n",
            "resetting env. episode 118.000000, reward total was -19.000000. running mean: -20.496735\n",
            "resetting env. episode 119.000000, reward total was -19.000000. running mean: -20.481767\n",
            "resetting env. episode 120.000000, reward total was -20.000000. running mean: -20.476950\n",
            "resetting env. episode 121.000000, reward total was -20.000000. running mean: -20.472180\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.477458\n",
            "resetting env. episode 123.000000, reward total was -20.000000. running mean: -20.472684\n",
            "resetting env. episode 124.000000, reward total was -19.000000. running mean: -20.457957\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.463377\n",
            "resetting env. episode 126.000000, reward total was -20.000000. running mean: -20.458744\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.464156\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.469515\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.474820\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.480071\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.485271\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.490418\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.495514\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.500559\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.505553\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -20.500498\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.505493\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -20.500438\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.505433\n",
            "resetting env. episode 140.000000, reward total was -20.000000. running mean: -20.500379\n",
            "resetting env. episode 141.000000, reward total was -20.000000. running mean: -20.495375\n",
            "resetting env. episode 142.000000, reward total was -19.000000. running mean: -20.480421\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.485617\n",
            "resetting env. episode 144.000000, reward total was -20.000000. running mean: -20.480761\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.485953\n",
            "resetting env. episode 146.000000, reward total was -18.000000. running mean: -20.461094\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.466483\n",
            "resetting env. episode 148.000000, reward total was -20.000000. running mean: -20.461818\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.467200\n",
            "resetting env. episode 150.000000, reward total was -20.000000. running mean: -20.462528\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.467903\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.473224\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.478491\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.483706\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.488869\n",
            "resetting env. episode 156.000000, reward total was -17.000000. running mean: -20.453981\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.459441\n",
            "resetting env. episode 158.000000, reward total was -20.000000. running mean: -20.454846\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.460298\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.465695\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.471038\n",
            "resetting env. episode 162.000000, reward total was -20.000000. running mean: -20.466328\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.471664\n",
            "resetting env. episode 164.000000, reward total was -19.000000. running mean: -20.456948\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.462378\n",
            "resetting env. episode 166.000000, reward total was -19.000000. running mean: -20.447754\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.453277\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.458744\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.464157\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.469515\n",
            "resetting env. episode 171.000000, reward total was -20.000000. running mean: -20.464820\n",
            "resetting env. episode 172.000000, reward total was -20.000000. running mean: -20.460172\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.465570\n",
            "resetting env. episode 174.000000, reward total was -18.000000. running mean: -20.440914\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.446505\n",
            "resetting env. episode 176.000000, reward total was -20.000000. running mean: -20.442040\n",
            "resetting env. episode 177.000000, reward total was -19.000000. running mean: -20.427620\n",
            "resetting env. episode 178.000000, reward total was -20.000000. running mean: -20.423344\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.429110\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.434819\n",
            "resetting env. episode 181.000000, reward total was -20.000000. running mean: -20.430471\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.436166\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.441805\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.447386\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.452913\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.458383\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.463800\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.469162\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.474470\n",
            "resetting env. episode 190.000000, reward total was -19.000000. running mean: -20.459725\n",
            "resetting env. episode 191.000000, reward total was -20.000000. running mean: -20.455128\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.460577\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.465971\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.471311\n",
            "resetting env. episode 195.000000, reward total was -20.000000. running mean: -20.466598\n",
            "resetting env. episode 196.000000, reward total was -20.000000. running mean: -20.461932\n",
            "resetting env. episode 197.000000, reward total was -19.000000. running mean: -20.447313\n",
            "resetting env. episode 198.000000, reward total was -20.000000. running mean: -20.442840\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.448411\n",
            "resetting env. episode 200.000000, reward total was -20.000000. running mean: -20.443927\n",
            "resetting env. episode 201.000000, reward total was -20.000000. running mean: -20.439488\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.445093\n",
            "resetting env. episode 203.000000, reward total was -20.000000. running mean: -20.440642\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.446236\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.451773\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.457256\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.462683\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.468056\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.473376\n",
            "resetting env. episode 210.000000, reward total was -19.000000. running mean: -20.458642\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.464056\n",
            "resetting env. episode 212.000000, reward total was -20.000000. running mean: -20.459415\n",
            "resetting env. episode 213.000000, reward total was -19.000000. running mean: -20.444821\n",
            "resetting env. episode 214.000000, reward total was -19.000000. running mean: -20.430373\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.436069\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.441708\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.447291\n",
            "resetting env. episode 218.000000, reward total was -19.000000. running mean: -20.432818\n",
            "resetting env. episode 219.000000, reward total was -20.000000. running mean: -20.428490\n",
            "resetting env. episode 220.000000, reward total was -20.000000. running mean: -20.424205\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.429963\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.435663\n",
            "resetting env. episode 223.000000, reward total was -20.000000. running mean: -20.431307\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.436994\n",
            "resetting env. episode 225.000000, reward total was -20.000000. running mean: -20.432624\n",
            "resetting env. episode 226.000000, reward total was -20.000000. running mean: -20.428298\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.434015\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.439674\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.445278\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.450825\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.456317\n",
            "resetting env. episode 232.000000, reward total was -19.000000. running mean: -20.441754\n",
            "resetting env. episode 233.000000, reward total was -20.000000. running mean: -20.437336\n",
            "resetting env. episode 234.000000, reward total was -20.000000. running mean: -20.432963\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.438633\n",
            "resetting env. episode 236.000000, reward total was -20.000000. running mean: -20.434247\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.439904\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.445505\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.451050\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.456540\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.461974\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.467354\n",
            "resetting env. episode 243.000000, reward total was -20.000000. running mean: -20.462681\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.468054\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.473374\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.478640\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.483853\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.489015\n",
            "resetting env. episode 249.000000, reward total was -19.000000. running mean: -20.474125\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.479383\n",
            "resetting env. episode 251.000000, reward total was -18.000000. running mean: -20.454590\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.460044\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.465443\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.470789\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.476081\n",
            "resetting env. episode 256.000000, reward total was -19.000000. running mean: -20.461320\n",
            "resetting env. episode 257.000000, reward total was -19.000000. running mean: -20.446707\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -20.442240\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.447818\n",
            "resetting env. episode 260.000000, reward total was -20.000000. running mean: -20.443339\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.448906\n",
            "resetting env. episode 262.000000, reward total was -20.000000. running mean: -20.444417\n",
            "resetting env. episode 263.000000, reward total was -18.000000. running mean: -20.419973\n",
            "resetting env. episode 264.000000, reward total was -20.000000. running mean: -20.415773\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.421615\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.427399\n",
            "resetting env. episode 267.000000, reward total was -20.000000. running mean: -20.423125\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.428894\n",
            "resetting env. episode 269.000000, reward total was -20.000000. running mean: -20.424605\n",
            "resetting env. episode 270.000000, reward total was -20.000000. running mean: -20.420359\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.426155\n",
            "resetting env. episode 272.000000, reward total was -18.000000. running mean: -20.401894\n",
            "resetting env. episode 273.000000, reward total was -20.000000. running mean: -20.397875\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.403896\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.409857\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.415759\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.421601\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.427385\n",
            "resetting env. episode 279.000000, reward total was -20.000000. running mean: -20.423111\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.428880\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.434591\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.440245\n",
            "resetting env. episode 283.000000, reward total was -20.000000. running mean: -20.435843\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.441484\n",
            "resetting env. episode 285.000000, reward total was -20.000000. running mean: -20.437070\n",
            "resetting env. episode 286.000000, reward total was -19.000000. running mean: -20.422699\n",
            "resetting env. episode 287.000000, reward total was -20.000000. running mean: -20.418472\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.424287\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.430044\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.435744\n",
            "resetting env. episode 291.000000, reward total was -19.000000. running mean: -20.421386\n",
            "resetting env. episode 292.000000, reward total was -17.000000. running mean: -20.387173\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.393301\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.399368\n",
            "resetting env. episode 295.000000, reward total was -20.000000. running mean: -20.395374\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.401420\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.407406\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.403332\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -20.399299\n",
            "resetting env. episode 300.000000, reward total was -20.000000. running mean: -20.395306\n",
            "resetting env. episode 301.000000, reward total was -20.000000. running mean: -20.391353\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.397439\n",
            "resetting env. episode 303.000000, reward total was -19.000000. running mean: -20.383465\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.389630\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.395734\n",
            "resetting env. episode 306.000000, reward total was -18.000000. running mean: -20.371777\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.378059\n",
            "resetting env. episode 308.000000, reward total was -20.000000. running mean: -20.374278\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.380535\n",
            "resetting env. episode 310.000000, reward total was -20.000000. running mean: -20.376730\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.382963\n",
            "resetting env. episode 312.000000, reward total was -20.000000. running mean: -20.379133\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.385342\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.391488\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.397573\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.403598\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.409562\n",
            "resetting env. episode 318.000000, reward total was -20.000000. running mean: -20.405466\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.411411\n",
            "resetting env. episode 320.000000, reward total was -20.000000. running mean: -20.407297\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.413224\n",
            "resetting env. episode 322.000000, reward total was -20.000000. running mean: -20.409092\n",
            "resetting env. episode 323.000000, reward total was -19.000000. running mean: -20.395001\n",
            "resetting env. episode 324.000000, reward total was -18.000000. running mean: -20.371051\n",
            "resetting env. episode 325.000000, reward total was -20.000000. running mean: -20.367341\n",
            "resetting env. episode 326.000000, reward total was -20.000000. running mean: -20.363667\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.370031\n",
            "resetting env. episode 328.000000, reward total was -19.000000. running mean: -20.356330\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.362767\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.369139\n",
            "resetting env. episode 331.000000, reward total was -20.000000. running mean: -20.365448\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.371793\n",
            "resetting env. episode 333.000000, reward total was -20.000000. running mean: -20.368076\n",
            "resetting env. episode 334.000000, reward total was -20.000000. running mean: -20.364395\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.370751\n",
            "resetting env. episode 336.000000, reward total was -20.000000. running mean: -20.367043\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.373373\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.379639\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.385843\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.391984\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.398065\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.404084\n",
            "resetting env. episode 343.000000, reward total was -19.000000. running mean: -20.390043\n",
            "resetting env. episode 344.000000, reward total was -20.000000. running mean: -20.386143\n",
            "resetting env. episode 345.000000, reward total was -18.000000. running mean: -20.362281\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.368658\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.374972\n",
            "resetting env. episode 348.000000, reward total was -20.000000. running mean: -20.371222\n",
            "resetting env. episode 349.000000, reward total was -20.000000. running mean: -20.367510\n",
            "resetting env. episode 350.000000, reward total was -20.000000. running mean: -20.363835\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -20.360196\n",
            "resetting env. episode 352.000000, reward total was -19.000000. running mean: -20.346594\n",
            "resetting env. episode 353.000000, reward total was -19.000000. running mean: -20.333128\n",
            "resetting env. episode 354.000000, reward total was -20.000000. running mean: -20.329797\n",
            "resetting env. episode 355.000000, reward total was -18.000000. running mean: -20.306499\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.313434\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.320300\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.327097\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.333826\n",
            "resetting env. episode 360.000000, reward total was -18.000000. running mean: -20.310488\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.317383\n",
            "resetting env. episode 362.000000, reward total was -19.000000. running mean: -20.304209\n",
            "resetting env. episode 363.000000, reward total was -20.000000. running mean: -20.301167\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -20.298155\n",
            "resetting env. episode 365.000000, reward total was -20.000000. running mean: -20.295174\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.302222\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.309200\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.316108\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.322947\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.329717\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.336420\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.343056\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.349625\n",
            "resetting env. episode 374.000000, reward total was -20.000000. running mean: -20.346129\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.352668\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.359141\n",
            "resetting env. episode 377.000000, reward total was -20.000000. running mean: -20.355550\n",
            "resetting env. episode 378.000000, reward total was -20.000000. running mean: -20.351994\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.358474\n",
            "resetting env. episode 380.000000, reward total was -20.000000. running mean: -20.354889\n",
            "resetting env. episode 381.000000, reward total was -19.000000. running mean: -20.341341\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.347927\n",
            "resetting env. episode 383.000000, reward total was -20.000000. running mean: -20.344448\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.351003\n",
            "resetting env. episode 385.000000, reward total was -20.000000. running mean: -20.347493\n",
            "resetting env. episode 386.000000, reward total was -19.000000. running mean: -20.334018\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.340678\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.347271\n",
            "resetting env. episode 389.000000, reward total was -18.000000. running mean: -20.323799\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.330561\n",
            "resetting env. episode 391.000000, reward total was -19.000000. running mean: -20.317255\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -20.314083\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.320942\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.327732\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.334455\n",
            "resetting env. episode 396.000000, reward total was -20.000000. running mean: -20.331110\n",
            "resetting env. episode 397.000000, reward total was -18.000000. running mean: -20.307799\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.314721\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.321574\n",
            "resetting env. episode 400.000000, reward total was -20.000000. running mean: -20.318358\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.325175\n",
            "resetting env. episode 402.000000, reward total was -20.000000. running mean: -20.321923\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.328704\n",
            "resetting env. episode 404.000000, reward total was -20.000000. running mean: -20.325417\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.332163\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.338841\n",
            "resetting env. episode 407.000000, reward total was -19.000000. running mean: -20.325453\n",
            "resetting env. episode 408.000000, reward total was -20.000000. running mean: -20.322198\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.328976\n",
            "resetting env. episode 410.000000, reward total was -19.000000. running mean: -20.315686\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.322529\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -20.319304\n",
            "resetting env. episode 413.000000, reward total was -20.000000. running mean: -20.316111\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.322950\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -20.319721\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.326523\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.333258\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.339925\n",
            "resetting env. episode 419.000000, reward total was -18.000000. running mean: -20.316526\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.323361\n",
            "resetting env. episode 421.000000, reward total was -20.000000. running mean: -20.320127\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.326926\n",
            "resetting env. episode 423.000000, reward total was -20.000000. running mean: -20.323657\n",
            "resetting env. episode 424.000000, reward total was -20.000000. running mean: -20.320420\n",
            "resetting env. episode 425.000000, reward total was -20.000000. running mean: -20.317216\n",
            "resetting env. episode 426.000000, reward total was -20.000000. running mean: -20.314044\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.320903\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.327694\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.334417\n",
            "resetting env. episode 430.000000, reward total was -18.000000. running mean: -20.311073\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.317963\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.324783\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.331535\n",
            "resetting env. episode 434.000000, reward total was -20.000000. running mean: -20.328220\n",
            "resetting env. episode 435.000000, reward total was -19.000000. running mean: -20.314938\n",
            "resetting env. episode 436.000000, reward total was -20.000000. running mean: -20.311788\n",
            "resetting env. episode 437.000000, reward total was -20.000000. running mean: -20.308670\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.315584\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.312428\n",
            "resetting env. episode 440.000000, reward total was -19.000000. running mean: -20.299303\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -20.296310\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.303347\n",
            "resetting env. episode 443.000000, reward total was -18.000000. running mean: -20.280314\n",
            "resetting env. episode 444.000000, reward total was -20.000000. running mean: -20.277511\n",
            "resetting env. episode 445.000000, reward total was -20.000000. running mean: -20.274736\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.281988\n",
            "resetting env. episode 447.000000, reward total was -19.000000. running mean: -20.269168\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.276477\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.283712\n",
            "resetting env. episode 450.000000, reward total was -20.000000. running mean: -20.280875\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.288066\n",
            "resetting env. episode 452.000000, reward total was -19.000000. running mean: -20.275185\n",
            "resetting env. episode 453.000000, reward total was -17.000000. running mean: -20.242434\n",
            "resetting env. episode 454.000000, reward total was -19.000000. running mean: -20.230009\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.237709\n",
            "resetting env. episode 456.000000, reward total was -19.000000. running mean: -20.225332\n",
            "resetting env. episode 457.000000, reward total was -20.000000. running mean: -20.223079\n",
            "resetting env. episode 458.000000, reward total was -20.000000. running mean: -20.220848\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.228639\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.236353\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.243990\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.251550\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.259034\n",
            "resetting env. episode 464.000000, reward total was -19.000000. running mean: -20.246444\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.253979\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.261440\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.268825\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.276137\n",
            "resetting env. episode 469.000000, reward total was -20.000000. running mean: -20.273376\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.280642\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.287835\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.294957\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.302007\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.308987\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.315898\n",
            "resetting env. episode 476.000000, reward total was -20.000000. running mean: -20.312739\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.319611\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.316415\n",
            "resetting env. episode 479.000000, reward total was -20.000000. running mean: -20.313251\n",
            "resetting env. episode 480.000000, reward total was -19.000000. running mean: -20.300118\n",
            "resetting env. episode 481.000000, reward total was -20.000000. running mean: -20.297117\n",
            "resetting env. episode 482.000000, reward total was -19.000000. running mean: -20.284146\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.291305\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.298392\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.305408\n",
            "resetting env. episode 486.000000, reward total was -20.000000. running mean: -20.302354\n",
            "resetting env. episode 487.000000, reward total was -19.000000. running mean: -20.289330\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.296437\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.303472\n",
            "resetting env. episode 490.000000, reward total was -18.000000. running mean: -20.280438\n",
            "resetting env. episode 491.000000, reward total was -20.000000. running mean: -20.277633\n",
            "resetting env. episode 492.000000, reward total was -19.000000. running mean: -20.264857\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.272208\n",
            "resetting env. episode 494.000000, reward total was -19.000000. running mean: -20.259486\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.266891\n",
            "resetting env. episode 496.000000, reward total was -20.000000. running mean: -20.264222\n",
            "resetting env. episode 497.000000, reward total was -17.000000. running mean: -20.231580\n",
            "resetting env. episode 498.000000, reward total was -20.000000. running mean: -20.229264\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.236972\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.244602\n",
            "CPU times: user 35min 11s, sys: 11min 3s, total: 46min 15s\n",
            "Wall time: 23min 50s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "8fheN9DRlWXQ",
        "outputId": "3c90dcb9-6a10-4635-b1f5-78e597165cd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "cell_type": "code",
      "source": [
        "play_game(env, model)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -17.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "9AxOcQhIsKow",
        "outputId": "3b989198-74cb-4f51-f8bb-c0b162b2df5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist3 = train_model(env, model, total_episodes=1500)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 2.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 3.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.010000\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.019900\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.029701\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.039404\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.049010\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.058520\n",
            "resetting env. episode 10.000000, reward total was -20.000000. running mean: -20.057935\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.067355\n",
            "resetting env. episode 12.000000, reward total was -20.000000. running mean: -20.066682\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.076015\n",
            "resetting env. episode 14.000000, reward total was -20.000000. running mean: -20.075255\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.084502\n",
            "resetting env. episode 16.000000, reward total was -20.000000. running mean: -20.083657\n",
            "resetting env. episode 17.000000, reward total was -19.000000. running mean: -20.072821\n",
            "resetting env. episode 18.000000, reward total was -20.000000. running mean: -20.072092\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.081372\n",
            "resetting env. episode 20.000000, reward total was -19.000000. running mean: -20.070558\n",
            "resetting env. episode 21.000000, reward total was -20.000000. running mean: -20.069852\n",
            "resetting env. episode 22.000000, reward total was -20.000000. running mean: -20.069154\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.078462\n",
            "resetting env. episode 24.000000, reward total was -19.000000. running mean: -20.067678\n",
            "resetting env. episode 25.000000, reward total was -19.000000. running mean: -20.057001\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.066431\n",
            "resetting env. episode 27.000000, reward total was -19.000000. running mean: -20.055766\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.065209\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.074557\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.083811\n",
            "resetting env. episode 31.000000, reward total was -19.000000. running mean: -20.072973\n",
            "resetting env. episode 32.000000, reward total was -20.000000. running mean: -20.072243\n",
            "resetting env. episode 33.000000, reward total was -20.000000. running mean: -20.071521\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.080806\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.089998\n",
            "resetting env. episode 36.000000, reward total was -20.000000. running mean: -20.089098\n",
            "resetting env. episode 37.000000, reward total was -20.000000. running mean: -20.088207\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.097325\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -20.106351\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.115288\n",
            "resetting env. episode 41.000000, reward total was -20.000000. running mean: -20.114135\n",
            "resetting env. episode 42.000000, reward total was -20.000000. running mean: -20.112994\n",
            "resetting env. episode 43.000000, reward total was -19.000000. running mean: -20.101864\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.110845\n",
            "resetting env. episode 45.000000, reward total was -20.000000. running mean: -20.109737\n",
            "resetting env. episode 46.000000, reward total was -20.000000. running mean: -20.108639\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.117553\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.126377\n",
            "resetting env. episode 49.000000, reward total was -20.000000. running mean: -20.125114\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.133862\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.142524\n",
            "resetting env. episode 52.000000, reward total was -20.000000. running mean: -20.141099\n",
            "resetting env. episode 53.000000, reward total was -19.000000. running mean: -20.129688\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.138391\n",
            "resetting env. episode 55.000000, reward total was -20.000000. running mean: -20.137007\n",
            "resetting env. episode 56.000000, reward total was -20.000000. running mean: -20.135637\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.144280\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.152838\n",
            "resetting env. episode 59.000000, reward total was -20.000000. running mean: -20.151309\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.159796\n",
            "resetting env. episode 61.000000, reward total was -20.000000. running mean: -20.158198\n",
            "resetting env. episode 62.000000, reward total was -16.000000. running mean: -20.116616\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.125450\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.134195\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.142853\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.151425\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.159911\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.168312\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -20.166628\n",
            "resetting env. episode 70.000000, reward total was -18.000000. running mean: -20.144962\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.153513\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.161977\n",
            "resetting env. episode 73.000000, reward total was -20.000000. running mean: -20.160358\n",
            "resetting env. episode 74.000000, reward total was -20.000000. running mean: -20.158754\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.167167\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.175495\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.183740\n",
            "resetting env. episode 78.000000, reward total was -19.000000. running mean: -20.171903\n",
            "resetting env. episode 79.000000, reward total was -18.000000. running mean: -20.150184\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.158682\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.167095\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.175424\n",
            "resetting env. episode 83.000000, reward total was -20.000000. running mean: -20.173670\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.181933\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.190114\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.198213\n",
            "resetting env. episode 87.000000, reward total was -19.000000. running mean: -20.186230\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.194368\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.202424\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.210400\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.218296\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.226113\n",
            "resetting env. episode 93.000000, reward total was -20.000000. running mean: -20.223852\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.231614\n",
            "resetting env. episode 95.000000, reward total was -19.000000. running mean: -20.219297\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.227104\n",
            "resetting env. episode 97.000000, reward total was -20.000000. running mean: -20.224833\n",
            "resetting env. episode 98.000000, reward total was -20.000000. running mean: -20.222585\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.230359\n",
            "resetting env. episode 100.000000, reward total was -20.000000. running mean: -20.228056\n",
            "resetting env. episode 101.000000, reward total was -20.000000. running mean: -20.225775\n",
            "resetting env. episode 102.000000, reward total was -20.000000. running mean: -20.223517\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -20.231282\n",
            "resetting env. episode 104.000000, reward total was -20.000000. running mean: -20.228969\n",
            "resetting env. episode 105.000000, reward total was -19.000000. running mean: -20.216680\n",
            "resetting env. episode 106.000000, reward total was -17.000000. running mean: -20.184513\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.192668\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -20.200741\n",
            "resetting env. episode 109.000000, reward total was -19.000000. running mean: -20.188734\n",
            "resetting env. episode 110.000000, reward total was -19.000000. running mean: -20.176846\n",
            "resetting env. episode 111.000000, reward total was -19.000000. running mean: -20.165078\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -20.173427\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -20.171693\n",
            "resetting env. episode 114.000000, reward total was -20.000000. running mean: -20.169976\n",
            "resetting env. episode 115.000000, reward total was -19.000000. running mean: -20.158276\n",
            "resetting env. episode 116.000000, reward total was -20.000000. running mean: -20.156693\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.165126\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.173475\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.181740\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.189923\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.198024\n",
            "resetting env. episode 122.000000, reward total was -19.000000. running mean: -20.186043\n",
            "resetting env. episode 123.000000, reward total was -20.000000. running mean: -20.184183\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.192341\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.200418\n",
            "resetting env. episode 126.000000, reward total was -20.000000. running mean: -20.198414\n",
            "resetting env. episode 127.000000, reward total was -20.000000. running mean: -20.196429\n",
            "resetting env. episode 128.000000, reward total was -18.000000. running mean: -20.174465\n",
            "resetting env. episode 129.000000, reward total was -20.000000. running mean: -20.172721\n",
            "resetting env. episode 130.000000, reward total was -17.000000. running mean: -20.140993\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.149583\n",
            "resetting env. episode 132.000000, reward total was -18.000000. running mean: -20.128088\n",
            "resetting env. episode 133.000000, reward total was -19.000000. running mean: -20.116807\n",
            "resetting env. episode 134.000000, reward total was -18.000000. running mean: -20.095639\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.104682\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.113635\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.122499\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.131274\n",
            "resetting env. episode 139.000000, reward total was -20.000000. running mean: -20.129961\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.138662\n",
            "resetting env. episode 141.000000, reward total was -19.000000. running mean: -20.127275\n",
            "resetting env. episode 142.000000, reward total was -20.000000. running mean: -20.126002\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.134742\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.143395\n",
            "resetting env. episode 145.000000, reward total was -20.000000. running mean: -20.141961\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.150541\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -20.149036\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.157546\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.165970\n",
            "resetting env. episode 150.000000, reward total was -20.000000. running mean: -20.164310\n",
            "resetting env. episode 151.000000, reward total was -19.000000. running mean: -20.152667\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.161141\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.169529\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.177834\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.186056\n",
            "resetting env. episode 156.000000, reward total was -20.000000. running mean: -20.184195\n",
            "resetting env. episode 157.000000, reward total was -20.000000. running mean: -20.182353\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.190530\n",
            "resetting env. episode 159.000000, reward total was -20.000000. running mean: -20.188624\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.196738\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.204771\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.212723\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.220596\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.228390\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.236106\n",
            "resetting env. episode 166.000000, reward total was -19.000000. running mean: -20.223745\n",
            "resetting env. episode 167.000000, reward total was -18.000000. running mean: -20.201507\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.209492\n",
            "resetting env. episode 169.000000, reward total was -18.000000. running mean: -20.187397\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.195523\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.203568\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.211532\n",
            "resetting env. episode 173.000000, reward total was -20.000000. running mean: -20.209417\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.217323\n",
            "resetting env. episode 175.000000, reward total was -20.000000. running mean: -20.215150\n",
            "resetting env. episode 176.000000, reward total was -19.000000. running mean: -20.202998\n",
            "resetting env. episode 177.000000, reward total was -18.000000. running mean: -20.180968\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.189159\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.197267\n",
            "resetting env. episode 180.000000, reward total was -20.000000. running mean: -20.195294\n",
            "resetting env. episode 181.000000, reward total was -20.000000. running mean: -20.193341\n",
            "resetting env. episode 182.000000, reward total was -19.000000. running mean: -20.181408\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.189594\n",
            "resetting env. episode 184.000000, reward total was -20.000000. running mean: -20.187698\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.195821\n",
            "resetting env. episode 186.000000, reward total was -20.000000. running mean: -20.193863\n",
            "resetting env. episode 187.000000, reward total was -20.000000. running mean: -20.191924\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.200005\n",
            "resetting env. episode 189.000000, reward total was -19.000000. running mean: -20.188005\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.196125\n",
            "resetting env. episode 191.000000, reward total was -20.000000. running mean: -20.194164\n",
            "resetting env. episode 192.000000, reward total was -18.000000. running mean: -20.172222\n",
            "resetting env. episode 193.000000, reward total was -20.000000. running mean: -20.170500\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.178795\n",
            "resetting env. episode 195.000000, reward total was -20.000000. running mean: -20.177007\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.185237\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.193384\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.201450\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.209436\n",
            "resetting env. episode 200.000000, reward total was -17.000000. running mean: -20.177342\n",
            "resetting env. episode 201.000000, reward total was -20.000000. running mean: -20.175568\n",
            "resetting env. episode 202.000000, reward total was -20.000000. running mean: -20.173813\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.182074\n",
            "resetting env. episode 204.000000, reward total was -20.000000. running mean: -20.180254\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.188451\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.196567\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.204601\n",
            "resetting env. episode 208.000000, reward total was -20.000000. running mean: -20.202555\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.210529\n",
            "resetting env. episode 210.000000, reward total was -20.000000. running mean: -20.208424\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.216340\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.224176\n",
            "resetting env. episode 213.000000, reward total was -20.000000. running mean: -20.221935\n",
            "resetting env. episode 214.000000, reward total was -20.000000. running mean: -20.219715\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.227518\n",
            "resetting env. episode 216.000000, reward total was -19.000000. running mean: -20.215243\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.223091\n",
            "resetting env. episode 218.000000, reward total was -20.000000. running mean: -20.220860\n",
            "resetting env. episode 219.000000, reward total was -20.000000. running mean: -20.218651\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.226465\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.224200\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.231958\n",
            "resetting env. episode 223.000000, reward total was -19.000000. running mean: -20.219638\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.227442\n",
            "resetting env. episode 225.000000, reward total was -20.000000. running mean: -20.225168\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.232916\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.240587\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.248181\n",
            "resetting env. episode 229.000000, reward total was -19.000000. running mean: -20.235699\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.243342\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.250909\n",
            "resetting env. episode 232.000000, reward total was -20.000000. running mean: -20.248400\n",
            "resetting env. episode 233.000000, reward total was -20.000000. running mean: -20.245916\n",
            "resetting env. episode 234.000000, reward total was -20.000000. running mean: -20.243456\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.251022\n",
            "resetting env. episode 236.000000, reward total was -19.000000. running mean: -20.238512\n",
            "resetting env. episode 237.000000, reward total was -20.000000. running mean: -20.236126\n",
            "resetting env. episode 238.000000, reward total was -20.000000. running mean: -20.233765\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.241428\n",
            "resetting env. episode 240.000000, reward total was -19.000000. running mean: -20.229013\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.236723\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.244356\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.251912\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.259393\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.266799\n",
            "resetting env. episode 246.000000, reward total was -20.000000. running mean: -20.264131\n",
            "resetting env. episode 247.000000, reward total was -20.000000. running mean: -20.261490\n",
            "resetting env. episode 248.000000, reward total was -20.000000. running mean: -20.258875\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.266286\n",
            "resetting env. episode 250.000000, reward total was -20.000000. running mean: -20.263623\n",
            "resetting env. episode 251.000000, reward total was -20.000000. running mean: -20.260987\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.268377\n",
            "resetting env. episode 253.000000, reward total was -20.000000. running mean: -20.265694\n",
            "resetting env. episode 254.000000, reward total was -20.000000. running mean: -20.263037\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.270406\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.277702\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.284925\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.292076\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.299155\n",
            "resetting env. episode 260.000000, reward total was -19.000000. running mean: -20.286164\n",
            "resetting env. episode 261.000000, reward total was -20.000000. running mean: -20.283302\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.290469\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.297564\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.304589\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.311543\n",
            "resetting env. episode 266.000000, reward total was -20.000000. running mean: -20.308427\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.315343\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.322190\n",
            "resetting env. episode 269.000000, reward total was -20.000000. running mean: -20.318968\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.325778\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.332520\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.339195\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.345803\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.352345\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.358822\n",
            "resetting env. episode 276.000000, reward total was -20.000000. running mean: -20.355233\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.361681\n",
            "resetting env. episode 278.000000, reward total was -20.000000. running mean: -20.358064\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.364484\n",
            "resetting env. episode 280.000000, reward total was -19.000000. running mean: -20.350839\n",
            "resetting env. episode 281.000000, reward total was -20.000000. running mean: -20.347330\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.353857\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.360319\n",
            "resetting env. episode 284.000000, reward total was -20.000000. running mean: -20.356715\n",
            "resetting env. episode 285.000000, reward total was -20.000000. running mean: -20.353148\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.359617\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.366021\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.372360\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.378637\n",
            "resetting env. episode 290.000000, reward total was -20.000000. running mean: -20.374850\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.381102\n",
            "resetting env. episode 292.000000, reward total was -19.000000. running mean: -20.367291\n",
            "resetting env. episode 293.000000, reward total was -20.000000. running mean: -20.363618\n",
            "resetting env. episode 294.000000, reward total was -20.000000. running mean: -20.359982\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.366382\n",
            "resetting env. episode 296.000000, reward total was -20.000000. running mean: -20.362718\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.369091\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.375400\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -20.371646\n",
            "resetting env. episode 300.000000, reward total was -20.000000. running mean: -20.367930\n",
            "resetting env. episode 301.000000, reward total was -20.000000. running mean: -20.364250\n",
            "resetting env. episode 302.000000, reward total was -20.000000. running mean: -20.360608\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.367002\n",
            "resetting env. episode 304.000000, reward total was -19.000000. running mean: -20.353332\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.359798\n",
            "resetting env. episode 306.000000, reward total was -19.000000. running mean: -20.346200\n",
            "resetting env. episode 307.000000, reward total was -20.000000. running mean: -20.342738\n",
            "resetting env. episode 308.000000, reward total was -20.000000. running mean: -20.339311\n",
            "resetting env. episode 309.000000, reward total was -20.000000. running mean: -20.335918\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.342559\n",
            "resetting env. episode 311.000000, reward total was -19.000000. running mean: -20.329133\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.335842\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.342483\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.349059\n",
            "resetting env. episode 315.000000, reward total was -19.000000. running mean: -20.335568\n",
            "resetting env. episode 316.000000, reward total was -20.000000. running mean: -20.332212\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.338890\n",
            "resetting env. episode 318.000000, reward total was -20.000000. running mean: -20.335501\n",
            "resetting env. episode 319.000000, reward total was -20.000000. running mean: -20.332146\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.338825\n",
            "resetting env. episode 321.000000, reward total was -20.000000. running mean: -20.335437\n",
            "resetting env. episode 322.000000, reward total was -20.000000. running mean: -20.332082\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.338761\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.345374\n",
            "resetting env. episode 325.000000, reward total was -20.000000. running mean: -20.341920\n",
            "resetting env. episode 326.000000, reward total was -20.000000. running mean: -20.338501\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.345116\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.351665\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.358148\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.364566\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.370921\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.377212\n",
            "resetting env. episode 333.000000, reward total was -20.000000. running mean: -20.373440\n",
            "resetting env. episode 334.000000, reward total was -18.000000. running mean: -20.349705\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.356208\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.362646\n",
            "resetting env. episode 337.000000, reward total was -20.000000. running mean: -20.359020\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.365429\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.371775\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.378057\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.384277\n",
            "resetting env. episode 342.000000, reward total was -20.000000. running mean: -20.380434\n",
            "resetting env. episode 343.000000, reward total was -20.000000. running mean: -20.376630\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.382863\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.389035\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.395144\n",
            "resetting env. episode 347.000000, reward total was -20.000000. running mean: -20.391193\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.397281\n",
            "resetting env. episode 349.000000, reward total was -19.000000. running mean: -20.383308\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.389475\n",
            "resetting env. episode 351.000000, reward total was -19.000000. running mean: -20.375580\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.381825\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -20.378006\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.384226\n",
            "resetting env. episode 355.000000, reward total was -20.000000. running mean: -20.380384\n",
            "resetting env. episode 356.000000, reward total was -20.000000. running mean: -20.376580\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.382814\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.388986\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.395096\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.401145\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.407134\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -20.403063\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.409032\n",
            "resetting env. episode 364.000000, reward total was -19.000000. running mean: -20.394942\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.400992\n",
            "resetting env. episode 366.000000, reward total was -19.000000. running mean: -20.386982\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.393112\n",
            "resetting env. episode 368.000000, reward total was -20.000000. running mean: -20.389181\n",
            "resetting env. episode 369.000000, reward total was -19.000000. running mean: -20.375289\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.381537\n",
            "resetting env. episode 371.000000, reward total was -20.000000. running mean: -20.377721\n",
            "resetting env. episode 372.000000, reward total was -20.000000. running mean: -20.373944\n",
            "resetting env. episode 373.000000, reward total was -20.000000. running mean: -20.370205\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.376503\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.382738\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.388910\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.395021\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.401071\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.407060\n",
            "resetting env. episode 380.000000, reward total was -20.000000. running mean: -20.402990\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.408960\n",
            "resetting env. episode 382.000000, reward total was -18.000000. running mean: -20.384870\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.391021\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.397111\n",
            "resetting env. episode 385.000000, reward total was -20.000000. running mean: -20.393140\n",
            "resetting env. episode 386.000000, reward total was -20.000000. running mean: -20.389209\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.395317\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.401363\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.407350\n",
            "resetting env. episode 390.000000, reward total was -20.000000. running mean: -20.403276\n",
            "resetting env. episode 391.000000, reward total was -20.000000. running mean: -20.399243\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.405251\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.411199\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.417087\n",
            "resetting env. episode 395.000000, reward total was -20.000000. running mean: -20.412916\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.418787\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.424599\n",
            "resetting env. episode 398.000000, reward total was -20.000000. running mean: -20.420353\n",
            "resetting env. episode 399.000000, reward total was -17.000000. running mean: -20.386149\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.392288\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.398365\n",
            "resetting env. episode 402.000000, reward total was -19.000000. running mean: -20.384381\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.390537\n",
            "resetting env. episode 404.000000, reward total was -19.000000. running mean: -20.376632\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.382866\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.389037\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.395147\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.401195\n",
            "resetting env. episode 409.000000, reward total was -20.000000. running mean: -20.397183\n",
            "resetting env. episode 410.000000, reward total was -20.000000. running mean: -20.393211\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -20.389279\n",
            "resetting env. episode 412.000000, reward total was -19.000000. running mean: -20.375386\n",
            "resetting env. episode 413.000000, reward total was -18.000000. running mean: -20.351633\n",
            "resetting env. episode 414.000000, reward total was -18.000000. running mean: -20.328116\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -20.324835\n",
            "resetting env. episode 416.000000, reward total was -20.000000. running mean: -20.321587\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.328371\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.335087\n",
            "resetting env. episode 419.000000, reward total was -19.000000. running mean: -20.321736\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.328519\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.335234\n",
            "resetting env. episode 422.000000, reward total was -20.000000. running mean: -20.331881\n",
            "resetting env. episode 423.000000, reward total was -20.000000. running mean: -20.328563\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.335277\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.341924\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.348505\n",
            "resetting env. episode 427.000000, reward total was -19.000000. running mean: -20.335020\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.341670\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.348253\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.354770\n",
            "resetting env. episode 431.000000, reward total was -19.000000. running mean: -20.341223\n",
            "resetting env. episode 432.000000, reward total was -20.000000. running mean: -20.337811\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.344432\n",
            "resetting env. episode 434.000000, reward total was -19.000000. running mean: -20.330988\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.337678\n",
            "resetting env. episode 436.000000, reward total was -20.000000. running mean: -20.334301\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.340958\n",
            "resetting env. episode 438.000000, reward total was -20.000000. running mean: -20.337549\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.344173\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.350732\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -20.347224\n",
            "resetting env. episode 442.000000, reward total was -20.000000. running mean: -20.343752\n",
            "resetting env. episode 443.000000, reward total was -20.000000. running mean: -20.340315\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.346911\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.353442\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.359908\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.366309\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.372646\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.378919\n",
            "resetting env. episode 450.000000, reward total was -20.000000. running mean: -20.375130\n",
            "resetting env. episode 451.000000, reward total was -19.000000. running mean: -20.361379\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.367765\n",
            "resetting env. episode 453.000000, reward total was -20.000000. running mean: -20.364087\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.370446\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.376742\n",
            "resetting env. episode 456.000000, reward total was -20.000000. running mean: -20.372975\n",
            "resetting env. episode 457.000000, reward total was -20.000000. running mean: -20.369245\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.375552\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.381797\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.387979\n",
            "resetting env. episode 461.000000, reward total was -20.000000. running mean: -20.384099\n",
            "resetting env. episode 462.000000, reward total was -20.000000. running mean: -20.380258\n",
            "resetting env. episode 463.000000, reward total was -18.000000. running mean: -20.356456\n",
            "resetting env. episode 464.000000, reward total was -19.000000. running mean: -20.342891\n",
            "resetting env. episode 465.000000, reward total was -20.000000. running mean: -20.339462\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.346067\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.352607\n",
            "resetting env. episode 468.000000, reward total was -20.000000. running mean: -20.349081\n",
            "resetting env. episode 469.000000, reward total was -19.000000. running mean: -20.335590\n",
            "resetting env. episode 470.000000, reward total was -20.000000. running mean: -20.332234\n",
            "resetting env. episode 471.000000, reward total was -20.000000. running mean: -20.328912\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.335623\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -20.332266\n",
            "resetting env. episode 474.000000, reward total was -20.000000. running mean: -20.328944\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.335654\n",
            "resetting env. episode 476.000000, reward total was -20.000000. running mean: -20.332298\n",
            "resetting env. episode 477.000000, reward total was -20.000000. running mean: -20.328975\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.325685\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.332428\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.339104\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.345713\n",
            "resetting env. episode 482.000000, reward total was -20.000000. running mean: -20.342256\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.348833\n",
            "resetting env. episode 484.000000, reward total was -18.000000. running mean: -20.325345\n",
            "resetting env. episode 485.000000, reward total was -20.000000. running mean: -20.322091\n",
            "resetting env. episode 486.000000, reward total was -20.000000. running mean: -20.318870\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.325682\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.332425\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.339101\n",
            "resetting env. episode 490.000000, reward total was -20.000000. running mean: -20.335710\n",
            "resetting env. episode 491.000000, reward total was -18.000000. running mean: -20.312353\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.319229\n",
            "resetting env. episode 493.000000, reward total was -19.000000. running mean: -20.306037\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.312976\n",
            "resetting env. episode 495.000000, reward total was -19.000000. running mean: -20.299847\n",
            "resetting env. episode 496.000000, reward total was -20.000000. running mean: -20.296848\n",
            "resetting env. episode 497.000000, reward total was -20.000000. running mean: -20.293880\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.300941\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.307931\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.314852\n",
            "resetting env. episode 501.000000, reward total was -21.000000. running mean: -20.321704\n",
            "resetting env. episode 502.000000, reward total was -20.000000. running mean: -20.318487\n",
            "resetting env. episode 503.000000, reward total was -21.000000. running mean: -20.325302\n",
            "resetting env. episode 504.000000, reward total was -21.000000. running mean: -20.332049\n",
            "resetting env. episode 505.000000, reward total was -20.000000. running mean: -20.328728\n",
            "resetting env. episode 506.000000, reward total was -21.000000. running mean: -20.335441\n",
            "resetting env. episode 507.000000, reward total was -20.000000. running mean: -20.332086\n",
            "resetting env. episode 508.000000, reward total was -18.000000. running mean: -20.308766\n",
            "resetting env. episode 509.000000, reward total was -19.000000. running mean: -20.295678\n",
            "resetting env. episode 510.000000, reward total was -20.000000. running mean: -20.292721\n",
            "resetting env. episode 511.000000, reward total was -21.000000. running mean: -20.299794\n",
            "resetting env. episode 512.000000, reward total was -20.000000. running mean: -20.296796\n",
            "resetting env. episode 513.000000, reward total was -20.000000. running mean: -20.293828\n",
            "resetting env. episode 514.000000, reward total was -21.000000. running mean: -20.300890\n",
            "resetting env. episode 515.000000, reward total was -20.000000. running mean: -20.297881\n",
            "resetting env. episode 516.000000, reward total was -21.000000. running mean: -20.304902\n",
            "resetting env. episode 517.000000, reward total was -20.000000. running mean: -20.301853\n",
            "resetting env. episode 518.000000, reward total was -21.000000. running mean: -20.308835\n",
            "resetting env. episode 519.000000, reward total was -21.000000. running mean: -20.315746\n",
            "resetting env. episode 520.000000, reward total was -21.000000. running mean: -20.322589\n",
            "resetting env. episode 521.000000, reward total was -21.000000. running mean: -20.329363\n",
            "resetting env. episode 522.000000, reward total was -21.000000. running mean: -20.336069\n",
            "resetting env. episode 523.000000, reward total was -21.000000. running mean: -20.342709\n",
            "resetting env. episode 524.000000, reward total was -21.000000. running mean: -20.349281\n",
            "resetting env. episode 525.000000, reward total was -20.000000. running mean: -20.345789\n",
            "resetting env. episode 526.000000, reward total was -19.000000. running mean: -20.332331\n",
            "resetting env. episode 527.000000, reward total was -21.000000. running mean: -20.339007\n",
            "resetting env. episode 528.000000, reward total was -21.000000. running mean: -20.345617\n",
            "resetting env. episode 529.000000, reward total was -19.000000. running mean: -20.332161\n",
            "resetting env. episode 530.000000, reward total was -20.000000. running mean: -20.328840\n",
            "resetting env. episode 531.000000, reward total was -21.000000. running mean: -20.335551\n",
            "resetting env. episode 532.000000, reward total was -20.000000. running mean: -20.332196\n",
            "resetting env. episode 533.000000, reward total was -21.000000. running mean: -20.338874\n",
            "resetting env. episode 534.000000, reward total was -21.000000. running mean: -20.345485\n",
            "resetting env. episode 535.000000, reward total was -20.000000. running mean: -20.342030\n",
            "resetting env. episode 536.000000, reward total was -21.000000. running mean: -20.348610\n",
            "resetting env. episode 537.000000, reward total was -21.000000. running mean: -20.355124\n",
            "resetting env. episode 538.000000, reward total was -20.000000. running mean: -20.351572\n",
            "resetting env. episode 539.000000, reward total was -21.000000. running mean: -20.358057\n",
            "resetting env. episode 540.000000, reward total was -21.000000. running mean: -20.364476\n",
            "resetting env. episode 541.000000, reward total was -21.000000. running mean: -20.370831\n",
            "resetting env. episode 542.000000, reward total was -21.000000. running mean: -20.377123\n",
            "resetting env. episode 543.000000, reward total was -18.000000. running mean: -20.353352\n",
            "resetting env. episode 544.000000, reward total was -19.000000. running mean: -20.339818\n",
            "resetting env. episode 545.000000, reward total was -21.000000. running mean: -20.346420\n",
            "resetting env. episode 546.000000, reward total was -20.000000. running mean: -20.342956\n",
            "resetting env. episode 547.000000, reward total was -21.000000. running mean: -20.349526\n",
            "resetting env. episode 548.000000, reward total was -21.000000. running mean: -20.356031\n",
            "resetting env. episode 549.000000, reward total was -20.000000. running mean: -20.352471\n",
            "resetting env. episode 550.000000, reward total was -21.000000. running mean: -20.358946\n",
            "resetting env. episode 551.000000, reward total was -19.000000. running mean: -20.345357\n",
            "resetting env. episode 552.000000, reward total was -20.000000. running mean: -20.341903\n",
            "resetting env. episode 553.000000, reward total was -21.000000. running mean: -20.348484\n",
            "resetting env. episode 554.000000, reward total was -20.000000. running mean: -20.344999\n",
            "resetting env. episode 555.000000, reward total was -19.000000. running mean: -20.331549\n",
            "resetting env. episode 556.000000, reward total was -19.000000. running mean: -20.318234\n",
            "resetting env. episode 557.000000, reward total was -21.000000. running mean: -20.325051\n",
            "resetting env. episode 558.000000, reward total was -21.000000. running mean: -20.331801\n",
            "resetting env. episode 559.000000, reward total was -21.000000. running mean: -20.338483\n",
            "resetting env. episode 560.000000, reward total was -20.000000. running mean: -20.335098\n",
            "resetting env. episode 561.000000, reward total was -21.000000. running mean: -20.341747\n",
            "resetting env. episode 562.000000, reward total was -21.000000. running mean: -20.348330\n",
            "resetting env. episode 563.000000, reward total was -21.000000. running mean: -20.354846\n",
            "resetting env. episode 564.000000, reward total was -21.000000. running mean: -20.361298\n",
            "resetting env. episode 565.000000, reward total was -21.000000. running mean: -20.367685\n",
            "resetting env. episode 566.000000, reward total was -19.000000. running mean: -20.354008\n",
            "resetting env. episode 567.000000, reward total was -21.000000. running mean: -20.360468\n",
            "resetting env. episode 568.000000, reward total was -19.000000. running mean: -20.346863\n",
            "resetting env. episode 569.000000, reward total was -21.000000. running mean: -20.353395\n",
            "resetting env. episode 570.000000, reward total was -20.000000. running mean: -20.349861\n",
            "resetting env. episode 571.000000, reward total was -19.000000. running mean: -20.336362\n",
            "resetting env. episode 572.000000, reward total was -19.000000. running mean: -20.322998\n",
            "resetting env. episode 573.000000, reward total was -21.000000. running mean: -20.329768\n",
            "resetting env. episode 574.000000, reward total was -21.000000. running mean: -20.336471\n",
            "resetting env. episode 575.000000, reward total was -20.000000. running mean: -20.333106\n",
            "resetting env. episode 576.000000, reward total was -20.000000. running mean: -20.329775\n",
            "resetting env. episode 577.000000, reward total was -20.000000. running mean: -20.326477\n",
            "resetting env. episode 578.000000, reward total was -18.000000. running mean: -20.303212\n",
            "resetting env. episode 579.000000, reward total was -21.000000. running mean: -20.310180\n",
            "resetting env. episode 580.000000, reward total was -20.000000. running mean: -20.307079\n",
            "resetting env. episode 581.000000, reward total was -21.000000. running mean: -20.314008\n",
            "resetting env. episode 582.000000, reward total was -21.000000. running mean: -20.320868\n",
            "resetting env. episode 583.000000, reward total was -21.000000. running mean: -20.327659\n",
            "resetting env. episode 584.000000, reward total was -21.000000. running mean: -20.334382\n",
            "resetting env. episode 585.000000, reward total was -21.000000. running mean: -20.341039\n",
            "resetting env. episode 586.000000, reward total was -21.000000. running mean: -20.347628\n",
            "resetting env. episode 587.000000, reward total was -19.000000. running mean: -20.334152\n",
            "resetting env. episode 588.000000, reward total was -21.000000. running mean: -20.340810\n",
            "resetting env. episode 589.000000, reward total was -21.000000. running mean: -20.347402\n",
            "resetting env. episode 590.000000, reward total was -21.000000. running mean: -20.353928\n",
            "resetting env. episode 591.000000, reward total was -19.000000. running mean: -20.340389\n",
            "resetting env. episode 592.000000, reward total was -20.000000. running mean: -20.336985\n",
            "resetting env. episode 593.000000, reward total was -21.000000. running mean: -20.343615\n",
            "resetting env. episode 594.000000, reward total was -20.000000. running mean: -20.340179\n",
            "resetting env. episode 595.000000, reward total was -20.000000. running mean: -20.336777\n",
            "resetting env. episode 596.000000, reward total was -21.000000. running mean: -20.343410\n",
            "resetting env. episode 597.000000, reward total was -19.000000. running mean: -20.329975\n",
            "resetting env. episode 598.000000, reward total was -21.000000. running mean: -20.336676\n",
            "resetting env. episode 599.000000, reward total was -21.000000. running mean: -20.343309\n",
            "resetting env. episode 600.000000, reward total was -21.000000. running mean: -20.349876\n",
            "resetting env. episode 601.000000, reward total was -21.000000. running mean: -20.356377\n",
            "resetting env. episode 602.000000, reward total was -20.000000. running mean: -20.352813\n",
            "resetting env. episode 603.000000, reward total was -21.000000. running mean: -20.359285\n",
            "resetting env. episode 604.000000, reward total was -21.000000. running mean: -20.365692\n",
            "resetting env. episode 605.000000, reward total was -20.000000. running mean: -20.362035\n",
            "resetting env. episode 606.000000, reward total was -20.000000. running mean: -20.358415\n",
            "resetting env. episode 607.000000, reward total was -20.000000. running mean: -20.354831\n",
            "resetting env. episode 608.000000, reward total was -21.000000. running mean: -20.361283\n",
            "resetting env. episode 609.000000, reward total was -19.000000. running mean: -20.347670\n",
            "resetting env. episode 610.000000, reward total was -21.000000. running mean: -20.354193\n",
            "resetting env. episode 611.000000, reward total was -19.000000. running mean: -20.340651\n",
            "resetting env. episode 612.000000, reward total was -21.000000. running mean: -20.347245\n",
            "resetting env. episode 613.000000, reward total was -18.000000. running mean: -20.323772\n",
            "resetting env. episode 614.000000, reward total was -20.000000. running mean: -20.320534\n",
            "resetting env. episode 615.000000, reward total was -20.000000. running mean: -20.317329\n",
            "resetting env. episode 616.000000, reward total was -19.000000. running mean: -20.304156\n",
            "resetting env. episode 617.000000, reward total was -20.000000. running mean: -20.301114\n",
            "resetting env. episode 618.000000, reward total was -21.000000. running mean: -20.308103\n",
            "resetting env. episode 619.000000, reward total was -20.000000. running mean: -20.305022\n",
            "resetting env. episode 620.000000, reward total was -21.000000. running mean: -20.311972\n",
            "resetting env. episode 621.000000, reward total was -18.000000. running mean: -20.288852\n",
            "resetting env. episode 622.000000, reward total was -20.000000. running mean: -20.285964\n",
            "resetting env. episode 623.000000, reward total was -19.000000. running mean: -20.273104\n",
            "resetting env. episode 624.000000, reward total was -19.000000. running mean: -20.260373\n",
            "resetting env. episode 625.000000, reward total was -19.000000. running mean: -20.247769\n",
            "resetting env. episode 626.000000, reward total was -21.000000. running mean: -20.255292\n",
            "resetting env. episode 627.000000, reward total was -17.000000. running mean: -20.222739\n",
            "resetting env. episode 628.000000, reward total was -19.000000. running mean: -20.210511\n",
            "resetting env. episode 629.000000, reward total was -20.000000. running mean: -20.208406\n",
            "resetting env. episode 630.000000, reward total was -19.000000. running mean: -20.196322\n",
            "resetting env. episode 631.000000, reward total was -21.000000. running mean: -20.204359\n",
            "resetting env. episode 632.000000, reward total was -21.000000. running mean: -20.212315\n",
            "resetting env. episode 633.000000, reward total was -21.000000. running mean: -20.220192\n",
            "resetting env. episode 634.000000, reward total was -20.000000. running mean: -20.217990\n",
            "resetting env. episode 635.000000, reward total was -21.000000. running mean: -20.225810\n",
            "resetting env. episode 636.000000, reward total was -21.000000. running mean: -20.233552\n",
            "resetting env. episode 637.000000, reward total was -21.000000. running mean: -20.241217\n",
            "resetting env. episode 638.000000, reward total was -21.000000. running mean: -20.248804\n",
            "resetting env. episode 639.000000, reward total was -20.000000. running mean: -20.246316\n",
            "resetting env. episode 640.000000, reward total was -21.000000. running mean: -20.253853\n",
            "resetting env. episode 641.000000, reward total was -21.000000. running mean: -20.261315\n",
            "resetting env. episode 642.000000, reward total was -20.000000. running mean: -20.258702\n",
            "resetting env. episode 643.000000, reward total was -20.000000. running mean: -20.256115\n",
            "resetting env. episode 644.000000, reward total was -21.000000. running mean: -20.263553\n",
            "resetting env. episode 645.000000, reward total was -20.000000. running mean: -20.260918\n",
            "resetting env. episode 646.000000, reward total was -21.000000. running mean: -20.268309\n",
            "resetting env. episode 647.000000, reward total was -19.000000. running mean: -20.255626\n",
            "resetting env. episode 648.000000, reward total was -19.000000. running mean: -20.243069\n",
            "resetting env. episode 649.000000, reward total was -19.000000. running mean: -20.230639\n",
            "resetting env. episode 650.000000, reward total was -20.000000. running mean: -20.228332\n",
            "resetting env. episode 651.000000, reward total was -20.000000. running mean: -20.226049\n",
            "resetting env. episode 652.000000, reward total was -21.000000. running mean: -20.233788\n",
            "resetting env. episode 653.000000, reward total was -20.000000. running mean: -20.231451\n",
            "resetting env. episode 654.000000, reward total was -19.000000. running mean: -20.219136\n",
            "resetting env. episode 655.000000, reward total was -20.000000. running mean: -20.216945\n",
            "resetting env. episode 656.000000, reward total was -21.000000. running mean: -20.224775\n",
            "resetting env. episode 657.000000, reward total was -20.000000. running mean: -20.222528\n",
            "resetting env. episode 658.000000, reward total was -19.000000. running mean: -20.210302\n",
            "resetting env. episode 659.000000, reward total was -19.000000. running mean: -20.198199\n",
            "resetting env. episode 660.000000, reward total was -20.000000. running mean: -20.196217\n",
            "resetting env. episode 661.000000, reward total was -21.000000. running mean: -20.204255\n",
            "resetting env. episode 662.000000, reward total was -19.000000. running mean: -20.192213\n",
            "resetting env. episode 663.000000, reward total was -21.000000. running mean: -20.200290\n",
            "resetting env. episode 664.000000, reward total was -21.000000. running mean: -20.208287\n",
            "resetting env. episode 665.000000, reward total was -21.000000. running mean: -20.216205\n",
            "resetting env. episode 666.000000, reward total was -20.000000. running mean: -20.214043\n",
            "resetting env. episode 667.000000, reward total was -19.000000. running mean: -20.201902\n",
            "resetting env. episode 668.000000, reward total was -21.000000. running mean: -20.209883\n",
            "resetting env. episode 669.000000, reward total was -21.000000. running mean: -20.217784\n",
            "resetting env. episode 670.000000, reward total was -18.000000. running mean: -20.195606\n",
            "resetting env. episode 671.000000, reward total was -20.000000. running mean: -20.193650\n",
            "resetting env. episode 672.000000, reward total was -21.000000. running mean: -20.201714\n",
            "resetting env. episode 673.000000, reward total was -21.000000. running mean: -20.209697\n",
            "resetting env. episode 674.000000, reward total was -20.000000. running mean: -20.207600\n",
            "resetting env. episode 675.000000, reward total was -21.000000. running mean: -20.215524\n",
            "resetting env. episode 676.000000, reward total was -18.000000. running mean: -20.193369\n",
            "resetting env. episode 677.000000, reward total was -20.000000. running mean: -20.191435\n",
            "resetting env. episode 678.000000, reward total was -20.000000. running mean: -20.189521\n",
            "resetting env. episode 679.000000, reward total was -20.000000. running mean: -20.187625\n",
            "resetting env. episode 680.000000, reward total was -21.000000. running mean: -20.195749\n",
            "resetting env. episode 681.000000, reward total was -21.000000. running mean: -20.203792\n",
            "resetting env. episode 682.000000, reward total was -20.000000. running mean: -20.201754\n",
            "resetting env. episode 683.000000, reward total was -20.000000. running mean: -20.199736\n",
            "resetting env. episode 684.000000, reward total was -20.000000. running mean: -20.197739\n",
            "resetting env. episode 685.000000, reward total was -21.000000. running mean: -20.205761\n",
            "resetting env. episode 686.000000, reward total was -19.000000. running mean: -20.193704\n",
            "resetting env. episode 687.000000, reward total was -21.000000. running mean: -20.201767\n",
            "resetting env. episode 688.000000, reward total was -20.000000. running mean: -20.199749\n",
            "resetting env. episode 689.000000, reward total was -21.000000. running mean: -20.207752\n",
            "resetting env. episode 690.000000, reward total was -20.000000. running mean: -20.205674\n",
            "resetting env. episode 691.000000, reward total was -18.000000. running mean: -20.183617\n",
            "resetting env. episode 692.000000, reward total was -21.000000. running mean: -20.191781\n",
            "resetting env. episode 693.000000, reward total was -20.000000. running mean: -20.189863\n",
            "resetting env. episode 694.000000, reward total was -20.000000. running mean: -20.187965\n",
            "resetting env. episode 695.000000, reward total was -20.000000. running mean: -20.186085\n",
            "resetting env. episode 696.000000, reward total was -17.000000. running mean: -20.154224\n",
            "resetting env. episode 697.000000, reward total was -18.000000. running mean: -20.132682\n",
            "resetting env. episode 698.000000, reward total was -21.000000. running mean: -20.141355\n",
            "resetting env. episode 699.000000, reward total was -20.000000. running mean: -20.139942\n",
            "resetting env. episode 700.000000, reward total was -21.000000. running mean: -20.148542\n",
            "resetting env. episode 701.000000, reward total was -21.000000. running mean: -20.157057\n",
            "resetting env. episode 702.000000, reward total was -21.000000. running mean: -20.165486\n",
            "resetting env. episode 703.000000, reward total was -21.000000. running mean: -20.173831\n",
            "resetting env. episode 704.000000, reward total was -21.000000. running mean: -20.182093\n",
            "resetting env. episode 705.000000, reward total was -20.000000. running mean: -20.180272\n",
            "resetting env. episode 706.000000, reward total was -21.000000. running mean: -20.188469\n",
            "resetting env. episode 707.000000, reward total was -19.000000. running mean: -20.176585\n",
            "resetting env. episode 708.000000, reward total was -19.000000. running mean: -20.164819\n",
            "resetting env. episode 709.000000, reward total was -21.000000. running mean: -20.173171\n",
            "resetting env. episode 710.000000, reward total was -20.000000. running mean: -20.171439\n",
            "resetting env. episode 711.000000, reward total was -21.000000. running mean: -20.179725\n",
            "resetting env. episode 712.000000, reward total was -20.000000. running mean: -20.177927\n",
            "resetting env. episode 713.000000, reward total was -21.000000. running mean: -20.186148\n",
            "resetting env. episode 714.000000, reward total was -21.000000. running mean: -20.194287\n",
            "resetting env. episode 715.000000, reward total was -21.000000. running mean: -20.202344\n",
            "resetting env. episode 716.000000, reward total was -18.000000. running mean: -20.180320\n",
            "resetting env. episode 717.000000, reward total was -21.000000. running mean: -20.188517\n",
            "resetting env. episode 718.000000, reward total was -20.000000. running mean: -20.186632\n",
            "resetting env. episode 719.000000, reward total was -21.000000. running mean: -20.194766\n",
            "resetting env. episode 720.000000, reward total was -19.000000. running mean: -20.182818\n",
            "resetting env. episode 721.000000, reward total was -19.000000. running mean: -20.170990\n",
            "resetting env. episode 722.000000, reward total was -21.000000. running mean: -20.179280\n",
            "resetting env. episode 723.000000, reward total was -21.000000. running mean: -20.187487\n",
            "resetting env. episode 724.000000, reward total was -20.000000. running mean: -20.185612\n",
            "resetting env. episode 725.000000, reward total was -20.000000. running mean: -20.183756\n",
            "resetting env. episode 726.000000, reward total was -21.000000. running mean: -20.191918\n",
            "resetting env. episode 727.000000, reward total was -20.000000. running mean: -20.189999\n",
            "resetting env. episode 728.000000, reward total was -21.000000. running mean: -20.198099\n",
            "resetting env. episode 729.000000, reward total was -21.000000. running mean: -20.206118\n",
            "resetting env. episode 730.000000, reward total was -20.000000. running mean: -20.204057\n",
            "resetting env. episode 731.000000, reward total was -21.000000. running mean: -20.212017\n",
            "resetting env. episode 732.000000, reward total was -21.000000. running mean: -20.219896\n",
            "resetting env. episode 733.000000, reward total was -20.000000. running mean: -20.217697\n",
            "resetting env. episode 734.000000, reward total was -20.000000. running mean: -20.215520\n",
            "resetting env. episode 735.000000, reward total was -21.000000. running mean: -20.223365\n",
            "resetting env. episode 736.000000, reward total was -21.000000. running mean: -20.231132\n",
            "resetting env. episode 737.000000, reward total was -21.000000. running mean: -20.238820\n",
            "resetting env. episode 738.000000, reward total was -21.000000. running mean: -20.246432\n",
            "resetting env. episode 739.000000, reward total was -21.000000. running mean: -20.253968\n",
            "resetting env. episode 740.000000, reward total was -21.000000. running mean: -20.261428\n",
            "resetting env. episode 741.000000, reward total was -21.000000. running mean: -20.268814\n",
            "resetting env. episode 742.000000, reward total was -19.000000. running mean: -20.256126\n",
            "resetting env. episode 743.000000, reward total was -21.000000. running mean: -20.263564\n",
            "resetting env. episode 744.000000, reward total was -20.000000. running mean: -20.260929\n",
            "resetting env. episode 745.000000, reward total was -21.000000. running mean: -20.268319\n",
            "resetting env. episode 746.000000, reward total was -20.000000. running mean: -20.265636\n",
            "resetting env. episode 747.000000, reward total was -21.000000. running mean: -20.272980\n",
            "resetting env. episode 748.000000, reward total was -20.000000. running mean: -20.270250\n",
            "resetting env. episode 749.000000, reward total was -21.000000. running mean: -20.277548\n",
            "resetting env. episode 750.000000, reward total was -21.000000. running mean: -20.284772\n",
            "resetting env. episode 751.000000, reward total was -21.000000. running mean: -20.291924\n",
            "resetting env. episode 752.000000, reward total was -21.000000. running mean: -20.299005\n",
            "resetting env. episode 753.000000, reward total was -20.000000. running mean: -20.296015\n",
            "resetting env. episode 754.000000, reward total was -17.000000. running mean: -20.263055\n",
            "resetting env. episode 755.000000, reward total was -20.000000. running mean: -20.260424\n",
            "resetting env. episode 756.000000, reward total was -18.000000. running mean: -20.237820\n",
            "resetting env. episode 757.000000, reward total was -21.000000. running mean: -20.245442\n",
            "resetting env. episode 758.000000, reward total was -19.000000. running mean: -20.232988\n",
            "resetting env. episode 759.000000, reward total was -20.000000. running mean: -20.230658\n",
            "resetting env. episode 760.000000, reward total was -21.000000. running mean: -20.238351\n",
            "resetting env. episode 761.000000, reward total was -19.000000. running mean: -20.225968\n",
            "resetting env. episode 762.000000, reward total was -20.000000. running mean: -20.223708\n",
            "resetting env. episode 763.000000, reward total was -19.000000. running mean: -20.211471\n",
            "resetting env. episode 764.000000, reward total was -21.000000. running mean: -20.219356\n",
            "resetting env. episode 765.000000, reward total was -21.000000. running mean: -20.227163\n",
            "resetting env. episode 766.000000, reward total was -20.000000. running mean: -20.224891\n",
            "resetting env. episode 767.000000, reward total was -19.000000. running mean: -20.212642\n",
            "resetting env. episode 768.000000, reward total was -21.000000. running mean: -20.220516\n",
            "resetting env. episode 769.000000, reward total was -21.000000. running mean: -20.228310\n",
            "resetting env. episode 770.000000, reward total was -21.000000. running mean: -20.236027\n",
            "resetting env. episode 771.000000, reward total was -21.000000. running mean: -20.243667\n",
            "resetting env. episode 772.000000, reward total was -20.000000. running mean: -20.241230\n",
            "resetting env. episode 773.000000, reward total was -19.000000. running mean: -20.228818\n",
            "resetting env. episode 774.000000, reward total was -20.000000. running mean: -20.226530\n",
            "resetting env. episode 775.000000, reward total was -21.000000. running mean: -20.234265\n",
            "resetting env. episode 776.000000, reward total was -20.000000. running mean: -20.231922\n",
            "resetting env. episode 777.000000, reward total was -19.000000. running mean: -20.219603\n",
            "resetting env. episode 778.000000, reward total was -21.000000. running mean: -20.227407\n",
            "resetting env. episode 779.000000, reward total was -20.000000. running mean: -20.225133\n",
            "resetting env. episode 780.000000, reward total was -21.000000. running mean: -20.232881\n",
            "resetting env. episode 781.000000, reward total was -20.000000. running mean: -20.230552\n",
            "resetting env. episode 782.000000, reward total was -21.000000. running mean: -20.238247\n",
            "resetting env. episode 783.000000, reward total was -21.000000. running mean: -20.245865\n",
            "resetting env. episode 784.000000, reward total was -19.000000. running mean: -20.233406\n",
            "resetting env. episode 785.000000, reward total was -21.000000. running mean: -20.241072\n",
            "resetting env. episode 786.000000, reward total was -19.000000. running mean: -20.228661\n",
            "resetting env. episode 787.000000, reward total was -20.000000. running mean: -20.226374\n",
            "resetting env. episode 788.000000, reward total was -21.000000. running mean: -20.234111\n",
            "resetting env. episode 789.000000, reward total was -21.000000. running mean: -20.241770\n",
            "resetting env. episode 790.000000, reward total was -21.000000. running mean: -20.249352\n",
            "resetting env. episode 791.000000, reward total was -21.000000. running mean: -20.256858\n",
            "resetting env. episode 792.000000, reward total was -21.000000. running mean: -20.264290\n",
            "resetting env. episode 793.000000, reward total was -19.000000. running mean: -20.251647\n",
            "resetting env. episode 794.000000, reward total was -21.000000. running mean: -20.259130\n",
            "resetting env. episode 795.000000, reward total was -21.000000. running mean: -20.266539\n",
            "resetting env. episode 796.000000, reward total was -21.000000. running mean: -20.273874\n",
            "resetting env. episode 797.000000, reward total was -20.000000. running mean: -20.271135\n",
            "resetting env. episode 798.000000, reward total was -20.000000. running mean: -20.268424\n",
            "resetting env. episode 799.000000, reward total was -21.000000. running mean: -20.275739\n",
            "resetting env. episode 800.000000, reward total was -21.000000. running mean: -20.282982\n",
            "resetting env. episode 801.000000, reward total was -21.000000. running mean: -20.290152\n",
            "resetting env. episode 802.000000, reward total was -21.000000. running mean: -20.297251\n",
            "resetting env. episode 803.000000, reward total was -21.000000. running mean: -20.304278\n",
            "resetting env. episode 804.000000, reward total was -21.000000. running mean: -20.311235\n",
            "resetting env. episode 805.000000, reward total was -20.000000. running mean: -20.308123\n",
            "resetting env. episode 806.000000, reward total was -21.000000. running mean: -20.315042\n",
            "resetting env. episode 807.000000, reward total was -20.000000. running mean: -20.311891\n",
            "resetting env. episode 808.000000, reward total was -18.000000. running mean: -20.288772\n",
            "resetting env. episode 809.000000, reward total was -21.000000. running mean: -20.295885\n",
            "resetting env. episode 810.000000, reward total was -20.000000. running mean: -20.292926\n",
            "resetting env. episode 811.000000, reward total was -19.000000. running mean: -20.279997\n",
            "resetting env. episode 812.000000, reward total was -21.000000. running mean: -20.287197\n",
            "resetting env. episode 813.000000, reward total was -21.000000. running mean: -20.294325\n",
            "resetting env. episode 814.000000, reward total was -20.000000. running mean: -20.291381\n",
            "resetting env. episode 815.000000, reward total was -20.000000. running mean: -20.288468\n",
            "resetting env. episode 816.000000, reward total was -20.000000. running mean: -20.285583\n",
            "resetting env. episode 817.000000, reward total was -20.000000. running mean: -20.282727\n",
            "resetting env. episode 818.000000, reward total was -20.000000. running mean: -20.279900\n",
            "resetting env. episode 819.000000, reward total was -20.000000. running mean: -20.277101\n",
            "resetting env. episode 820.000000, reward total was -21.000000. running mean: -20.284330\n",
            "resetting env. episode 821.000000, reward total was -21.000000. running mean: -20.291487\n",
            "resetting env. episode 822.000000, reward total was -21.000000. running mean: -20.298572\n",
            "resetting env. episode 823.000000, reward total was -21.000000. running mean: -20.305586\n",
            "resetting env. episode 824.000000, reward total was -19.000000. running mean: -20.292530\n",
            "resetting env. episode 825.000000, reward total was -20.000000. running mean: -20.289605\n",
            "resetting env. episode 826.000000, reward total was -21.000000. running mean: -20.296709\n",
            "resetting env. episode 827.000000, reward total was -21.000000. running mean: -20.303742\n",
            "resetting env. episode 828.000000, reward total was -20.000000. running mean: -20.300704\n",
            "resetting env. episode 829.000000, reward total was -19.000000. running mean: -20.287697\n",
            "resetting env. episode 830.000000, reward total was -21.000000. running mean: -20.294820\n",
            "resetting env. episode 831.000000, reward total was -19.000000. running mean: -20.281872\n",
            "resetting env. episode 832.000000, reward total was -19.000000. running mean: -20.269053\n",
            "resetting env. episode 833.000000, reward total was -20.000000. running mean: -20.266363\n",
            "resetting env. episode 834.000000, reward total was -21.000000. running mean: -20.273699\n",
            "resetting env. episode 835.000000, reward total was -20.000000. running mean: -20.270962\n",
            "resetting env. episode 836.000000, reward total was -21.000000. running mean: -20.278253\n",
            "resetting env. episode 837.000000, reward total was -20.000000. running mean: -20.275470\n",
            "resetting env. episode 838.000000, reward total was -20.000000. running mean: -20.272715\n",
            "resetting env. episode 839.000000, reward total was -21.000000. running mean: -20.279988\n",
            "resetting env. episode 840.000000, reward total was -20.000000. running mean: -20.277188\n",
            "resetting env. episode 841.000000, reward total was -20.000000. running mean: -20.274416\n",
            "resetting env. episode 842.000000, reward total was -18.000000. running mean: -20.251672\n",
            "resetting env. episode 843.000000, reward total was -21.000000. running mean: -20.259156\n",
            "resetting env. episode 844.000000, reward total was -21.000000. running mean: -20.266564\n",
            "resetting env. episode 845.000000, reward total was -21.000000. running mean: -20.273898\n",
            "resetting env. episode 846.000000, reward total was -21.000000. running mean: -20.281159\n",
            "resetting env. episode 847.000000, reward total was -20.000000. running mean: -20.278348\n",
            "resetting env. episode 848.000000, reward total was -20.000000. running mean: -20.275564\n",
            "resetting env. episode 849.000000, reward total was -21.000000. running mean: -20.282809\n",
            "resetting env. episode 850.000000, reward total was -20.000000. running mean: -20.279981\n",
            "resetting env. episode 851.000000, reward total was -20.000000. running mean: -20.277181\n",
            "resetting env. episode 852.000000, reward total was -21.000000. running mean: -20.284409\n",
            "resetting env. episode 853.000000, reward total was -21.000000. running mean: -20.291565\n",
            "resetting env. episode 854.000000, reward total was -20.000000. running mean: -20.288649\n",
            "resetting env. episode 855.000000, reward total was -20.000000. running mean: -20.285763\n",
            "resetting env. episode 856.000000, reward total was -20.000000. running mean: -20.282905\n",
            "resetting env. episode 857.000000, reward total was -21.000000. running mean: -20.290076\n",
            "resetting env. episode 858.000000, reward total was -20.000000. running mean: -20.287175\n",
            "resetting env. episode 859.000000, reward total was -20.000000. running mean: -20.284304\n",
            "resetting env. episode 860.000000, reward total was -20.000000. running mean: -20.281460\n",
            "resetting env. episode 861.000000, reward total was -18.000000. running mean: -20.258646\n",
            "resetting env. episode 862.000000, reward total was -21.000000. running mean: -20.266059\n",
            "resetting env. episode 863.000000, reward total was -21.000000. running mean: -20.273399\n",
            "resetting env. episode 864.000000, reward total was -21.000000. running mean: -20.280665\n",
            "resetting env. episode 865.000000, reward total was -20.000000. running mean: -20.277858\n",
            "resetting env. episode 866.000000, reward total was -21.000000. running mean: -20.285080\n",
            "resetting env. episode 867.000000, reward total was -20.000000. running mean: -20.282229\n",
            "resetting env. episode 868.000000, reward total was -20.000000. running mean: -20.279407\n",
            "resetting env. episode 869.000000, reward total was -20.000000. running mean: -20.276612\n",
            "resetting env. episode 870.000000, reward total was -18.000000. running mean: -20.253846\n",
            "resetting env. episode 871.000000, reward total was -21.000000. running mean: -20.261308\n",
            "resetting env. episode 872.000000, reward total was -21.000000. running mean: -20.268695\n",
            "resetting env. episode 873.000000, reward total was -20.000000. running mean: -20.266008\n",
            "resetting env. episode 874.000000, reward total was -21.000000. running mean: -20.273348\n",
            "resetting env. episode 875.000000, reward total was -21.000000. running mean: -20.280614\n",
            "resetting env. episode 876.000000, reward total was -19.000000. running mean: -20.267808\n",
            "resetting env. episode 877.000000, reward total was -21.000000. running mean: -20.275130\n",
            "resetting env. episode 878.000000, reward total was -21.000000. running mean: -20.282379\n",
            "resetting env. episode 879.000000, reward total was -20.000000. running mean: -20.279555\n",
            "resetting env. episode 880.000000, reward total was -20.000000. running mean: -20.276759\n",
            "resetting env. episode 881.000000, reward total was -21.000000. running mean: -20.283992\n",
            "resetting env. episode 882.000000, reward total was -21.000000. running mean: -20.291152\n",
            "resetting env. episode 883.000000, reward total was -21.000000. running mean: -20.298240\n",
            "resetting env. episode 884.000000, reward total was -21.000000. running mean: -20.305258\n",
            "resetting env. episode 885.000000, reward total was -20.000000. running mean: -20.302205\n",
            "resetting env. episode 886.000000, reward total was -21.000000. running mean: -20.309183\n",
            "resetting env. episode 887.000000, reward total was -20.000000. running mean: -20.306092\n",
            "resetting env. episode 888.000000, reward total was -19.000000. running mean: -20.293031\n",
            "resetting env. episode 889.000000, reward total was -20.000000. running mean: -20.290100\n",
            "resetting env. episode 890.000000, reward total was -20.000000. running mean: -20.287199\n",
            "resetting env. episode 891.000000, reward total was -19.000000. running mean: -20.274327\n",
            "resetting env. episode 892.000000, reward total was -21.000000. running mean: -20.281584\n",
            "resetting env. episode 893.000000, reward total was -20.000000. running mean: -20.278768\n",
            "resetting env. episode 894.000000, reward total was -20.000000. running mean: -20.275981\n",
            "resetting env. episode 895.000000, reward total was -19.000000. running mean: -20.263221\n",
            "resetting env. episode 896.000000, reward total was -21.000000. running mean: -20.270588\n",
            "resetting env. episode 897.000000, reward total was -21.000000. running mean: -20.277883\n",
            "resetting env. episode 898.000000, reward total was -21.000000. running mean: -20.285104\n",
            "resetting env. episode 899.000000, reward total was -19.000000. running mean: -20.272253\n",
            "resetting env. episode 900.000000, reward total was -20.000000. running mean: -20.269530\n",
            "resetting env. episode 901.000000, reward total was -19.000000. running mean: -20.256835\n",
            "resetting env. episode 902.000000, reward total was -20.000000. running mean: -20.254267\n",
            "resetting env. episode 903.000000, reward total was -21.000000. running mean: -20.261724\n",
            "resetting env. episode 904.000000, reward total was -21.000000. running mean: -20.269107\n",
            "resetting env. episode 905.000000, reward total was -21.000000. running mean: -20.276416\n",
            "resetting env. episode 906.000000, reward total was -21.000000. running mean: -20.283651\n",
            "resetting env. episode 907.000000, reward total was -20.000000. running mean: -20.280815\n",
            "resetting env. episode 908.000000, reward total was -21.000000. running mean: -20.288007\n",
            "resetting env. episode 909.000000, reward total was -21.000000. running mean: -20.295127\n",
            "resetting env. episode 910.000000, reward total was -18.000000. running mean: -20.272175\n",
            "resetting env. episode 911.000000, reward total was -20.000000. running mean: -20.269454\n",
            "resetting env. episode 912.000000, reward total was -19.000000. running mean: -20.256759\n",
            "resetting env. episode 913.000000, reward total was -20.000000. running mean: -20.254192\n",
            "resetting env. episode 914.000000, reward total was -20.000000. running mean: -20.251650\n",
            "resetting env. episode 915.000000, reward total was -21.000000. running mean: -20.259133\n",
            "resetting env. episode 916.000000, reward total was -18.000000. running mean: -20.236542\n",
            "resetting env. episode 917.000000, reward total was -21.000000. running mean: -20.244176\n",
            "resetting env. episode 918.000000, reward total was -21.000000. running mean: -20.251735\n",
            "resetting env. episode 919.000000, reward total was -21.000000. running mean: -20.259217\n",
            "resetting env. episode 920.000000, reward total was -20.000000. running mean: -20.256625\n",
            "resetting env. episode 921.000000, reward total was -20.000000. running mean: -20.254059\n",
            "resetting env. episode 922.000000, reward total was -20.000000. running mean: -20.251518\n",
            "resetting env. episode 923.000000, reward total was -21.000000. running mean: -20.259003\n",
            "resetting env. episode 924.000000, reward total was -20.000000. running mean: -20.256413\n",
            "resetting env. episode 925.000000, reward total was -20.000000. running mean: -20.253849\n",
            "resetting env. episode 926.000000, reward total was -19.000000. running mean: -20.241310\n",
            "resetting env. episode 927.000000, reward total was -21.000000. running mean: -20.248897\n",
            "resetting env. episode 928.000000, reward total was -21.000000. running mean: -20.256408\n",
            "resetting env. episode 929.000000, reward total was -21.000000. running mean: -20.263844\n",
            "resetting env. episode 930.000000, reward total was -21.000000. running mean: -20.271206\n",
            "resetting env. episode 931.000000, reward total was -21.000000. running mean: -20.278494\n",
            "resetting env. episode 932.000000, reward total was -21.000000. running mean: -20.285709\n",
            "resetting env. episode 933.000000, reward total was -20.000000. running mean: -20.282852\n",
            "resetting env. episode 934.000000, reward total was -19.000000. running mean: -20.270023\n",
            "resetting env. episode 935.000000, reward total was -19.000000. running mean: -20.257323\n",
            "resetting env. episode 936.000000, reward total was -19.000000. running mean: -20.244750\n",
            "resetting env. episode 937.000000, reward total was -21.000000. running mean: -20.252302\n",
            "resetting env. episode 938.000000, reward total was -20.000000. running mean: -20.249779\n",
            "resetting env. episode 939.000000, reward total was -21.000000. running mean: -20.257281\n",
            "resetting env. episode 940.000000, reward total was -21.000000. running mean: -20.264709\n",
            "resetting env. episode 941.000000, reward total was -21.000000. running mean: -20.272062\n",
            "resetting env. episode 942.000000, reward total was -20.000000. running mean: -20.269341\n",
            "resetting env. episode 943.000000, reward total was -20.000000. running mean: -20.266648\n",
            "resetting env. episode 944.000000, reward total was -21.000000. running mean: -20.273981\n",
            "resetting env. episode 945.000000, reward total was -21.000000. running mean: -20.281241\n",
            "resetting env. episode 946.000000, reward total was -18.000000. running mean: -20.258429\n",
            "resetting env. episode 947.000000, reward total was -20.000000. running mean: -20.255845\n",
            "resetting env. episode 948.000000, reward total was -21.000000. running mean: -20.263286\n",
            "resetting env. episode 949.000000, reward total was -20.000000. running mean: -20.260653\n",
            "resetting env. episode 950.000000, reward total was -19.000000. running mean: -20.248047\n",
            "resetting env. episode 951.000000, reward total was -21.000000. running mean: -20.255566\n",
            "resetting env. episode 952.000000, reward total was -20.000000. running mean: -20.253011\n",
            "resetting env. episode 953.000000, reward total was -21.000000. running mean: -20.260480\n",
            "resetting env. episode 954.000000, reward total was -21.000000. running mean: -20.267876\n",
            "resetting env. episode 955.000000, reward total was -20.000000. running mean: -20.265197\n",
            "resetting env. episode 956.000000, reward total was -21.000000. running mean: -20.272545\n",
            "resetting env. episode 957.000000, reward total was -21.000000. running mean: -20.279819\n",
            "resetting env. episode 958.000000, reward total was -19.000000. running mean: -20.267021\n",
            "resetting env. episode 959.000000, reward total was -21.000000. running mean: -20.274351\n",
            "resetting env. episode 960.000000, reward total was -21.000000. running mean: -20.281608\n",
            "resetting env. episode 961.000000, reward total was -20.000000. running mean: -20.278792\n",
            "resetting env. episode 962.000000, reward total was -20.000000. running mean: -20.276004\n",
            "resetting env. episode 963.000000, reward total was -21.000000. running mean: -20.283244\n",
            "resetting env. episode 964.000000, reward total was -21.000000. running mean: -20.290411\n",
            "resetting env. episode 965.000000, reward total was -21.000000. running mean: -20.297507\n",
            "resetting env. episode 966.000000, reward total was -21.000000. running mean: -20.304532\n",
            "resetting env. episode 967.000000, reward total was -18.000000. running mean: -20.281487\n",
            "resetting env. episode 968.000000, reward total was -21.000000. running mean: -20.288672\n",
            "resetting env. episode 969.000000, reward total was -21.000000. running mean: -20.295785\n",
            "resetting env. episode 970.000000, reward total was -21.000000. running mean: -20.302827\n",
            "resetting env. episode 971.000000, reward total was -21.000000. running mean: -20.309799\n",
            "resetting env. episode 972.000000, reward total was -20.000000. running mean: -20.306701\n",
            "resetting env. episode 973.000000, reward total was -20.000000. running mean: -20.303634\n",
            "resetting env. episode 974.000000, reward total was -18.000000. running mean: -20.280598\n",
            "resetting env. episode 975.000000, reward total was -21.000000. running mean: -20.287792\n",
            "resetting env. episode 976.000000, reward total was -18.000000. running mean: -20.264914\n",
            "resetting env. episode 977.000000, reward total was -20.000000. running mean: -20.262265\n",
            "resetting env. episode 978.000000, reward total was -21.000000. running mean: -20.269642\n",
            "resetting env. episode 979.000000, reward total was -21.000000. running mean: -20.276945\n",
            "resetting env. episode 980.000000, reward total was -20.000000. running mean: -20.274176\n",
            "resetting env. episode 981.000000, reward total was -21.000000. running mean: -20.281434\n",
            "resetting env. episode 982.000000, reward total was -21.000000. running mean: -20.288620\n",
            "resetting env. episode 983.000000, reward total was -21.000000. running mean: -20.295734\n",
            "resetting env. episode 984.000000, reward total was -21.000000. running mean: -20.302776\n",
            "resetting env. episode 985.000000, reward total was -21.000000. running mean: -20.309749\n",
            "resetting env. episode 986.000000, reward total was -21.000000. running mean: -20.316651\n",
            "resetting env. episode 987.000000, reward total was -21.000000. running mean: -20.323485\n",
            "resetting env. episode 988.000000, reward total was -21.000000. running mean: -20.330250\n",
            "resetting env. episode 989.000000, reward total was -21.000000. running mean: -20.336947\n",
            "resetting env. episode 990.000000, reward total was -21.000000. running mean: -20.343578\n",
            "resetting env. episode 991.000000, reward total was -21.000000. running mean: -20.350142\n",
            "resetting env. episode 992.000000, reward total was -21.000000. running mean: -20.356641\n",
            "resetting env. episode 993.000000, reward total was -20.000000. running mean: -20.353074\n",
            "resetting env. episode 994.000000, reward total was -20.000000. running mean: -20.349543\n",
            "resetting env. episode 995.000000, reward total was -17.000000. running mean: -20.316048\n",
            "resetting env. episode 996.000000, reward total was -21.000000. running mean: -20.322888\n",
            "resetting env. episode 997.000000, reward total was -19.000000. running mean: -20.309659\n",
            "resetting env. episode 998.000000, reward total was -21.000000. running mean: -20.316562\n",
            "resetting env. episode 999.000000, reward total was -19.000000. running mean: -20.303396\n",
            "resetting env. episode 1000.000000, reward total was -21.000000. running mean: -20.310362\n",
            "resetting env. episode 1001.000000, reward total was -21.000000. running mean: -20.317259\n",
            "resetting env. episode 1002.000000, reward total was -19.000000. running mean: -20.304086\n",
            "resetting env. episode 1003.000000, reward total was -18.000000. running mean: -20.281045\n",
            "resetting env. episode 1004.000000, reward total was -21.000000. running mean: -20.288235\n",
            "resetting env. episode 1005.000000, reward total was -20.000000. running mean: -20.285353\n",
            "resetting env. episode 1006.000000, reward total was -20.000000. running mean: -20.282499\n",
            "resetting env. episode 1007.000000, reward total was -20.000000. running mean: -20.279674\n",
            "resetting env. episode 1008.000000, reward total was -21.000000. running mean: -20.286877\n",
            "resetting env. episode 1009.000000, reward total was -18.000000. running mean: -20.264009\n",
            "resetting env. episode 1010.000000, reward total was -20.000000. running mean: -20.261368\n",
            "resetting env. episode 1011.000000, reward total was -21.000000. running mean: -20.268755\n",
            "resetting env. episode 1012.000000, reward total was -19.000000. running mean: -20.256067\n",
            "resetting env. episode 1013.000000, reward total was -16.000000. running mean: -20.213507\n",
            "resetting env. episode 1014.000000, reward total was -20.000000. running mean: -20.211372\n",
            "resetting env. episode 1015.000000, reward total was -21.000000. running mean: -20.219258\n",
            "resetting env. episode 1016.000000, reward total was -20.000000. running mean: -20.217065\n",
            "resetting env. episode 1017.000000, reward total was -21.000000. running mean: -20.224895\n",
            "resetting env. episode 1018.000000, reward total was -20.000000. running mean: -20.222646\n",
            "resetting env. episode 1019.000000, reward total was -21.000000. running mean: -20.230419\n",
            "resetting env. episode 1020.000000, reward total was -20.000000. running mean: -20.228115\n",
            "resetting env. episode 1021.000000, reward total was -20.000000. running mean: -20.225834\n",
            "resetting env. episode 1022.000000, reward total was -21.000000. running mean: -20.233575\n",
            "resetting env. episode 1023.000000, reward total was -20.000000. running mean: -20.231240\n",
            "resetting env. episode 1024.000000, reward total was -18.000000. running mean: -20.208927\n",
            "resetting env. episode 1025.000000, reward total was -21.000000. running mean: -20.216838\n",
            "resetting env. episode 1026.000000, reward total was -21.000000. running mean: -20.224670\n",
            "resetting env. episode 1027.000000, reward total was -20.000000. running mean: -20.222423\n",
            "resetting env. episode 1028.000000, reward total was -20.000000. running mean: -20.220199\n",
            "resetting env. episode 1029.000000, reward total was -21.000000. running mean: -20.227997\n",
            "resetting env. episode 1030.000000, reward total was -21.000000. running mean: -20.235717\n",
            "resetting env. episode 1031.000000, reward total was -21.000000. running mean: -20.243360\n",
            "resetting env. episode 1032.000000, reward total was -20.000000. running mean: -20.240926\n",
            "resetting env. episode 1033.000000, reward total was -20.000000. running mean: -20.238517\n",
            "resetting env. episode 1034.000000, reward total was -21.000000. running mean: -20.246132\n",
            "resetting env. episode 1035.000000, reward total was -20.000000. running mean: -20.243670\n",
            "resetting env. episode 1036.000000, reward total was -19.000000. running mean: -20.231234\n",
            "resetting env. episode 1037.000000, reward total was -21.000000. running mean: -20.238921\n",
            "resetting env. episode 1038.000000, reward total was -21.000000. running mean: -20.246532\n",
            "resetting env. episode 1039.000000, reward total was -21.000000. running mean: -20.254067\n",
            "resetting env. episode 1040.000000, reward total was -21.000000. running mean: -20.261526\n",
            "resetting env. episode 1041.000000, reward total was -19.000000. running mean: -20.248911\n",
            "resetting env. episode 1042.000000, reward total was -18.000000. running mean: -20.226422\n",
            "resetting env. episode 1043.000000, reward total was -18.000000. running mean: -20.204157\n",
            "resetting env. episode 1044.000000, reward total was -19.000000. running mean: -20.192116\n",
            "resetting env. episode 1045.000000, reward total was -20.000000. running mean: -20.190195\n",
            "resetting env. episode 1046.000000, reward total was -21.000000. running mean: -20.198293\n",
            "resetting env. episode 1047.000000, reward total was -21.000000. running mean: -20.206310\n",
            "resetting env. episode 1048.000000, reward total was -21.000000. running mean: -20.214247\n",
            "resetting env. episode 1049.000000, reward total was -21.000000. running mean: -20.222104\n",
            "resetting env. episode 1050.000000, reward total was -21.000000. running mean: -20.229883\n",
            "resetting env. episode 1051.000000, reward total was -21.000000. running mean: -20.237584\n",
            "resetting env. episode 1052.000000, reward total was -20.000000. running mean: -20.235209\n",
            "resetting env. episode 1053.000000, reward total was -20.000000. running mean: -20.232856\n",
            "resetting env. episode 1054.000000, reward total was -21.000000. running mean: -20.240528\n",
            "resetting env. episode 1055.000000, reward total was -20.000000. running mean: -20.238123\n",
            "resetting env. episode 1056.000000, reward total was -21.000000. running mean: -20.245741\n",
            "resetting env. episode 1057.000000, reward total was -19.000000. running mean: -20.233284\n",
            "resetting env. episode 1058.000000, reward total was -21.000000. running mean: -20.240951\n",
            "resetting env. episode 1059.000000, reward total was -19.000000. running mean: -20.228542\n",
            "resetting env. episode 1060.000000, reward total was -21.000000. running mean: -20.236256\n",
            "resetting env. episode 1061.000000, reward total was -21.000000. running mean: -20.243894\n",
            "resetting env. episode 1062.000000, reward total was -21.000000. running mean: -20.251455\n",
            "resetting env. episode 1063.000000, reward total was -19.000000. running mean: -20.238940\n",
            "resetting env. episode 1064.000000, reward total was -20.000000. running mean: -20.236551\n",
            "resetting env. episode 1065.000000, reward total was -19.000000. running mean: -20.224185\n",
            "resetting env. episode 1066.000000, reward total was -21.000000. running mean: -20.231943\n",
            "resetting env. episode 1067.000000, reward total was -20.000000. running mean: -20.229624\n",
            "resetting env. episode 1068.000000, reward total was -20.000000. running mean: -20.227328\n",
            "resetting env. episode 1069.000000, reward total was -21.000000. running mean: -20.235054\n",
            "resetting env. episode 1070.000000, reward total was -21.000000. running mean: -20.242704\n",
            "resetting env. episode 1071.000000, reward total was -21.000000. running mean: -20.250277\n",
            "resetting env. episode 1072.000000, reward total was -20.000000. running mean: -20.247774\n",
            "resetting env. episode 1073.000000, reward total was -21.000000. running mean: -20.255296\n",
            "resetting env. episode 1074.000000, reward total was -21.000000. running mean: -20.262743\n",
            "resetting env. episode 1075.000000, reward total was -19.000000. running mean: -20.250116\n",
            "resetting env. episode 1076.000000, reward total was -20.000000. running mean: -20.247615\n",
            "resetting env. episode 1077.000000, reward total was -21.000000. running mean: -20.255139\n",
            "resetting env. episode 1078.000000, reward total was -17.000000. running mean: -20.222587\n",
            "resetting env. episode 1079.000000, reward total was -21.000000. running mean: -20.230361\n",
            "resetting env. episode 1080.000000, reward total was -21.000000. running mean: -20.238058\n",
            "resetting env. episode 1081.000000, reward total was -20.000000. running mean: -20.235677\n",
            "resetting env. episode 1082.000000, reward total was -21.000000. running mean: -20.243320\n",
            "resetting env. episode 1083.000000, reward total was -21.000000. running mean: -20.250887\n",
            "resetting env. episode 1084.000000, reward total was -19.000000. running mean: -20.238378\n",
            "resetting env. episode 1085.000000, reward total was -21.000000. running mean: -20.245995\n",
            "resetting env. episode 1086.000000, reward total was -21.000000. running mean: -20.253535\n",
            "resetting env. episode 1087.000000, reward total was -20.000000. running mean: -20.250999\n",
            "resetting env. episode 1088.000000, reward total was -21.000000. running mean: -20.258489\n",
            "resetting env. episode 1089.000000, reward total was -20.000000. running mean: -20.255904\n",
            "resetting env. episode 1090.000000, reward total was -20.000000. running mean: -20.253345\n",
            "resetting env. episode 1091.000000, reward total was -21.000000. running mean: -20.260812\n",
            "resetting env. episode 1092.000000, reward total was -18.000000. running mean: -20.238204\n",
            "resetting env. episode 1093.000000, reward total was -21.000000. running mean: -20.245822\n",
            "resetting env. episode 1094.000000, reward total was -21.000000. running mean: -20.253364\n",
            "resetting env. episode 1095.000000, reward total was -20.000000. running mean: -20.250830\n",
            "resetting env. episode 1096.000000, reward total was -20.000000. running mean: -20.248322\n",
            "resetting env. episode 1097.000000, reward total was -19.000000. running mean: -20.235838\n",
            "resetting env. episode 1098.000000, reward total was -21.000000. running mean: -20.243480\n",
            "resetting env. episode 1099.000000, reward total was -21.000000. running mean: -20.251045\n",
            "resetting env. episode 1100.000000, reward total was -20.000000. running mean: -20.248535\n",
            "resetting env. episode 1101.000000, reward total was -21.000000. running mean: -20.256049\n",
            "resetting env. episode 1102.000000, reward total was -21.000000. running mean: -20.263489\n",
            "resetting env. episode 1103.000000, reward total was -20.000000. running mean: -20.260854\n",
            "resetting env. episode 1104.000000, reward total was -21.000000. running mean: -20.268245\n",
            "resetting env. episode 1105.000000, reward total was -21.000000. running mean: -20.275563\n",
            "resetting env. episode 1106.000000, reward total was -21.000000. running mean: -20.282807\n",
            "resetting env. episode 1107.000000, reward total was -21.000000. running mean: -20.289979\n",
            "resetting env. episode 1108.000000, reward total was -18.000000. running mean: -20.267080\n",
            "resetting env. episode 1109.000000, reward total was -21.000000. running mean: -20.274409\n",
            "resetting env. episode 1110.000000, reward total was -21.000000. running mean: -20.281665\n",
            "resetting env. episode 1111.000000, reward total was -20.000000. running mean: -20.278848\n",
            "resetting env. episode 1112.000000, reward total was -20.000000. running mean: -20.276060\n",
            "resetting env. episode 1113.000000, reward total was -20.000000. running mean: -20.273299\n",
            "resetting env. episode 1114.000000, reward total was -20.000000. running mean: -20.270566\n",
            "resetting env. episode 1115.000000, reward total was -21.000000. running mean: -20.277860\n",
            "resetting env. episode 1116.000000, reward total was -20.000000. running mean: -20.275082\n",
            "resetting env. episode 1117.000000, reward total was -20.000000. running mean: -20.272331\n",
            "resetting env. episode 1118.000000, reward total was -19.000000. running mean: -20.259608\n",
            "resetting env. episode 1119.000000, reward total was -21.000000. running mean: -20.267011\n",
            "resetting env. episode 1120.000000, reward total was -18.000000. running mean: -20.244341\n",
            "resetting env. episode 1121.000000, reward total was -21.000000. running mean: -20.251898\n",
            "resetting env. episode 1122.000000, reward total was -21.000000. running mean: -20.259379\n",
            "resetting env. episode 1123.000000, reward total was -21.000000. running mean: -20.266785\n",
            "resetting env. episode 1124.000000, reward total was -20.000000. running mean: -20.264117\n",
            "resetting env. episode 1125.000000, reward total was -21.000000. running mean: -20.271476\n",
            "resetting env. episode 1126.000000, reward total was -21.000000. running mean: -20.278761\n",
            "resetting env. episode 1127.000000, reward total was -21.000000. running mean: -20.285974\n",
            "resetting env. episode 1128.000000, reward total was -19.000000. running mean: -20.273114\n",
            "resetting env. episode 1129.000000, reward total was -20.000000. running mean: -20.270383\n",
            "resetting env. episode 1130.000000, reward total was -20.000000. running mean: -20.267679\n",
            "resetting env. episode 1131.000000, reward total was -19.000000. running mean: -20.255002\n",
            "resetting env. episode 1132.000000, reward total was -21.000000. running mean: -20.262452\n",
            "resetting env. episode 1133.000000, reward total was -19.000000. running mean: -20.249828\n",
            "resetting env. episode 1134.000000, reward total was -21.000000. running mean: -20.257329\n",
            "resetting env. episode 1135.000000, reward total was -19.000000. running mean: -20.244756\n",
            "resetting env. episode 1136.000000, reward total was -21.000000. running mean: -20.252309\n",
            "resetting env. episode 1137.000000, reward total was -21.000000. running mean: -20.259786\n",
            "resetting env. episode 1138.000000, reward total was -21.000000. running mean: -20.267188\n",
            "resetting env. episode 1139.000000, reward total was -19.000000. running mean: -20.254516\n",
            "resetting env. episode 1140.000000, reward total was -21.000000. running mean: -20.261971\n",
            "resetting env. episode 1141.000000, reward total was -21.000000. running mean: -20.269351\n",
            "resetting env. episode 1142.000000, reward total was -20.000000. running mean: -20.266657\n",
            "resetting env. episode 1143.000000, reward total was -18.000000. running mean: -20.243991\n",
            "resetting env. episode 1144.000000, reward total was -20.000000. running mean: -20.241551\n",
            "resetting env. episode 1145.000000, reward total was -20.000000. running mean: -20.239135\n",
            "resetting env. episode 1146.000000, reward total was -21.000000. running mean: -20.246744\n",
            "resetting env. episode 1147.000000, reward total was -21.000000. running mean: -20.254277\n",
            "resetting env. episode 1148.000000, reward total was -20.000000. running mean: -20.251734\n",
            "resetting env. episode 1149.000000, reward total was -21.000000. running mean: -20.259217\n",
            "resetting env. episode 1150.000000, reward total was -20.000000. running mean: -20.256624\n",
            "resetting env. episode 1151.000000, reward total was -20.000000. running mean: -20.254058\n",
            "resetting env. episode 1152.000000, reward total was -21.000000. running mean: -20.261518\n",
            "resetting env. episode 1153.000000, reward total was -21.000000. running mean: -20.268902\n",
            "resetting env. episode 1154.000000, reward total was -20.000000. running mean: -20.266213\n",
            "resetting env. episode 1155.000000, reward total was -21.000000. running mean: -20.273551\n",
            "resetting env. episode 1156.000000, reward total was -20.000000. running mean: -20.270816\n",
            "resetting env. episode 1157.000000, reward total was -21.000000. running mean: -20.278108\n",
            "resetting env. episode 1158.000000, reward total was -21.000000. running mean: -20.285326\n",
            "resetting env. episode 1159.000000, reward total was -21.000000. running mean: -20.292473\n",
            "resetting env. episode 1160.000000, reward total was -21.000000. running mean: -20.299548\n",
            "resetting env. episode 1161.000000, reward total was -21.000000. running mean: -20.306553\n",
            "resetting env. episode 1162.000000, reward total was -21.000000. running mean: -20.313487\n",
            "resetting env. episode 1163.000000, reward total was -21.000000. running mean: -20.320353\n",
            "resetting env. episode 1164.000000, reward total was -21.000000. running mean: -20.327149\n",
            "resetting env. episode 1165.000000, reward total was -18.000000. running mean: -20.303878\n",
            "resetting env. episode 1166.000000, reward total was -21.000000. running mean: -20.310839\n",
            "resetting env. episode 1167.000000, reward total was -21.000000. running mean: -20.317730\n",
            "resetting env. episode 1168.000000, reward total was -21.000000. running mean: -20.324553\n",
            "resetting env. episode 1169.000000, reward total was -20.000000. running mean: -20.321308\n",
            "resetting env. episode 1170.000000, reward total was -21.000000. running mean: -20.328094\n",
            "resetting env. episode 1171.000000, reward total was -21.000000. running mean: -20.334814\n",
            "resetting env. episode 1172.000000, reward total was -18.000000. running mean: -20.311465\n",
            "resetting env. episode 1173.000000, reward total was -19.000000. running mean: -20.298351\n",
            "resetting env. episode 1174.000000, reward total was -20.000000. running mean: -20.295367\n",
            "resetting env. episode 1175.000000, reward total was -19.000000. running mean: -20.282414\n",
            "resetting env. episode 1176.000000, reward total was -20.000000. running mean: -20.279589\n",
            "resetting env. episode 1177.000000, reward total was -19.000000. running mean: -20.266794\n",
            "resetting env. episode 1178.000000, reward total was -21.000000. running mean: -20.274126\n",
            "resetting env. episode 1179.000000, reward total was -20.000000. running mean: -20.271384\n",
            "resetting env. episode 1180.000000, reward total was -21.000000. running mean: -20.278670\n",
            "resetting env. episode 1181.000000, reward total was -20.000000. running mean: -20.275884\n",
            "resetting env. episode 1182.000000, reward total was -20.000000. running mean: -20.273125\n",
            "resetting env. episode 1183.000000, reward total was -20.000000. running mean: -20.270394\n",
            "resetting env. episode 1184.000000, reward total was -19.000000. running mean: -20.257690\n",
            "resetting env. episode 1185.000000, reward total was -21.000000. running mean: -20.265113\n",
            "resetting env. episode 1186.000000, reward total was -21.000000. running mean: -20.272462\n",
            "resetting env. episode 1187.000000, reward total was -21.000000. running mean: -20.279737\n",
            "resetting env. episode 1188.000000, reward total was -21.000000. running mean: -20.286940\n",
            "resetting env. episode 1189.000000, reward total was -21.000000. running mean: -20.294070\n",
            "resetting env. episode 1190.000000, reward total was -21.000000. running mean: -20.301130\n",
            "resetting env. episode 1191.000000, reward total was -20.000000. running mean: -20.298118\n",
            "resetting env. episode 1192.000000, reward total was -20.000000. running mean: -20.295137\n",
            "resetting env. episode 1193.000000, reward total was -21.000000. running mean: -20.302186\n",
            "resetting env. episode 1194.000000, reward total was -21.000000. running mean: -20.309164\n",
            "resetting env. episode 1195.000000, reward total was -21.000000. running mean: -20.316072\n",
            "resetting env. episode 1196.000000, reward total was -21.000000. running mean: -20.322912\n",
            "resetting env. episode 1197.000000, reward total was -17.000000. running mean: -20.289682\n",
            "resetting env. episode 1198.000000, reward total was -21.000000. running mean: -20.296786\n",
            "resetting env. episode 1199.000000, reward total was -20.000000. running mean: -20.293818\n",
            "resetting env. episode 1200.000000, reward total was -20.000000. running mean: -20.290880\n",
            "resetting env. episode 1201.000000, reward total was -19.000000. running mean: -20.277971\n",
            "resetting env. episode 1202.000000, reward total was -21.000000. running mean: -20.285191\n",
            "resetting env. episode 1203.000000, reward total was -20.000000. running mean: -20.282339\n",
            "resetting env. episode 1204.000000, reward total was -19.000000. running mean: -20.269516\n",
            "resetting env. episode 1205.000000, reward total was -17.000000. running mean: -20.236821\n",
            "resetting env. episode 1206.000000, reward total was -20.000000. running mean: -20.234452\n",
            "resetting env. episode 1207.000000, reward total was -19.000000. running mean: -20.222108\n",
            "resetting env. episode 1208.000000, reward total was -20.000000. running mean: -20.219887\n",
            "resetting env. episode 1209.000000, reward total was -20.000000. running mean: -20.217688\n",
            "resetting env. episode 1210.000000, reward total was -20.000000. running mean: -20.215511\n",
            "resetting env. episode 1211.000000, reward total was -17.000000. running mean: -20.183356\n",
            "resetting env. episode 1212.000000, reward total was -21.000000. running mean: -20.191522\n",
            "resetting env. episode 1213.000000, reward total was -21.000000. running mean: -20.199607\n",
            "resetting env. episode 1214.000000, reward total was -20.000000. running mean: -20.197611\n",
            "resetting env. episode 1215.000000, reward total was -19.000000. running mean: -20.185635\n",
            "resetting env. episode 1216.000000, reward total was -20.000000. running mean: -20.183779\n",
            "resetting env. episode 1217.000000, reward total was -20.000000. running mean: -20.181941\n",
            "resetting env. episode 1218.000000, reward total was -18.000000. running mean: -20.160121\n",
            "resetting env. episode 1219.000000, reward total was -21.000000. running mean: -20.168520\n",
            "resetting env. episode 1220.000000, reward total was -20.000000. running mean: -20.166835\n",
            "resetting env. episode 1221.000000, reward total was -20.000000. running mean: -20.165167\n",
            "resetting env. episode 1222.000000, reward total was -20.000000. running mean: -20.163515\n",
            "resetting env. episode 1223.000000, reward total was -19.000000. running mean: -20.151880\n",
            "resetting env. episode 1224.000000, reward total was -21.000000. running mean: -20.160361\n",
            "resetting env. episode 1225.000000, reward total was -20.000000. running mean: -20.158757\n",
            "resetting env. episode 1226.000000, reward total was -21.000000. running mean: -20.167170\n",
            "resetting env. episode 1227.000000, reward total was -21.000000. running mean: -20.175498\n",
            "resetting env. episode 1228.000000, reward total was -19.000000. running mean: -20.163743\n",
            "resetting env. episode 1229.000000, reward total was -17.000000. running mean: -20.132106\n",
            "resetting env. episode 1230.000000, reward total was -21.000000. running mean: -20.140785\n",
            "resetting env. episode 1231.000000, reward total was -20.000000. running mean: -20.139377\n",
            "resetting env. episode 1232.000000, reward total was -21.000000. running mean: -20.147983\n",
            "resetting env. episode 1233.000000, reward total was -21.000000. running mean: -20.156503\n",
            "resetting env. episode 1234.000000, reward total was -19.000000. running mean: -20.144938\n",
            "resetting env. episode 1235.000000, reward total was -20.000000. running mean: -20.143489\n",
            "resetting env. episode 1236.000000, reward total was -21.000000. running mean: -20.152054\n",
            "resetting env. episode 1237.000000, reward total was -20.000000. running mean: -20.150533\n",
            "resetting env. episode 1238.000000, reward total was -21.000000. running mean: -20.159028\n",
            "resetting env. episode 1239.000000, reward total was -21.000000. running mean: -20.167438\n",
            "resetting env. episode 1240.000000, reward total was -19.000000. running mean: -20.155763\n",
            "resetting env. episode 1241.000000, reward total was -18.000000. running mean: -20.134206\n",
            "resetting env. episode 1242.000000, reward total was -21.000000. running mean: -20.142864\n",
            "resetting env. episode 1243.000000, reward total was -19.000000. running mean: -20.131435\n",
            "resetting env. episode 1244.000000, reward total was -21.000000. running mean: -20.140121\n",
            "resetting env. episode 1245.000000, reward total was -20.000000. running mean: -20.138720\n",
            "resetting env. episode 1246.000000, reward total was -21.000000. running mean: -20.147332\n",
            "resetting env. episode 1247.000000, reward total was -20.000000. running mean: -20.145859\n",
            "resetting env. episode 1248.000000, reward total was -20.000000. running mean: -20.144400\n",
            "resetting env. episode 1249.000000, reward total was -21.000000. running mean: -20.152956\n",
            "resetting env. episode 1250.000000, reward total was -21.000000. running mean: -20.161427\n",
            "resetting env. episode 1251.000000, reward total was -21.000000. running mean: -20.169813\n",
            "resetting env. episode 1252.000000, reward total was -21.000000. running mean: -20.178114\n",
            "resetting env. episode 1253.000000, reward total was -19.000000. running mean: -20.166333\n",
            "resetting env. episode 1254.000000, reward total was -18.000000. running mean: -20.144670\n",
            "resetting env. episode 1255.000000, reward total was -20.000000. running mean: -20.143223\n",
            "resetting env. episode 1256.000000, reward total was -21.000000. running mean: -20.151791\n",
            "resetting env. episode 1257.000000, reward total was -20.000000. running mean: -20.150273\n",
            "resetting env. episode 1258.000000, reward total was -21.000000. running mean: -20.158770\n",
            "resetting env. episode 1259.000000, reward total was -21.000000. running mean: -20.167183\n",
            "resetting env. episode 1260.000000, reward total was -20.000000. running mean: -20.165511\n",
            "resetting env. episode 1261.000000, reward total was -21.000000. running mean: -20.173856\n",
            "resetting env. episode 1262.000000, reward total was -21.000000. running mean: -20.182117\n",
            "resetting env. episode 1263.000000, reward total was -19.000000. running mean: -20.170296\n",
            "resetting env. episode 1264.000000, reward total was -18.000000. running mean: -20.148593\n",
            "resetting env. episode 1265.000000, reward total was -20.000000. running mean: -20.147107\n",
            "resetting env. episode 1266.000000, reward total was -21.000000. running mean: -20.155636\n",
            "resetting env. episode 1267.000000, reward total was -21.000000. running mean: -20.164080\n",
            "resetting env. episode 1268.000000, reward total was -20.000000. running mean: -20.162439\n",
            "resetting env. episode 1269.000000, reward total was -20.000000. running mean: -20.160815\n",
            "resetting env. episode 1270.000000, reward total was -21.000000. running mean: -20.169206\n",
            "resetting env. episode 1271.000000, reward total was -19.000000. running mean: -20.157514\n",
            "resetting env. episode 1272.000000, reward total was -19.000000. running mean: -20.145939\n",
            "resetting env. episode 1273.000000, reward total was -20.000000. running mean: -20.144480\n",
            "resetting env. episode 1274.000000, reward total was -19.000000. running mean: -20.133035\n",
            "resetting env. episode 1275.000000, reward total was -20.000000. running mean: -20.131705\n",
            "resetting env. episode 1276.000000, reward total was -21.000000. running mean: -20.140388\n",
            "resetting env. episode 1277.000000, reward total was -21.000000. running mean: -20.148984\n",
            "resetting env. episode 1278.000000, reward total was -21.000000. running mean: -20.157494\n",
            "resetting env. episode 1279.000000, reward total was -20.000000. running mean: -20.155919\n",
            "resetting env. episode 1280.000000, reward total was -20.000000. running mean: -20.154360\n",
            "resetting env. episode 1281.000000, reward total was -21.000000. running mean: -20.162816\n",
            "resetting env. episode 1282.000000, reward total was -21.000000. running mean: -20.171188\n",
            "resetting env. episode 1283.000000, reward total was -21.000000. running mean: -20.179476\n",
            "resetting env. episode 1284.000000, reward total was -21.000000. running mean: -20.187681\n",
            "resetting env. episode 1285.000000, reward total was -20.000000. running mean: -20.185805\n",
            "resetting env. episode 1286.000000, reward total was -21.000000. running mean: -20.193947\n",
            "resetting env. episode 1287.000000, reward total was -21.000000. running mean: -20.202007\n",
            "resetting env. episode 1288.000000, reward total was -20.000000. running mean: -20.199987\n",
            "resetting env. episode 1289.000000, reward total was -20.000000. running mean: -20.197987\n",
            "resetting env. episode 1290.000000, reward total was -20.000000. running mean: -20.196007\n",
            "resetting env. episode 1291.000000, reward total was -21.000000. running mean: -20.204047\n",
            "resetting env. episode 1292.000000, reward total was -20.000000. running mean: -20.202007\n",
            "resetting env. episode 1293.000000, reward total was -20.000000. running mean: -20.199987\n",
            "resetting env. episode 1294.000000, reward total was -21.000000. running mean: -20.207987\n",
            "resetting env. episode 1295.000000, reward total was -20.000000. running mean: -20.205907\n",
            "resetting env. episode 1296.000000, reward total was -21.000000. running mean: -20.213848\n",
            "resetting env. episode 1297.000000, reward total was -21.000000. running mean: -20.221709\n",
            "resetting env. episode 1298.000000, reward total was -21.000000. running mean: -20.229492\n",
            "resetting env. episode 1299.000000, reward total was -21.000000. running mean: -20.237197\n",
            "resetting env. episode 1300.000000, reward total was -21.000000. running mean: -20.244825\n",
            "resetting env. episode 1301.000000, reward total was -21.000000. running mean: -20.252377\n",
            "resetting env. episode 1302.000000, reward total was -20.000000. running mean: -20.249853\n",
            "resetting env. episode 1303.000000, reward total was -19.000000. running mean: -20.237355\n",
            "resetting env. episode 1304.000000, reward total was -19.000000. running mean: -20.224981\n",
            "resetting env. episode 1305.000000, reward total was -21.000000. running mean: -20.232731\n",
            "resetting env. episode 1306.000000, reward total was -20.000000. running mean: -20.230404\n",
            "resetting env. episode 1307.000000, reward total was -19.000000. running mean: -20.218100\n",
            "resetting env. episode 1308.000000, reward total was -21.000000. running mean: -20.225919\n",
            "resetting env. episode 1309.000000, reward total was -20.000000. running mean: -20.223660\n",
            "resetting env. episode 1310.000000, reward total was -21.000000. running mean: -20.231423\n",
            "resetting env. episode 1311.000000, reward total was -20.000000. running mean: -20.229109\n",
            "resetting env. episode 1312.000000, reward total was -19.000000. running mean: -20.216818\n",
            "resetting env. episode 1313.000000, reward total was -21.000000. running mean: -20.224650\n",
            "resetting env. episode 1314.000000, reward total was -19.000000. running mean: -20.212403\n",
            "resetting env. episode 1315.000000, reward total was -21.000000. running mean: -20.220279\n",
            "resetting env. episode 1316.000000, reward total was -21.000000. running mean: -20.228076\n",
            "resetting env. episode 1317.000000, reward total was -19.000000. running mean: -20.215796\n",
            "resetting env. episode 1318.000000, reward total was -19.000000. running mean: -20.203638\n",
            "resetting env. episode 1319.000000, reward total was -21.000000. running mean: -20.211601\n",
            "resetting env. episode 1320.000000, reward total was -20.000000. running mean: -20.209485\n",
            "resetting env. episode 1321.000000, reward total was -21.000000. running mean: -20.217390\n",
            "resetting env. episode 1322.000000, reward total was -20.000000. running mean: -20.215217\n",
            "resetting env. episode 1323.000000, reward total was -20.000000. running mean: -20.213064\n",
            "resetting env. episode 1324.000000, reward total was -21.000000. running mean: -20.220934\n",
            "resetting env. episode 1325.000000, reward total was -20.000000. running mean: -20.218724\n",
            "resetting env. episode 1326.000000, reward total was -20.000000. running mean: -20.216537\n",
            "resetting env. episode 1327.000000, reward total was -20.000000. running mean: -20.214372\n",
            "resetting env. episode 1328.000000, reward total was -21.000000. running mean: -20.222228\n",
            "resetting env. episode 1329.000000, reward total was -21.000000. running mean: -20.230006\n",
            "resetting env. episode 1330.000000, reward total was -20.000000. running mean: -20.227706\n",
            "resetting env. episode 1331.000000, reward total was -20.000000. running mean: -20.225429\n",
            "resetting env. episode 1332.000000, reward total was -21.000000. running mean: -20.233174\n",
            "resetting env. episode 1333.000000, reward total was -20.000000. running mean: -20.230843\n",
            "resetting env. episode 1334.000000, reward total was -21.000000. running mean: -20.238534\n",
            "resetting env. episode 1335.000000, reward total was -21.000000. running mean: -20.246149\n",
            "resetting env. episode 1336.000000, reward total was -21.000000. running mean: -20.253687\n",
            "resetting env. episode 1337.000000, reward total was -20.000000. running mean: -20.251151\n",
            "resetting env. episode 1338.000000, reward total was -19.000000. running mean: -20.238639\n",
            "resetting env. episode 1339.000000, reward total was -19.000000. running mean: -20.226253\n",
            "resetting env. episode 1340.000000, reward total was -21.000000. running mean: -20.233990\n",
            "resetting env. episode 1341.000000, reward total was -18.000000. running mean: -20.211650\n",
            "resetting env. episode 1342.000000, reward total was -20.000000. running mean: -20.209534\n",
            "resetting env. episode 1343.000000, reward total was -21.000000. running mean: -20.217438\n",
            "resetting env. episode 1344.000000, reward total was -18.000000. running mean: -20.195264\n",
            "resetting env. episode 1345.000000, reward total was -21.000000. running mean: -20.203311\n",
            "resetting env. episode 1346.000000, reward total was -20.000000. running mean: -20.201278\n",
            "resetting env. episode 1347.000000, reward total was -20.000000. running mean: -20.199265\n",
            "resetting env. episode 1348.000000, reward total was -20.000000. running mean: -20.197273\n",
            "resetting env. episode 1349.000000, reward total was -18.000000. running mean: -20.175300\n",
            "resetting env. episode 1350.000000, reward total was -19.000000. running mean: -20.163547\n",
            "resetting env. episode 1351.000000, reward total was -20.000000. running mean: -20.161912\n",
            "resetting env. episode 1352.000000, reward total was -19.000000. running mean: -20.150292\n",
            "resetting env. episode 1353.000000, reward total was -21.000000. running mean: -20.158790\n",
            "resetting env. episode 1354.000000, reward total was -21.000000. running mean: -20.167202\n",
            "resetting env. episode 1355.000000, reward total was -19.000000. running mean: -20.155530\n",
            "resetting env. episode 1356.000000, reward total was -18.000000. running mean: -20.133974\n",
            "resetting env. episode 1357.000000, reward total was -21.000000. running mean: -20.142635\n",
            "resetting env. episode 1358.000000, reward total was -21.000000. running mean: -20.151208\n",
            "resetting env. episode 1359.000000, reward total was -17.000000. running mean: -20.119696\n",
            "resetting env. episode 1360.000000, reward total was -21.000000. running mean: -20.128499\n",
            "resetting env. episode 1361.000000, reward total was -21.000000. running mean: -20.137214\n",
            "resetting env. episode 1362.000000, reward total was -21.000000. running mean: -20.145842\n",
            "resetting env. episode 1363.000000, reward total was -18.000000. running mean: -20.124384\n",
            "resetting env. episode 1364.000000, reward total was -20.000000. running mean: -20.123140\n",
            "resetting env. episode 1365.000000, reward total was -20.000000. running mean: -20.121908\n",
            "resetting env. episode 1366.000000, reward total was -18.000000. running mean: -20.100689\n",
            "resetting env. episode 1367.000000, reward total was -20.000000. running mean: -20.099682\n",
            "resetting env. episode 1368.000000, reward total was -18.000000. running mean: -20.078686\n",
            "resetting env. episode 1369.000000, reward total was -20.000000. running mean: -20.077899\n",
            "resetting env. episode 1370.000000, reward total was -19.000000. running mean: -20.067120\n",
            "resetting env. episode 1371.000000, reward total was -21.000000. running mean: -20.076449\n",
            "resetting env. episode 1372.000000, reward total was -20.000000. running mean: -20.075684\n",
            "resetting env. episode 1373.000000, reward total was -20.000000. running mean: -20.074927\n",
            "resetting env. episode 1374.000000, reward total was -21.000000. running mean: -20.084178\n",
            "resetting env. episode 1375.000000, reward total was -20.000000. running mean: -20.083336\n",
            "resetting env. episode 1376.000000, reward total was -21.000000. running mean: -20.092503\n",
            "resetting env. episode 1377.000000, reward total was -20.000000. running mean: -20.091578\n",
            "resetting env. episode 1378.000000, reward total was -21.000000. running mean: -20.100662\n",
            "resetting env. episode 1379.000000, reward total was -19.000000. running mean: -20.089655\n",
            "resetting env. episode 1380.000000, reward total was -19.000000. running mean: -20.078759\n",
            "resetting env. episode 1381.000000, reward total was -20.000000. running mean: -20.077971\n",
            "resetting env. episode 1382.000000, reward total was -21.000000. running mean: -20.087192\n",
            "resetting env. episode 1383.000000, reward total was -20.000000. running mean: -20.086320\n",
            "resetting env. episode 1384.000000, reward total was -20.000000. running mean: -20.085456\n",
            "resetting env. episode 1385.000000, reward total was -21.000000. running mean: -20.094602\n",
            "resetting env. episode 1386.000000, reward total was -21.000000. running mean: -20.103656\n",
            "resetting env. episode 1387.000000, reward total was -21.000000. running mean: -20.112619\n",
            "resetting env. episode 1388.000000, reward total was -20.000000. running mean: -20.111493\n",
            "resetting env. episode 1389.000000, reward total was -20.000000. running mean: -20.110378\n",
            "resetting env. episode 1390.000000, reward total was -21.000000. running mean: -20.119274\n",
            "resetting env. episode 1391.000000, reward total was -21.000000. running mean: -20.128082\n",
            "resetting env. episode 1392.000000, reward total was -20.000000. running mean: -20.126801\n",
            "resetting env. episode 1393.000000, reward total was -21.000000. running mean: -20.135533\n",
            "resetting env. episode 1394.000000, reward total was -20.000000. running mean: -20.134178\n",
            "resetting env. episode 1395.000000, reward total was -20.000000. running mean: -20.132836\n",
            "resetting env. episode 1396.000000, reward total was -19.000000. running mean: -20.121507\n",
            "resetting env. episode 1397.000000, reward total was -20.000000. running mean: -20.120292\n",
            "resetting env. episode 1398.000000, reward total was -21.000000. running mean: -20.129089\n",
            "resetting env. episode 1399.000000, reward total was -21.000000. running mean: -20.137798\n",
            "resetting env. episode 1400.000000, reward total was -17.000000. running mean: -20.106420\n",
            "resetting env. episode 1401.000000, reward total was -21.000000. running mean: -20.115356\n",
            "resetting env. episode 1402.000000, reward total was -19.000000. running mean: -20.104203\n",
            "resetting env. episode 1403.000000, reward total was -21.000000. running mean: -20.113161\n",
            "resetting env. episode 1404.000000, reward total was -21.000000. running mean: -20.122029\n",
            "resetting env. episode 1405.000000, reward total was -20.000000. running mean: -20.120809\n",
            "resetting env. episode 1406.000000, reward total was -19.000000. running mean: -20.109601\n",
            "resetting env. episode 1407.000000, reward total was -19.000000. running mean: -20.098505\n",
            "resetting env. episode 1408.000000, reward total was -20.000000. running mean: -20.097520\n",
            "resetting env. episode 1409.000000, reward total was -21.000000. running mean: -20.106544\n",
            "resetting env. episode 1410.000000, reward total was -21.000000. running mean: -20.115479\n",
            "resetting env. episode 1411.000000, reward total was -19.000000. running mean: -20.104324\n",
            "resetting env. episode 1412.000000, reward total was -21.000000. running mean: -20.113281\n",
            "resetting env. episode 1413.000000, reward total was -21.000000. running mean: -20.122148\n",
            "resetting env. episode 1414.000000, reward total was -21.000000. running mean: -20.130927\n",
            "resetting env. episode 1415.000000, reward total was -19.000000. running mean: -20.119617\n",
            "resetting env. episode 1416.000000, reward total was -21.000000. running mean: -20.128421\n",
            "resetting env. episode 1417.000000, reward total was -21.000000. running mean: -20.137137\n",
            "resetting env. episode 1418.000000, reward total was -21.000000. running mean: -20.145766\n",
            "resetting env. episode 1419.000000, reward total was -19.000000. running mean: -20.134308\n",
            "resetting env. episode 1420.000000, reward total was -20.000000. running mean: -20.132965\n",
            "resetting env. episode 1421.000000, reward total was -21.000000. running mean: -20.141635\n",
            "resetting env. episode 1422.000000, reward total was -20.000000. running mean: -20.140219\n",
            "resetting env. episode 1423.000000, reward total was -21.000000. running mean: -20.148817\n",
            "resetting env. episode 1424.000000, reward total was -21.000000. running mean: -20.157329\n",
            "resetting env. episode 1425.000000, reward total was -20.000000. running mean: -20.155755\n",
            "resetting env. episode 1426.000000, reward total was -21.000000. running mean: -20.164198\n",
            "resetting env. episode 1427.000000, reward total was -20.000000. running mean: -20.162556\n",
            "resetting env. episode 1428.000000, reward total was -21.000000. running mean: -20.170930\n",
            "resetting env. episode 1429.000000, reward total was -20.000000. running mean: -20.169221\n",
            "resetting env. episode 1430.000000, reward total was -19.000000. running mean: -20.157529\n",
            "resetting env. episode 1431.000000, reward total was -17.000000. running mean: -20.125953\n",
            "resetting env. episode 1432.000000, reward total was -19.000000. running mean: -20.114694\n",
            "resetting env. episode 1433.000000, reward total was -21.000000. running mean: -20.123547\n",
            "resetting env. episode 1434.000000, reward total was -21.000000. running mean: -20.132311\n",
            "resetting env. episode 1435.000000, reward total was -20.000000. running mean: -20.130988\n",
            "resetting env. episode 1436.000000, reward total was -19.000000. running mean: -20.119678\n",
            "resetting env. episode 1437.000000, reward total was -21.000000. running mean: -20.128482\n",
            "resetting env. episode 1438.000000, reward total was -21.000000. running mean: -20.137197\n",
            "resetting env. episode 1439.000000, reward total was -21.000000. running mean: -20.145825\n",
            "resetting env. episode 1440.000000, reward total was -19.000000. running mean: -20.134367\n",
            "resetting env. episode 1441.000000, reward total was -21.000000. running mean: -20.143023\n",
            "resetting env. episode 1442.000000, reward total was -21.000000. running mean: -20.151593\n",
            "resetting env. episode 1443.000000, reward total was -21.000000. running mean: -20.160077\n",
            "resetting env. episode 1444.000000, reward total was -19.000000. running mean: -20.148476\n",
            "resetting env. episode 1445.000000, reward total was -20.000000. running mean: -20.146991\n",
            "resetting env. episode 1446.000000, reward total was -21.000000. running mean: -20.155521\n",
            "resetting env. episode 1447.000000, reward total was -20.000000. running mean: -20.153966\n",
            "resetting env. episode 1448.000000, reward total was -21.000000. running mean: -20.162427\n",
            "resetting env. episode 1449.000000, reward total was -20.000000. running mean: -20.160802\n",
            "resetting env. episode 1450.000000, reward total was -19.000000. running mean: -20.149194\n",
            "resetting env. episode 1451.000000, reward total was -17.000000. running mean: -20.117702\n",
            "resetting env. episode 1452.000000, reward total was -18.000000. running mean: -20.096525\n",
            "resetting env. episode 1453.000000, reward total was -19.000000. running mean: -20.085560\n",
            "resetting env. episode 1454.000000, reward total was -20.000000. running mean: -20.084704\n",
            "resetting env. episode 1455.000000, reward total was -21.000000. running mean: -20.093857\n",
            "resetting env. episode 1456.000000, reward total was -21.000000. running mean: -20.102919\n",
            "resetting env. episode 1457.000000, reward total was -21.000000. running mean: -20.111890\n",
            "resetting env. episode 1458.000000, reward total was -20.000000. running mean: -20.110771\n",
            "resetting env. episode 1459.000000, reward total was -20.000000. running mean: -20.109663\n",
            "resetting env. episode 1460.000000, reward total was -20.000000. running mean: -20.108566\n",
            "resetting env. episode 1461.000000, reward total was -20.000000. running mean: -20.107481\n",
            "resetting env. episode 1462.000000, reward total was -21.000000. running mean: -20.116406\n",
            "resetting env. episode 1463.000000, reward total was -20.000000. running mean: -20.115242\n",
            "resetting env. episode 1464.000000, reward total was -21.000000. running mean: -20.124089\n",
            "resetting env. episode 1465.000000, reward total was -18.000000. running mean: -20.102849\n",
            "resetting env. episode 1466.000000, reward total was -21.000000. running mean: -20.111820\n",
            "resetting env. episode 1467.000000, reward total was -21.000000. running mean: -20.120702\n",
            "resetting env. episode 1468.000000, reward total was -21.000000. running mean: -20.129495\n",
            "resetting env. episode 1469.000000, reward total was -19.000000. running mean: -20.118200\n",
            "resetting env. episode 1470.000000, reward total was -18.000000. running mean: -20.097018\n",
            "resetting env. episode 1471.000000, reward total was -19.000000. running mean: -20.086048\n",
            "resetting env. episode 1472.000000, reward total was -21.000000. running mean: -20.095187\n",
            "resetting env. episode 1473.000000, reward total was -20.000000. running mean: -20.094235\n",
            "resetting env. episode 1474.000000, reward total was -20.000000. running mean: -20.093293\n",
            "resetting env. episode 1475.000000, reward total was -19.000000. running mean: -20.082360\n",
            "resetting env. episode 1476.000000, reward total was -18.000000. running mean: -20.061536\n",
            "resetting env. episode 1477.000000, reward total was -20.000000. running mean: -20.060921\n",
            "resetting env. episode 1478.000000, reward total was -18.000000. running mean: -20.040312\n",
            "resetting env. episode 1479.000000, reward total was -20.000000. running mean: -20.039909\n",
            "resetting env. episode 1480.000000, reward total was -21.000000. running mean: -20.049510\n",
            "resetting env. episode 1481.000000, reward total was -21.000000. running mean: -20.059015\n",
            "resetting env. episode 1482.000000, reward total was -20.000000. running mean: -20.058424\n",
            "resetting env. episode 1483.000000, reward total was -21.000000. running mean: -20.067840\n",
            "resetting env. episode 1484.000000, reward total was -19.000000. running mean: -20.057162\n",
            "resetting env. episode 1485.000000, reward total was -21.000000. running mean: -20.066590\n",
            "resetting env. episode 1486.000000, reward total was -21.000000. running mean: -20.075924\n",
            "resetting env. episode 1487.000000, reward total was -21.000000. running mean: -20.085165\n",
            "resetting env. episode 1488.000000, reward total was -21.000000. running mean: -20.094313\n",
            "resetting env. episode 1489.000000, reward total was -19.000000. running mean: -20.083370\n",
            "resetting env. episode 1490.000000, reward total was -19.000000. running mean: -20.072537\n",
            "resetting env. episode 1491.000000, reward total was -21.000000. running mean: -20.081811\n",
            "resetting env. episode 1492.000000, reward total was -19.000000. running mean: -20.070993\n",
            "resetting env. episode 1493.000000, reward total was -21.000000. running mean: -20.080283\n",
            "resetting env. episode 1494.000000, reward total was -21.000000. running mean: -20.089480\n",
            "resetting env. episode 1495.000000, reward total was -20.000000. running mean: -20.088585\n",
            "resetting env. episode 1496.000000, reward total was -20.000000. running mean: -20.087700\n",
            "resetting env. episode 1497.000000, reward total was -20.000000. running mean: -20.086823\n",
            "resetting env. episode 1498.000000, reward total was -21.000000. running mean: -20.095954\n",
            "resetting env. episode 1499.000000, reward total was -21.000000. running mean: -20.104995\n",
            "resetting env. episode 1500.000000, reward total was -21.000000. running mean: -20.113945\n",
            "CPU times: user 1h 52min 18s, sys: 35min 43s, total: 2h 28min 2s\n",
            "Wall time: 1h 16min 26s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "w2NblmwDsL3y",
        "outputId": "f9dc105f-f739-410e-96d9-d41199829fd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "cell_type": "code",
      "source": [
        "play_game(env, model)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -8.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}
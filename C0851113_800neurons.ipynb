{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "C0851113_800neurons.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "cWACPRL869I4"
      },
      "cell_type": "code",
      "source": [
        "!pip install gym >/dev/null"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Os6feRY6ec_"
      },
      "cell_type": "code",
      "source": [
        "!pip install JSAnimation >/dev/null"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wotUOa_e6edP"
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    HTML(anim.to_jshtml())"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R66_INeZ9nYX"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 2: Playing Pong"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ngMhg3fB9aA",
        "outputId": "bfdf0815-3dc7-4d4e-fb91-ed118b19f904"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=ec0694ca0bcf1cdf2c1932f0716a08cbfa67b497f81146e9a54a6a9965d53c4b\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "MtT2GyK_6edc",
        "outputId": "cfcf3402-42f0-456c-86f8-fd8604d96ee5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "env = gym.make('Pong-v0')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "oRE6WmXQJ1Z0",
        "outputId": "184626b3-271a-4406-cd2a-5f480253b8de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "env.action_space"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "yl_9d4HFJ31W",
        "outputId": "38f61cc6-dac5-4b5e-b419-8a9e3f5c7741",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "env.observation_space"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "trwRXI-h6eeI",
        "outputId": "c69ecaad-4cda-4f82-fabc-81b6e55dc62a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# Run a demo of the environment\n",
        "observation = env.reset()\n",
        "cumulated_reward = 0\n",
        "\n",
        "frames = []\n",
        "for t in range(1000):\n",
        "#     print(observation)\n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    # very stupid agent, just makes a random action within the allowd action space\n",
        "    action = env.action_space.sample()\n",
        "#     print(\"Action: {}\".format(t+1))    \n",
        "    observation, reward, done, info = env.step(action)\n",
        "#     print(reward)\n",
        "    cumulated_reward += reward\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "        break\n",
        "print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "\n",
        "env.close()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -15.0\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "3zZTecVWLLes"
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(x): \n",
        "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
        "\n",
        "def prepro(I):\n",
        "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "  I = I[35:195] # crop\n",
        "  I = I[::2,::2,0] # downsample by factor of 2\n",
        "  I[I == 144] = 0 # erase background (background type 1)\n",
        "  I[I == 109] = 0 # erase background (background type 2)\n",
        "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  return I.astype(np.float).ravel()\n",
        "\n",
        "def policy_forward(x):\n",
        "  h = np.dot(model['W1'], x)\n",
        "  h[h<0] = 0 # ReLU nonlinearity\n",
        "  logp = np.dot(model['W2'], h)\n",
        "  p = sigmoid(logp)\n",
        "  return p, h # return probability of taking action 2, and hidden state\n",
        "\n",
        "def model_step(model, observation, prev_x):\n",
        "  # preprocess the observation, set input to network to be difference image\n",
        "  cur_x = prepro(observation)\n",
        "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "  prev_x = cur_x\n",
        "  \n",
        "  # forward the policy network and sample an action from the returned probability\n",
        "  aprob, _ = policy_forward(x)\n",
        "  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n",
        "  \n",
        "  return action, prev_x\n",
        "\n",
        "def play_game(env, model):\n",
        "  observation = env.reset()\n",
        "\n",
        "  frames = []\n",
        "  cumulated_reward = 0\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "\n",
        "  for t in range(1000):\n",
        "      frames.append(env.render(mode = 'rgb_array'))\n",
        "      action, prev_x = model_step(model, observation, prev_x)\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      cumulated_reward += reward\n",
        "      if done:\n",
        "          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "          break\n",
        "  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "  display_frames_as_gif(frames)\n",
        "  env.close()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6gWvZQ7AQLQt"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 3: Policy Gradient from Scratch"
      ]
    },
    {
      "metadata": {
        "id": "eqFm7hqcItWl"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# model initialization\n",
        "H = 200 # number of hidden layer neurons\n",
        "D = 80 * 80 # input dimensionality: 80x80 grid\n",
        "model = {}\n",
        "model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
        "model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
        "\n",
        "# import pickle\n",
        "# model = pickle.load(open('model.pkl', 'rb'))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TwjiwKisQM19"
      },
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 10 # every how many episodes to do a param update?\n",
        "# learning_rate = 1e-4\n",
        "learning_rate = 1e-4\n",
        " \n",
        "gamma = 0.99 # discount factor for reward\n",
        "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
        "  \n",
        "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
        "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
        "\n",
        "def discount_rewards(r):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "    running_add = running_add * gamma + r[t]\n",
        "    discounted_r[t] = running_add\n",
        "  return discounted_r\n",
        "\n",
        "def policy_backward(epx, eph, epdlogp):\n",
        "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
        "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
        "  dh = np.outer(epdlogp, model['W2'])\n",
        "  dh[eph <= 0] = 0 # backpro prelu\n",
        "  dW1 = np.dot(dh.T, epx)\n",
        "  return {'W1':dW1, 'W2':dW2}\n",
        "\n",
        "def train_model(env, model, total_episodes = 100):\n",
        "  hist = []\n",
        "  observation = env.reset()\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "  xs,hs,dlogps,drs = [],[],[],[]\n",
        "  running_reward = None\n",
        "  reward_sum = 0\n",
        "  episode_number = 0\n",
        "\n",
        "  while True:\n",
        "    # preprocess the observation, set input to network to be difference image\n",
        "    cur_x = prepro(observation)\n",
        "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "    prev_x = cur_x\n",
        "\n",
        "    # forward the policy network and sample an action from the returned probability\n",
        "    aprob, h = policy_forward(x)\n",
        "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
        "\n",
        "    # record various intermediates (needed later for backprop)\n",
        "    xs.append(x) # observation\n",
        "    hs.append(h) # hidden state\n",
        "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
        "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
        "\n",
        "    # step the environment and get new measurements\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    reward_sum += reward\n",
        "\n",
        "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
        "\n",
        "    if done: # an episode finished\n",
        "      episode_number += 1\n",
        "\n",
        "      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
        "      epx = np.vstack(xs)\n",
        "      eph = np.vstack(hs)\n",
        "      epdlogp = np.vstack(dlogps)\n",
        "      epr = np.vstack(drs)\n",
        "      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
        "\n",
        "      # compute the discounted reward backwards through time\n",
        "      discounted_epr = discount_rewards(epr)\n",
        "      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "      discounted_epr -= np.mean(discounted_epr)\n",
        "      discounted_epr /= np.std(discounted_epr)\n",
        "\n",
        "      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
        "      grad = policy_backward(epx, eph, epdlogp)\n",
        "      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
        "\n",
        "      # perform rmsprop parameter update every batch_size episodes\n",
        "      if episode_number % batch_size == 0:\n",
        "        for k,v in model.items():\n",
        "          g = grad_buffer[k] # gradient\n",
        "          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
        "          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
        "\n",
        "      # boring book-keeping\n",
        "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "      hist.append((episode_number, reward_sum, running_reward))\n",
        "      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
        "      reward_sum = 0\n",
        "      observation = env.reset() # reset env\n",
        "      prev_x = None\n",
        "      if episode_number == total_episodes: return hist\n",
        "\n",
        "      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
        "        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G6Ka_5Vl9Orm",
        "outputId": "ee669c99-7553-4deb-f961-3a5e5d607c70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist1 = train_model(env, model, total_episodes=500)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -19.000000. running mean: -19.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -19.020000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -19.039800\n",
            "resetting env. episode 4.000000, reward total was -19.000000. running mean: -19.039402\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -19.059008\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -19.078418\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -19.097634\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -19.116657\n",
            "resetting env. episode 9.000000, reward total was -20.000000. running mean: -19.125491\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -19.144236\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -19.162794\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -19.181166\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -19.199354\n",
            "resetting env. episode 14.000000, reward total was -19.000000. running mean: -19.197360\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -19.215387\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -19.233233\n",
            "resetting env. episode 17.000000, reward total was -18.000000. running mean: -19.220901\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -19.238692\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -19.246305\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -19.263842\n",
            "resetting env. episode 21.000000, reward total was -19.000000. running mean: -19.261203\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -19.278591\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -19.295805\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -19.312847\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -19.329719\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -19.346422\n",
            "resetting env. episode 27.000000, reward total was -19.000000. running mean: -19.342957\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -19.359528\n",
            "resetting env. episode 29.000000, reward total was -19.000000. running mean: -19.355933\n",
            "resetting env. episode 30.000000, reward total was -20.000000. running mean: -19.362373\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -19.368749\n",
            "resetting env. episode 32.000000, reward total was -19.000000. running mean: -19.365062\n",
            "resetting env. episode 33.000000, reward total was -19.000000. running mean: -19.361411\n",
            "resetting env. episode 34.000000, reward total was -20.000000. running mean: -19.367797\n",
            "resetting env. episode 35.000000, reward total was -19.000000. running mean: -19.364119\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -19.380478\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -19.396673\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -19.412707\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -19.428579\n",
            "resetting env. episode 40.000000, reward total was -20.000000. running mean: -19.434294\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -19.449951\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -19.465451\n",
            "resetting env. episode 43.000000, reward total was -19.000000. running mean: -19.460797\n",
            "resetting env. episode 44.000000, reward total was -18.000000. running mean: -19.446189\n",
            "resetting env. episode 45.000000, reward total was -18.000000. running mean: -19.431727\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -19.447410\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -19.462936\n",
            "resetting env. episode 48.000000, reward total was -19.000000. running mean: -19.458306\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -19.473723\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -19.488986\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -19.504096\n",
            "resetting env. episode 52.000000, reward total was -19.000000. running mean: -19.499055\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -19.514064\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -19.528924\n",
            "resetting env. episode 55.000000, reward total was -18.000000. running mean: -19.513635\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -19.528498\n",
            "resetting env. episode 57.000000, reward total was -20.000000. running mean: -19.533213\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -19.547881\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -19.562402\n",
            "resetting env. episode 60.000000, reward total was -19.000000. running mean: -19.556778\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -19.571211\n",
            "resetting env. episode 62.000000, reward total was -20.000000. running mean: -19.575498\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -19.589743\n",
            "resetting env. episode 64.000000, reward total was -19.000000. running mean: -19.583846\n",
            "resetting env. episode 65.000000, reward total was -20.000000. running mean: -19.588008\n",
            "resetting env. episode 66.000000, reward total was -19.000000. running mean: -19.582127\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -19.596306\n",
            "resetting env. episode 68.000000, reward total was -20.000000. running mean: -19.600343\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -19.614340\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -19.628196\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -19.631914\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -19.645595\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -19.659139\n",
            "resetting env. episode 74.000000, reward total was -20.000000. running mean: -19.662548\n",
            "resetting env. episode 75.000000, reward total was -20.000000. running mean: -19.665922\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -19.679263\n",
            "resetting env. episode 77.000000, reward total was -19.000000. running mean: -19.672471\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -19.685746\n",
            "resetting env. episode 79.000000, reward total was -20.000000. running mean: -19.688888\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -19.701999\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -19.714979\n",
            "resetting env. episode 82.000000, reward total was -20.000000. running mean: -19.717830\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -19.730651\n",
            "resetting env. episode 84.000000, reward total was -18.000000. running mean: -19.713345\n",
            "resetting env. episode 85.000000, reward total was -18.000000. running mean: -19.696211\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -19.699249\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -19.712257\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -19.725134\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -19.737883\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -19.750504\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -19.762999\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -19.775369\n",
            "resetting env. episode 93.000000, reward total was -19.000000. running mean: -19.767615\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -19.779939\n",
            "resetting env. episode 95.000000, reward total was -19.000000. running mean: -19.772140\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -19.784418\n",
            "resetting env. episode 97.000000, reward total was -20.000000. running mean: -19.786574\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -19.798708\n",
            "resetting env. episode 99.000000, reward total was -20.000000. running mean: -19.800721\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -19.812714\n",
            "resetting env. episode 101.000000, reward total was -19.000000. running mean: -19.804587\n",
            "resetting env. episode 102.000000, reward total was -18.000000. running mean: -19.786541\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -19.798676\n",
            "resetting env. episode 104.000000, reward total was -19.000000. running mean: -19.790689\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -19.802782\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -19.814754\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -19.826607\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -19.838341\n",
            "resetting env. episode 109.000000, reward total was -20.000000. running mean: -19.839957\n",
            "resetting env. episode 110.000000, reward total was -20.000000. running mean: -19.841558\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -19.853142\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -19.864611\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -19.875965\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -19.887205\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -19.898333\n",
            "resetting env. episode 116.000000, reward total was -19.000000. running mean: -19.889350\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -19.900456\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -19.911452\n",
            "resetting env. episode 119.000000, reward total was -18.000000. running mean: -19.892337\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -19.903414\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -19.914379\n",
            "resetting env. episode 122.000000, reward total was -19.000000. running mean: -19.905236\n",
            "resetting env. episode 123.000000, reward total was -19.000000. running mean: -19.896183\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -19.907222\n",
            "resetting env. episode 125.000000, reward total was -20.000000. running mean: -19.908149\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -19.919068\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -19.929877\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -19.940578\n",
            "resetting env. episode 129.000000, reward total was -19.000000. running mean: -19.931173\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -19.941861\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -19.952442\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -19.962918\n",
            "resetting env. episode 133.000000, reward total was -20.000000. running mean: -19.963289\n",
            "resetting env. episode 134.000000, reward total was -19.000000. running mean: -19.953656\n",
            "resetting env. episode 135.000000, reward total was -20.000000. running mean: -19.954119\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -19.954578\n",
            "resetting env. episode 137.000000, reward total was -19.000000. running mean: -19.945032\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -19.955582\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -19.966026\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -19.976366\n",
            "resetting env. episode 141.000000, reward total was -19.000000. running mean: -19.966602\n",
            "resetting env. episode 142.000000, reward total was -20.000000. running mean: -19.966936\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -19.977267\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -19.987494\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -19.997619\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.007643\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -20.007567\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.017491\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.027316\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.037043\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.046672\n",
            "resetting env. episode 152.000000, reward total was -20.000000. running mean: -20.046206\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.055744\n",
            "resetting env. episode 154.000000, reward total was -20.000000. running mean: -20.055186\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.064634\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.073988\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.083248\n",
            "resetting env. episode 158.000000, reward total was -17.000000. running mean: -20.052416\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.061891\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.071273\n",
            "resetting env. episode 161.000000, reward total was -19.000000. running mean: -20.060560\n",
            "resetting env. episode 162.000000, reward total was -18.000000. running mean: -20.039954\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.049555\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.059059\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.068469\n",
            "resetting env. episode 166.000000, reward total was -20.000000. running mean: -20.067784\n",
            "resetting env. episode 167.000000, reward total was -19.000000. running mean: -20.057106\n",
            "resetting env. episode 168.000000, reward total was -20.000000. running mean: -20.056535\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.065970\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.075310\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.084557\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.093711\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.102774\n",
            "resetting env. episode 174.000000, reward total was -20.000000. running mean: -20.101746\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.110729\n",
            "resetting env. episode 176.000000, reward total was -20.000000. running mean: -20.109622\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.118525\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.127340\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.136067\n",
            "resetting env. episode 180.000000, reward total was -20.000000. running mean: -20.134706\n",
            "resetting env. episode 181.000000, reward total was -18.000000. running mean: -20.113359\n",
            "resetting env. episode 182.000000, reward total was -18.000000. running mean: -20.092225\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.101303\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.110290\n",
            "resetting env. episode 185.000000, reward total was -19.000000. running mean: -20.099187\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.108195\n",
            "resetting env. episode 187.000000, reward total was -19.000000. running mean: -20.097113\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.106142\n",
            "resetting env. episode 189.000000, reward total was -20.000000. running mean: -20.105081\n",
            "resetting env. episode 190.000000, reward total was -19.000000. running mean: -20.094030\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.103090\n",
            "resetting env. episode 192.000000, reward total was -20.000000. running mean: -20.102059\n",
            "resetting env. episode 193.000000, reward total was -20.000000. running mean: -20.101038\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.110028\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.118928\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.127738\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.136461\n",
            "resetting env. episode 198.000000, reward total was -20.000000. running mean: -20.135096\n",
            "resetting env. episode 199.000000, reward total was -20.000000. running mean: -20.133745\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.142408\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.150984\n",
            "resetting env. episode 202.000000, reward total was -19.000000. running mean: -20.139474\n",
            "resetting env. episode 203.000000, reward total was -19.000000. running mean: -20.128079\n",
            "resetting env. episode 204.000000, reward total was -20.000000. running mean: -20.126798\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.135530\n",
            "resetting env. episode 206.000000, reward total was -20.000000. running mean: -20.134175\n",
            "resetting env. episode 207.000000, reward total was -20.000000. running mean: -20.132833\n",
            "resetting env. episode 208.000000, reward total was -19.000000. running mean: -20.121505\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.130290\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.138987\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.147597\n",
            "resetting env. episode 212.000000, reward total was -20.000000. running mean: -20.146121\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.154660\n",
            "resetting env. episode 214.000000, reward total was -20.000000. running mean: -20.153113\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.161582\n",
            "resetting env. episode 216.000000, reward total was -20.000000. running mean: -20.159967\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.168367\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.176683\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.184916\n",
            "resetting env. episode 220.000000, reward total was -20.000000. running mean: -20.183067\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.191237\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.199324\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.207331\n",
            "resetting env. episode 224.000000, reward total was -20.000000. running mean: -20.205258\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.213205\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.221073\n",
            "resetting env. episode 227.000000, reward total was -20.000000. running mean: -20.218862\n",
            "resetting env. episode 228.000000, reward total was -20.000000. running mean: -20.216674\n",
            "resetting env. episode 229.000000, reward total was -19.000000. running mean: -20.204507\n",
            "resetting env. episode 230.000000, reward total was -20.000000. running mean: -20.202462\n",
            "resetting env. episode 231.000000, reward total was -20.000000. running mean: -20.200437\n",
            "resetting env. episode 232.000000, reward total was -19.000000. running mean: -20.188433\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.196549\n",
            "resetting env. episode 234.000000, reward total was -19.000000. running mean: -20.184583\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.192737\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.200810\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.208802\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.216714\n",
            "resetting env. episode 239.000000, reward total was -20.000000. running mean: -20.214547\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.222401\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.230177\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.237875\n",
            "resetting env. episode 243.000000, reward total was -19.000000. running mean: -20.225497\n",
            "resetting env. episode 244.000000, reward total was -20.000000. running mean: -20.223242\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.231009\n",
            "resetting env. episode 246.000000, reward total was -20.000000. running mean: -20.228699\n",
            "resetting env. episode 247.000000, reward total was -18.000000. running mean: -20.206412\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.214348\n",
            "resetting env. episode 249.000000, reward total was -20.000000. running mean: -20.212205\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.220082\n",
            "resetting env. episode 251.000000, reward total was -20.000000. running mean: -20.217882\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.225703\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.233446\n",
            "resetting env. episode 254.000000, reward total was -20.000000. running mean: -20.231111\n",
            "resetting env. episode 255.000000, reward total was -20.000000. running mean: -20.228800\n",
            "resetting env. episode 256.000000, reward total was -20.000000. running mean: -20.226512\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.234247\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -20.231905\n",
            "resetting env. episode 259.000000, reward total was -19.000000. running mean: -20.219586\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.227390\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.235116\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.242765\n",
            "resetting env. episode 263.000000, reward total was -19.000000. running mean: -20.230337\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.238034\n",
            "resetting env. episode 265.000000, reward total was -20.000000. running mean: -20.235653\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.243297\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.250864\n",
            "resetting env. episode 268.000000, reward total was -18.000000. running mean: -20.228355\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.236072\n",
            "resetting env. episode 270.000000, reward total was -20.000000. running mean: -20.233711\n",
            "resetting env. episode 271.000000, reward total was -19.000000. running mean: -20.221374\n",
            "resetting env. episode 272.000000, reward total was -19.000000. running mean: -20.209160\n",
            "resetting env. episode 273.000000, reward total was -20.000000. running mean: -20.207068\n",
            "resetting env. episode 274.000000, reward total was -19.000000. running mean: -20.194998\n",
            "resetting env. episode 275.000000, reward total was -20.000000. running mean: -20.193048\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.201117\n",
            "resetting env. episode 277.000000, reward total was -18.000000. running mean: -20.179106\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.187315\n",
            "resetting env. episode 279.000000, reward total was -20.000000. running mean: -20.185442\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.193588\n",
            "resetting env. episode 281.000000, reward total was -20.000000. running mean: -20.191652\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.199735\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.207738\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.215660\n",
            "resetting env. episode 285.000000, reward total was -20.000000. running mean: -20.213504\n",
            "resetting env. episode 286.000000, reward total was -20.000000. running mean: -20.211369\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.219255\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.227063\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.234792\n",
            "resetting env. episode 290.000000, reward total was -20.000000. running mean: -20.232444\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.240120\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.247718\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.255241\n",
            "resetting env. episode 294.000000, reward total was -19.000000. running mean: -20.242689\n",
            "resetting env. episode 295.000000, reward total was -19.000000. running mean: -20.230262\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.237959\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.245580\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.243124\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.250693\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.258186\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.265604\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.272948\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.280218\n",
            "resetting env. episode 304.000000, reward total was -19.000000. running mean: -20.267416\n",
            "resetting env. episode 305.000000, reward total was -20.000000. running mean: -20.264742\n",
            "resetting env. episode 306.000000, reward total was -19.000000. running mean: -20.252095\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.259574\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.266978\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.274308\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.281565\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.288749\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.295862\n",
            "resetting env. episode 313.000000, reward total was -20.000000. running mean: -20.292903\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.299974\n",
            "resetting env. episode 315.000000, reward total was -19.000000. running mean: -20.286974\n",
            "resetting env. episode 316.000000, reward total was -20.000000. running mean: -20.284105\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.291264\n",
            "resetting env. episode 318.000000, reward total was -20.000000. running mean: -20.288351\n",
            "resetting env. episode 319.000000, reward total was -18.000000. running mean: -20.265468\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.272813\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.280085\n",
            "resetting env. episode 322.000000, reward total was -20.000000. running mean: -20.277284\n",
            "resetting env. episode 323.000000, reward total was -20.000000. running mean: -20.274511\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.281766\n",
            "resetting env. episode 325.000000, reward total was -20.000000. running mean: -20.278948\n",
            "resetting env. episode 326.000000, reward total was -19.000000. running mean: -20.266159\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.273497\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.280762\n",
            "resetting env. episode 329.000000, reward total was -20.000000. running mean: -20.277955\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.285175\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.292323\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.299400\n",
            "resetting env. episode 333.000000, reward total was -20.000000. running mean: -20.296406\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.303442\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.310408\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.317304\n",
            "resetting env. episode 337.000000, reward total was -18.000000. running mean: -20.294130\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.301189\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.308177\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.315096\n",
            "resetting env. episode 341.000000, reward total was -20.000000. running mean: -20.311945\n",
            "resetting env. episode 342.000000, reward total was -19.000000. running mean: -20.298825\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.305837\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.312778\n",
            "resetting env. episode 345.000000, reward total was -20.000000. running mean: -20.309651\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.316554\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.323389\n",
            "resetting env. episode 348.000000, reward total was -20.000000. running mean: -20.320155\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.326953\n",
            "resetting env. episode 350.000000, reward total was -19.000000. running mean: -20.313684\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.320547\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.327341\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.334068\n",
            "resetting env. episode 354.000000, reward total was -20.000000. running mean: -20.330727\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.337420\n",
            "resetting env. episode 356.000000, reward total was -20.000000. running mean: -20.334046\n",
            "resetting env. episode 357.000000, reward total was -19.000000. running mean: -20.320705\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.327498\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.334223\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.340881\n",
            "resetting env. episode 361.000000, reward total was -20.000000. running mean: -20.337472\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -20.334098\n",
            "resetting env. episode 363.000000, reward total was -20.000000. running mean: -20.330757\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.337449\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.344075\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.350634\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.357127\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.363556\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.369921\n",
            "resetting env. episode 370.000000, reward total was -20.000000. running mean: -20.366221\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.372559\n",
            "resetting env. episode 372.000000, reward total was -20.000000. running mean: -20.368834\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.375145\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.381394\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -20.377580\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.383804\n",
            "resetting env. episode 377.000000, reward total was -20.000000. running mean: -20.379966\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.386166\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.392305\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.398382\n",
            "resetting env. episode 381.000000, reward total was -20.000000. running mean: -20.394398\n",
            "resetting env. episode 382.000000, reward total was -19.000000. running mean: -20.380454\n",
            "resetting env. episode 383.000000, reward total was -20.000000. running mean: -20.376649\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.382883\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.389054\n",
            "resetting env. episode 386.000000, reward total was -20.000000. running mean: -20.385163\n",
            "resetting env. episode 387.000000, reward total was -19.000000. running mean: -20.371312\n",
            "resetting env. episode 388.000000, reward total was -20.000000. running mean: -20.367599\n",
            "resetting env. episode 389.000000, reward total was -19.000000. running mean: -20.353923\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.360383\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.366780\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -20.363112\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.369481\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.375786\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.382028\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.388208\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.394326\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.400382\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.406379\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.412315\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.418192\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.424010\n",
            "resetting env. episode 403.000000, reward total was -19.000000. running mean: -20.409770\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.415672\n",
            "resetting env. episode 405.000000, reward total was -20.000000. running mean: -20.411515\n",
            "resetting env. episode 406.000000, reward total was -20.000000. running mean: -20.407400\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.413326\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.419193\n",
            "resetting env. episode 409.000000, reward total was -20.000000. running mean: -20.415001\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.420851\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.426642\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -20.422376\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.428152\n",
            "resetting env. episode 414.000000, reward total was -20.000000. running mean: -20.423871\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.429632\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.435336\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.440982\n",
            "resetting env. episode 418.000000, reward total was -18.000000. running mean: -20.416572\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.422407\n",
            "resetting env. episode 420.000000, reward total was -19.000000. running mean: -20.408183\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.414101\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.419960\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.425760\n",
            "resetting env. episode 424.000000, reward total was -20.000000. running mean: -20.421503\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.427288\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.433015\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.438685\n",
            "resetting env. episode 428.000000, reward total was -20.000000. running mean: -20.434298\n",
            "resetting env. episode 429.000000, reward total was -20.000000. running mean: -20.429955\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.435655\n",
            "resetting env. episode 431.000000, reward total was -19.000000. running mean: -20.421299\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.427086\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.432815\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.438487\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.444102\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.449661\n",
            "resetting env. episode 437.000000, reward total was -20.000000. running mean: -20.445164\n",
            "resetting env. episode 438.000000, reward total was -19.000000. running mean: -20.430713\n",
            "resetting env. episode 439.000000, reward total was -18.000000. running mean: -20.406405\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.412341\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.418218\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.424036\n",
            "resetting env. episode 443.000000, reward total was -20.000000. running mean: -20.419795\n",
            "resetting env. episode 444.000000, reward total was -20.000000. running mean: -20.415597\n",
            "resetting env. episode 445.000000, reward total was -20.000000. running mean: -20.411442\n",
            "resetting env. episode 446.000000, reward total was -20.000000. running mean: -20.407327\n",
            "resetting env. episode 447.000000, reward total was -20.000000. running mean: -20.403254\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.409221\n",
            "resetting env. episode 449.000000, reward total was -20.000000. running mean: -20.405129\n",
            "resetting env. episode 450.000000, reward total was -20.000000. running mean: -20.401078\n",
            "resetting env. episode 451.000000, reward total was -19.000000. running mean: -20.387067\n",
            "resetting env. episode 452.000000, reward total was -19.000000. running mean: -20.373196\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.379464\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.385670\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.391813\n",
            "resetting env. episode 456.000000, reward total was -20.000000. running mean: -20.387895\n",
            "resetting env. episode 457.000000, reward total was -20.000000. running mean: -20.384016\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.390176\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -20.386274\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.392411\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.398487\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.404502\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.410457\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.416353\n",
            "resetting env. episode 465.000000, reward total was -20.000000. running mean: -20.412189\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.418067\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.423887\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.429648\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.435351\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.440998\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.446588\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.452122\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -20.447601\n",
            "resetting env. episode 474.000000, reward total was -20.000000. running mean: -20.443125\n",
            "resetting env. episode 475.000000, reward total was -19.000000. running mean: -20.428693\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.434406\n",
            "resetting env. episode 477.000000, reward total was -19.000000. running mean: -20.420062\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.415862\n",
            "resetting env. episode 479.000000, reward total was -20.000000. running mean: -20.411703\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.417586\n",
            "resetting env. episode 481.000000, reward total was -19.000000. running mean: -20.403410\n",
            "resetting env. episode 482.000000, reward total was -19.000000. running mean: -20.389376\n",
            "resetting env. episode 483.000000, reward total was -18.000000. running mean: -20.365482\n",
            "resetting env. episode 484.000000, reward total was -20.000000. running mean: -20.361828\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.368209\n",
            "resetting env. episode 486.000000, reward total was -20.000000. running mean: -20.364527\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.370882\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.377173\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.383401\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.389567\n",
            "resetting env. episode 491.000000, reward total was -18.000000. running mean: -20.365672\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.372015\n",
            "resetting env. episode 493.000000, reward total was -20.000000. running mean: -20.368295\n",
            "resetting env. episode 494.000000, reward total was -20.000000. running mean: -20.364612\n",
            "resetting env. episode 495.000000, reward total was -20.000000. running mean: -20.360966\n",
            "resetting env. episode 496.000000, reward total was -20.000000. running mean: -20.357356\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.363783\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.370145\n",
            "resetting env. episode 499.000000, reward total was -20.000000. running mean: -20.366443\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.372779\n",
            "CPU times: user 28min 19s, sys: 12min 31s, total: 40min 51s\n",
            "Wall time: 21min 17s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "cHYCDYwhlVLV",
        "outputId": "dad5b9b7-11b7-41cf-fda0-33fd200546d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist2 = train_model(env, model, total_episodes=500)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 7.000000, reward total was -20.000000. running mean: -20.990000\n",
            "resetting env. episode 8.000000, reward total was -20.000000. running mean: -20.980100\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.980299\n",
            "resetting env. episode 10.000000, reward total was -20.000000. running mean: -20.970496\n",
            "resetting env. episode 11.000000, reward total was -20.000000. running mean: -20.960791\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.961183\n",
            "resetting env. episode 13.000000, reward total was -20.000000. running mean: -20.951571\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.952056\n",
            "resetting env. episode 15.000000, reward total was -20.000000. running mean: -20.942535\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.943110\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.943679\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.944242\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -20.934799\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.935451\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -20.936097\n",
            "resetting env. episode 22.000000, reward total was -20.000000. running mean: -20.926736\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.927469\n",
            "resetting env. episode 24.000000, reward total was -19.000000. running mean: -20.908194\n",
            "resetting env. episode 25.000000, reward total was -20.000000. running mean: -20.899112\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.900121\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.901120\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.902108\n",
            "resetting env. episode 29.000000, reward total was -20.000000. running mean: -20.893087\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.894156\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.895215\n",
            "resetting env. episode 32.000000, reward total was -20.000000. running mean: -20.886263\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.887400\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.888526\n",
            "resetting env. episode 35.000000, reward total was -20.000000. running mean: -20.879641\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.880844\n",
            "resetting env. episode 37.000000, reward total was -20.000000. running mean: -20.872036\n",
            "resetting env. episode 38.000000, reward total was -19.000000. running mean: -20.853316\n",
            "resetting env. episode 39.000000, reward total was -20.000000. running mean: -20.844782\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.846335\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.847871\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.849393\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.850899\n",
            "resetting env. episode 44.000000, reward total was -20.000000. running mean: -20.842390\n",
            "resetting env. episode 45.000000, reward total was -20.000000. running mean: -20.833966\n",
            "resetting env. episode 46.000000, reward total was -20.000000. running mean: -20.825626\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.827370\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.829096\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -20.830805\n",
            "resetting env. episode 50.000000, reward total was -20.000000. running mean: -20.822497\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.824272\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.826029\n",
            "resetting env. episode 53.000000, reward total was -18.000000. running mean: -20.797769\n",
            "resetting env. episode 54.000000, reward total was -20.000000. running mean: -20.789791\n",
            "resetting env. episode 55.000000, reward total was -19.000000. running mean: -20.771894\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.774175\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.776433\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.778669\n",
            "resetting env. episode 59.000000, reward total was -19.000000. running mean: -20.760882\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.763273\n",
            "resetting env. episode 61.000000, reward total was -19.000000. running mean: -20.745640\n",
            "resetting env. episode 62.000000, reward total was -18.000000. running mean: -20.718184\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.721002\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.723792\n",
            "resetting env. episode 65.000000, reward total was -18.000000. running mean: -20.696554\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.699589\n",
            "resetting env. episode 67.000000, reward total was -20.000000. running mean: -20.692593\n",
            "resetting env. episode 68.000000, reward total was -20.000000. running mean: -20.685667\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.688810\n",
            "resetting env. episode 70.000000, reward total was -19.000000. running mean: -20.671922\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -20.665203\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.668551\n",
            "resetting env. episode 73.000000, reward total was -20.000000. running mean: -20.661865\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.665247\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.668594\n",
            "resetting env. episode 76.000000, reward total was -20.000000. running mean: -20.661908\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.665289\n",
            "resetting env. episode 78.000000, reward total was -18.000000. running mean: -20.638636\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.642250\n",
            "resetting env. episode 80.000000, reward total was -20.000000. running mean: -20.635827\n",
            "resetting env. episode 81.000000, reward total was -18.000000. running mean: -20.609469\n",
            "resetting env. episode 82.000000, reward total was -20.000000. running mean: -20.603374\n",
            "resetting env. episode 83.000000, reward total was -20.000000. running mean: -20.597341\n",
            "resetting env. episode 84.000000, reward total was -20.000000. running mean: -20.591367\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.595454\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.599499\n",
            "resetting env. episode 87.000000, reward total was -20.000000. running mean: -20.593504\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.597569\n",
            "resetting env. episode 89.000000, reward total was -20.000000. running mean: -20.591593\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.595677\n",
            "resetting env. episode 91.000000, reward total was -20.000000. running mean: -20.589721\n",
            "resetting env. episode 92.000000, reward total was -19.000000. running mean: -20.573823\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.578085\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.582304\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.586481\n",
            "resetting env. episode 96.000000, reward total was -18.000000. running mean: -20.560616\n",
            "resetting env. episode 97.000000, reward total was -20.000000. running mean: -20.555010\n",
            "resetting env. episode 98.000000, reward total was -20.000000. running mean: -20.549460\n",
            "resetting env. episode 99.000000, reward total was -20.000000. running mean: -20.543966\n",
            "resetting env. episode 100.000000, reward total was -19.000000. running mean: -20.528526\n",
            "resetting env. episode 101.000000, reward total was -19.000000. running mean: -20.513241\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.518108\n",
            "resetting env. episode 103.000000, reward total was -19.000000. running mean: -20.502927\n",
            "resetting env. episode 104.000000, reward total was -20.000000. running mean: -20.497898\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.502919\n",
            "resetting env. episode 106.000000, reward total was -19.000000. running mean: -20.487890\n",
            "resetting env. episode 107.000000, reward total was -19.000000. running mean: -20.473011\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -20.468281\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.473598\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.478862\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.484073\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -20.489233\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -20.484340\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.489497\n",
            "resetting env. episode 115.000000, reward total was -20.000000. running mean: -20.484602\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.489756\n",
            "resetting env. episode 117.000000, reward total was -20.000000. running mean: -20.484858\n",
            "resetting env. episode 118.000000, reward total was -20.000000. running mean: -20.480010\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.485210\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.490358\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.495454\n",
            "resetting env. episode 122.000000, reward total was -20.000000. running mean: -20.490499\n",
            "resetting env. episode 123.000000, reward total was -20.000000. running mean: -20.485594\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.490738\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.495831\n",
            "resetting env. episode 126.000000, reward total was -18.000000. running mean: -20.470873\n",
            "resetting env. episode 127.000000, reward total was -20.000000. running mean: -20.466164\n",
            "resetting env. episode 128.000000, reward total was -19.000000. running mean: -20.451502\n",
            "resetting env. episode 129.000000, reward total was -20.000000. running mean: -20.446987\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.452518\n",
            "resetting env. episode 131.000000, reward total was -20.000000. running mean: -20.447992\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.453512\n",
            "resetting env. episode 133.000000, reward total was -20.000000. running mean: -20.448977\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.454488\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.459943\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.465343\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.470690\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.475983\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.481223\n",
            "resetting env. episode 140.000000, reward total was -18.000000. running mean: -20.456411\n",
            "resetting env. episode 141.000000, reward total was -20.000000. running mean: -20.451847\n",
            "resetting env. episode 142.000000, reward total was -18.000000. running mean: -20.427328\n",
            "resetting env. episode 143.000000, reward total was -20.000000. running mean: -20.423055\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.428824\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.434536\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.440191\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -20.435789\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.441431\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.447017\n",
            "resetting env. episode 150.000000, reward total was -20.000000. running mean: -20.442547\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -20.438121\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.443740\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.449302\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.454809\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.460261\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.465659\n",
            "resetting env. episode 157.000000, reward total was -19.000000. running mean: -20.451002\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.456492\n",
            "resetting env. episode 159.000000, reward total was -19.000000. running mean: -20.441927\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.447508\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.453033\n",
            "resetting env. episode 162.000000, reward total was -20.000000. running mean: -20.448503\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.454018\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.459477\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.464883\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.470234\n",
            "resetting env. episode 167.000000, reward total was -20.000000. running mean: -20.465531\n",
            "resetting env. episode 168.000000, reward total was -19.000000. running mean: -20.450876\n",
            "resetting env. episode 169.000000, reward total was -20.000000. running mean: -20.446367\n",
            "resetting env. episode 170.000000, reward total was -20.000000. running mean: -20.441904\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.447485\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.453010\n",
            "resetting env. episode 173.000000, reward total was -19.000000. running mean: -20.438480\n",
            "resetting env. episode 174.000000, reward total was -20.000000. running mean: -20.434095\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.439754\n",
            "resetting env. episode 176.000000, reward total was -20.000000. running mean: -20.435356\n",
            "resetting env. episode 177.000000, reward total was -20.000000. running mean: -20.431003\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.436693\n",
            "resetting env. episode 179.000000, reward total was -20.000000. running mean: -20.432326\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.438003\n",
            "resetting env. episode 181.000000, reward total was -20.000000. running mean: -20.433623\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.439286\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.444893\n",
            "resetting env. episode 184.000000, reward total was -20.000000. running mean: -20.440445\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.446040\n",
            "resetting env. episode 186.000000, reward total was -19.000000. running mean: -20.431580\n",
            "resetting env. episode 187.000000, reward total was -19.000000. running mean: -20.417264\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.423091\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.428860\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.434572\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.440226\n",
            "resetting env. episode 192.000000, reward total was -20.000000. running mean: -20.435824\n",
            "resetting env. episode 193.000000, reward total was -20.000000. running mean: -20.431466\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.437151\n",
            "resetting env. episode 195.000000, reward total was -20.000000. running mean: -20.432779\n",
            "resetting env. episode 196.000000, reward total was -20.000000. running mean: -20.428452\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.434167\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.439825\n",
            "resetting env. episode 199.000000, reward total was -20.000000. running mean: -20.435427\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.441073\n",
            "resetting env. episode 201.000000, reward total was -19.000000. running mean: -20.426662\n",
            "resetting env. episode 202.000000, reward total was -20.000000. running mean: -20.422396\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.428172\n",
            "resetting env. episode 204.000000, reward total was -20.000000. running mean: -20.423890\n",
            "resetting env. episode 205.000000, reward total was -19.000000. running mean: -20.409651\n",
            "resetting env. episode 206.000000, reward total was -20.000000. running mean: -20.405554\n",
            "resetting env. episode 207.000000, reward total was -19.000000. running mean: -20.391499\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.397584\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.403608\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.409572\n",
            "resetting env. episode 211.000000, reward total was -16.000000. running mean: -20.365476\n",
            "resetting env. episode 212.000000, reward total was -19.000000. running mean: -20.351822\n",
            "resetting env. episode 213.000000, reward total was -20.000000. running mean: -20.348303\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.354820\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.361272\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.367659\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.373983\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.380243\n",
            "resetting env. episode 219.000000, reward total was -20.000000. running mean: -20.376440\n",
            "resetting env. episode 220.000000, reward total was -20.000000. running mean: -20.372676\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.368949\n",
            "resetting env. episode 222.000000, reward total was -17.000000. running mean: -20.335260\n",
            "resetting env. episode 223.000000, reward total was -20.000000. running mean: -20.331907\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.338588\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.345202\n",
            "resetting env. episode 226.000000, reward total was -19.000000. running mean: -20.331750\n",
            "resetting env. episode 227.000000, reward total was -20.000000. running mean: -20.328433\n",
            "resetting env. episode 228.000000, reward total was -20.000000. running mean: -20.325148\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.331897\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.338578\n",
            "resetting env. episode 231.000000, reward total was -20.000000. running mean: -20.335192\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.341840\n",
            "resetting env. episode 233.000000, reward total was -20.000000. running mean: -20.338422\n",
            "resetting env. episode 234.000000, reward total was -19.000000. running mean: -20.325038\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.331787\n",
            "resetting env. episode 236.000000, reward total was -20.000000. running mean: -20.328469\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.335185\n",
            "resetting env. episode 238.000000, reward total was -20.000000. running mean: -20.331833\n",
            "resetting env. episode 239.000000, reward total was -20.000000. running mean: -20.328515\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.335229\n",
            "resetting env. episode 241.000000, reward total was -20.000000. running mean: -20.331877\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.338558\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.345173\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.351721\n",
            "resetting env. episode 245.000000, reward total was -19.000000. running mean: -20.338204\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.344822\n",
            "resetting env. episode 247.000000, reward total was -20.000000. running mean: -20.341374\n",
            "resetting env. episode 248.000000, reward total was -19.000000. running mean: -20.327960\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.334680\n",
            "resetting env. episode 250.000000, reward total was -20.000000. running mean: -20.331333\n",
            "resetting env. episode 251.000000, reward total was -20.000000. running mean: -20.328020\n",
            "resetting env. episode 252.000000, reward total was -19.000000. running mean: -20.314740\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.321592\n",
            "resetting env. episode 254.000000, reward total was -20.000000. running mean: -20.318377\n",
            "resetting env. episode 255.000000, reward total was -20.000000. running mean: -20.315193\n",
            "resetting env. episode 256.000000, reward total was -20.000000. running mean: -20.312041\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.318920\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -20.315731\n",
            "resetting env. episode 259.000000, reward total was -18.000000. running mean: -20.292574\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.299648\n",
            "resetting env. episode 261.000000, reward total was -20.000000. running mean: -20.296652\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.303685\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.310648\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.317542\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.324366\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.331123\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.337812\n",
            "resetting env. episode 268.000000, reward total was -20.000000. running mean: -20.334433\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.341089\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.347678\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.354201\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.360659\n",
            "resetting env. episode 273.000000, reward total was -20.000000. running mean: -20.357053\n",
            "resetting env. episode 274.000000, reward total was -19.000000. running mean: -20.343482\n",
            "resetting env. episode 275.000000, reward total was -20.000000. running mean: -20.340047\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.346647\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.353181\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.359649\n",
            "resetting env. episode 279.000000, reward total was -20.000000. running mean: -20.356052\n",
            "resetting env. episode 280.000000, reward total was -20.000000. running mean: -20.352492\n",
            "resetting env. episode 281.000000, reward total was -19.000000. running mean: -20.338967\n",
            "resetting env. episode 282.000000, reward total was -20.000000. running mean: -20.335577\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.342221\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.348799\n",
            "resetting env. episode 285.000000, reward total was -20.000000. running mean: -20.345311\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.351858\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.358339\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.364756\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.371108\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.377397\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.383623\n",
            "resetting env. episode 292.000000, reward total was -18.000000. running mean: -20.359787\n",
            "resetting env. episode 293.000000, reward total was -20.000000. running mean: -20.356189\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.362627\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.369001\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.375311\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.381558\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.377742\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.383965\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.390125\n",
            "resetting env. episode 301.000000, reward total was -18.000000. running mean: -20.366224\n",
            "resetting env. episode 302.000000, reward total was -20.000000. running mean: -20.362562\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.368936\n",
            "resetting env. episode 304.000000, reward total was -19.000000. running mean: -20.355247\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.361694\n",
            "resetting env. episode 306.000000, reward total was -19.000000. running mean: -20.348077\n",
            "resetting env. episode 307.000000, reward total was -18.000000. running mean: -20.324597\n",
            "resetting env. episode 308.000000, reward total was -20.000000. running mean: -20.321351\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.328137\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.334856\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.341507\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.348092\n",
            "resetting env. episode 313.000000, reward total was -20.000000. running mean: -20.344611\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.351165\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.357654\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.364077\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.370436\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.376732\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.382965\n",
            "resetting env. episode 320.000000, reward total was -20.000000. running mean: -20.379135\n",
            "resetting env. episode 321.000000, reward total was -19.000000. running mean: -20.365344\n",
            "resetting env. episode 322.000000, reward total was -19.000000. running mean: -20.351690\n",
            "resetting env. episode 323.000000, reward total was -20.000000. running mean: -20.348173\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.354692\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.361145\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.367533\n",
            "resetting env. episode 327.000000, reward total was -20.000000. running mean: -20.363858\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -20.360219\n",
            "resetting env. episode 329.000000, reward total was -19.000000. running mean: -20.346617\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.353151\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.359619\n",
            "resetting env. episode 332.000000, reward total was -20.000000. running mean: -20.356023\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.362463\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.368838\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.375150\n",
            "resetting env. episode 336.000000, reward total was -20.000000. running mean: -20.371398\n",
            "resetting env. episode 337.000000, reward total was -20.000000. running mean: -20.367684\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.374008\n",
            "resetting env. episode 339.000000, reward total was -20.000000. running mean: -20.370268\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.376565\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.382799\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.388971\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.395081\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.401131\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.407119\n",
            "resetting env. episode 346.000000, reward total was -20.000000. running mean: -20.403048\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.409018\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.414928\n",
            "resetting env. episode 349.000000, reward total was -20.000000. running mean: -20.410778\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.416670\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -20.412504\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.418379\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.424195\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.429953\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.435653\n",
            "resetting env. episode 356.000000, reward total was -20.000000. running mean: -20.431297\n",
            "resetting env. episode 357.000000, reward total was -20.000000. running mean: -20.426984\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -20.422714\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.428487\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.434202\n",
            "resetting env. episode 361.000000, reward total was -20.000000. running mean: -20.429860\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.435561\n",
            "resetting env. episode 363.000000, reward total was -20.000000. running mean: -20.431206\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.436894\n",
            "resetting env. episode 365.000000, reward total was -20.000000. running mean: -20.432525\n",
            "resetting env. episode 366.000000, reward total was -20.000000. running mean: -20.428200\n",
            "resetting env. episode 367.000000, reward total was -19.000000. running mean: -20.413918\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.419778\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.425581\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.431325\n",
            "resetting env. episode 371.000000, reward total was -18.000000. running mean: -20.407012\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.412941\n",
            "resetting env. episode 373.000000, reward total was -20.000000. running mean: -20.408812\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.414724\n",
            "resetting env. episode 375.000000, reward total was -19.000000. running mean: -20.400577\n",
            "resetting env. episode 376.000000, reward total was -20.000000. running mean: -20.396571\n",
            "resetting env. episode 377.000000, reward total was -20.000000. running mean: -20.392605\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.398679\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.404692\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.410645\n",
            "resetting env. episode 381.000000, reward total was -20.000000. running mean: -20.406539\n",
            "resetting env. episode 382.000000, reward total was -18.000000. running mean: -20.382474\n",
            "resetting env. episode 383.000000, reward total was -20.000000. running mean: -20.378649\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.384862\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.391014\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.397104\n",
            "resetting env. episode 387.000000, reward total was -15.000000. running mean: -20.343133\n",
            "resetting env. episode 388.000000, reward total was -20.000000. running mean: -20.339701\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.346304\n",
            "resetting env. episode 390.000000, reward total was -20.000000. running mean: -20.342841\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.349413\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.355919\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.362359\n",
            "resetting env. episode 394.000000, reward total was -19.000000. running mean: -20.348736\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.355249\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.361696\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.368079\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.374398\n",
            "resetting env. episode 399.000000, reward total was -20.000000. running mean: -20.370654\n",
            "resetting env. episode 400.000000, reward total was -20.000000. running mean: -20.366948\n",
            "resetting env. episode 401.000000, reward total was -20.000000. running mean: -20.363278\n",
            "resetting env. episode 402.000000, reward total was -20.000000. running mean: -20.359646\n",
            "resetting env. episode 403.000000, reward total was -20.000000. running mean: -20.356049\n",
            "resetting env. episode 404.000000, reward total was -19.000000. running mean: -20.342489\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.349064\n",
            "resetting env. episode 406.000000, reward total was -19.000000. running mean: -20.335573\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.342217\n",
            "resetting env. episode 408.000000, reward total was -20.000000. running mean: -20.338795\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.345407\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.351953\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.358434\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.364849\n",
            "resetting env. episode 413.000000, reward total was -19.000000. running mean: -20.351201\n",
            "resetting env. episode 414.000000, reward total was -19.000000. running mean: -20.337689\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.344312\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.350869\n",
            "resetting env. episode 417.000000, reward total was -19.000000. running mean: -20.337360\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.343986\n",
            "resetting env. episode 419.000000, reward total was -20.000000. running mean: -20.340547\n",
            "resetting env. episode 420.000000, reward total was -19.000000. running mean: -20.327141\n",
            "resetting env. episode 421.000000, reward total was -20.000000. running mean: -20.323870\n",
            "resetting env. episode 422.000000, reward total was -20.000000. running mean: -20.320631\n",
            "resetting env. episode 423.000000, reward total was -20.000000. running mean: -20.317425\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.324250\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.331008\n",
            "resetting env. episode 426.000000, reward total was -20.000000. running mean: -20.327698\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.334421\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.341077\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.347666\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.354189\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.360647\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.367041\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.373370\n",
            "resetting env. episode 434.000000, reward total was -19.000000. running mean: -20.359637\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.366040\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.372380\n",
            "resetting env. episode 437.000000, reward total was -20.000000. running mean: -20.368656\n",
            "resetting env. episode 438.000000, reward total was -19.000000. running mean: -20.354970\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.351420\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.357906\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -20.354327\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.360783\n",
            "resetting env. episode 443.000000, reward total was -20.000000. running mean: -20.357176\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.363604\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.369968\n",
            "resetting env. episode 446.000000, reward total was -19.000000. running mean: -20.356268\n",
            "resetting env. episode 447.000000, reward total was -19.000000. running mean: -20.342705\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.349278\n",
            "resetting env. episode 449.000000, reward total was -20.000000. running mean: -20.345786\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.352328\n",
            "resetting env. episode 451.000000, reward total was -20.000000. running mean: -20.348804\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.355316\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.361763\n",
            "resetting env. episode 454.000000, reward total was -20.000000. running mean: -20.358146\n",
            "resetting env. episode 455.000000, reward total was -17.000000. running mean: -20.324564\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.331319\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.338005\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.344625\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.351179\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.357667\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.364091\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.370450\n",
            "resetting env. episode 463.000000, reward total was -20.000000. running mean: -20.366745\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.373078\n",
            "resetting env. episode 465.000000, reward total was -20.000000. running mean: -20.369347\n",
            "resetting env. episode 466.000000, reward total was -20.000000. running mean: -20.365653\n",
            "resetting env. episode 467.000000, reward total was -18.000000. running mean: -20.341997\n",
            "resetting env. episode 468.000000, reward total was -20.000000. running mean: -20.338577\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.345191\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.351739\n",
            "resetting env. episode 471.000000, reward total was -18.000000. running mean: -20.328222\n",
            "resetting env. episode 472.000000, reward total was -20.000000. running mean: -20.324940\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -20.321690\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.328473\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.335189\n",
            "resetting env. episode 476.000000, reward total was -20.000000. running mean: -20.331837\n",
            "resetting env. episode 477.000000, reward total was -20.000000. running mean: -20.328518\n",
            "resetting env. episode 478.000000, reward total was -19.000000. running mean: -20.315233\n",
            "resetting env. episode 479.000000, reward total was -19.000000. running mean: -20.302081\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.309060\n",
            "resetting env. episode 481.000000, reward total was -18.000000. running mean: -20.285969\n",
            "resetting env. episode 482.000000, reward total was -18.000000. running mean: -20.263110\n",
            "resetting env. episode 483.000000, reward total was -20.000000. running mean: -20.260479\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.267874\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.275195\n",
            "resetting env. episode 486.000000, reward total was -19.000000. running mean: -20.262443\n",
            "resetting env. episode 487.000000, reward total was -20.000000. running mean: -20.259819\n",
            "resetting env. episode 488.000000, reward total was -20.000000. running mean: -20.257221\n",
            "resetting env. episode 489.000000, reward total was -19.000000. running mean: -20.244648\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.252202\n",
            "resetting env. episode 491.000000, reward total was -20.000000. running mean: -20.249680\n",
            "resetting env. episode 492.000000, reward total was -20.000000. running mean: -20.247183\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.254711\n",
            "resetting env. episode 494.000000, reward total was -20.000000. running mean: -20.252164\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.259642\n",
            "resetting env. episode 496.000000, reward total was -19.000000. running mean: -20.247046\n",
            "resetting env. episode 497.000000, reward total was -20.000000. running mean: -20.244576\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.252130\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.259609\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.267012\n",
            "CPU times: user 29min 8s, sys: 12min 49s, total: 41min 58s\n",
            "Wall time: 21min 47s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "8fheN9DRlWXQ",
        "outputId": "76b5f6ac-9695-4fc2-914e-0ed7b7494e0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "cell_type": "code",
      "source": [
        "play_game(env, model)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -10.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ4AAAFZCAYAAABpOsHqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAWJQAAFiUBSVIk8AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAGeUlEQVR4nO3dwW5c5RmA4TMoNI5NsBM7BlyEoQI2SGxgy4oNXEoXiKtgi9ReBtsuuIWuSsW2FVApUhJiJ3acZJKANF23g4TfY0fHdp5neaT/9zfSzKs5vzwzs8ViMQAUL009AHD+CAeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQCQeQXRq78LN3rxz7Y7UvzYbhk93Lw+rLz69Tr29tDqsrV5au39nfHx7N58feZ3NjfVh/5eqJ53nw6OGwd//gxPtw+g53t4ZHb1w78T6rdw6HjR9/PoWJpvPlt/dmY9aNDsfn7y2/SKf0+o0bw41ry0+GR/N5DMfGsLuzc+J5bt6+Ixxn1OHb28PPH71z4n22vv/PuQ/HWG5VgEw4gEw4gEw4gGz04eiL5uDoaHhw9HDp+tVX1oZrr746wUSctrVb94e1W8sH2o9fWx8e/vH6BBOdXcJxTPv3D4Yfbt5cur67syMcF8T6j3eHnb//a+n67Y//JBz/x60KkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkAkHkPkin2O6snJ5uL6+vnR9dWVlgml4Hp6urw4P3tpcuv5kY22Cac424Timne3tYWd7e+oxeI72P3hz2P/gzanHOBfcqgCZcACZcACZcADZhTkcfTyfD4eXlh/OL7/+mvZ58vTZcHh0dOJ55k+fnHgPno/LR/Pf/P2UvM/h8X/M/KKZLRaLUQv/8vn1cQthYqf5xJ2d4l5T+PLbe6MewoV5xwHHdd5f7GeBMw4gEw4gG32r8skXfz3NOYBzZPTh6P7+vsNROOc2NzdHHfm4VQEy4QAy4QAy4QAy4QAy4QAy4QAy4QAy4QAy4QAy4QAy4QAy4QCy0R+r/+c3X5/mHMAEPv3zV6PW+c5ReIGN/c5RtypAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAJhxAdmnqAeBF92Rjdbj74e7S9T8czYft734aZhPM9HuEAyb27OqV4c5H7wzD7H8TsXbrYNj+7qdphvodblWATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiATDiAzM8jwMQuzZ8NG/++vXR95eDxBNMcj3DAxFb3joZ3//aPqcdI3KoAmXAAmXAAmXAAmXAAmXAAmXAA2ej/47jx/senOQdwjswWi8WohXt7e+MWAmfG1tbWbMy60e84ZrNRfw+4AJxxAJlwAJlwAJlwAJlwAJlwAJlwAJlwAJlwAJlwAJlwAJlwAJlwAJlwAJlwAJlwAJlwAJlwAJlwAJlwAJlwAJlwANno31UBXlzecQCZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcACZcADZfwGPL5xgnSCrygAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "9AxOcQhIsKow",
        "outputId": "3833595d-07a7-4d49-e47b-14514d3dfafe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist3 = train_model(env, model, total_episodes=1500)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 5.000000, reward total was -20.000000. running mean: -20.990000\n",
            "resetting env. episode 6.000000, reward total was -20.000000. running mean: -20.980100\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.980299\n",
            "resetting env. episode 8.000000, reward total was -20.000000. running mean: -20.970496\n",
            "resetting env. episode 9.000000, reward total was -20.000000. running mean: -20.960791\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.961183\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.961571\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.961956\n",
            "resetting env. episode 13.000000, reward total was -19.000000. running mean: -20.942336\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.942913\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.943484\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.944049\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.944608\n",
            "resetting env. episode 18.000000, reward total was -20.000000. running mean: -20.935162\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -20.925811\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.926552\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -20.927287\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.928014\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.928734\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.929447\n",
            "resetting env. episode 25.000000, reward total was -18.000000. running mean: -20.900152\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.901151\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.902139\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.903118\n",
            "resetting env. episode 29.000000, reward total was -20.000000. running mean: -20.894086\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.895146\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.896194\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -20.897232\n",
            "resetting env. episode 33.000000, reward total was -20.000000. running mean: -20.888260\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.889377\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.890484\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.891579\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.892663\n",
            "resetting env. episode 38.000000, reward total was -20.000000. running mean: -20.883736\n",
            "resetting env. episode 39.000000, reward total was -20.000000. running mean: -20.874899\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.876150\n",
            "resetting env. episode 41.000000, reward total was -19.000000. running mean: -20.857388\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.858815\n",
            "resetting env. episode 43.000000, reward total was -19.000000. running mean: -20.840226\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.841824\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.843406\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.844972\n",
            "resetting env. episode 47.000000, reward total was -20.000000. running mean: -20.836522\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.838157\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -20.839775\n",
            "resetting env. episode 50.000000, reward total was -19.000000. running mean: -20.821378\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.823164\n",
            "resetting env. episode 52.000000, reward total was -20.000000. running mean: -20.814932\n",
            "resetting env. episode 53.000000, reward total was -19.000000. running mean: -20.796783\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.798815\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.800827\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.802819\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.804790\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.806743\n",
            "resetting env. episode 59.000000, reward total was -20.000000. running mean: -20.798675\n",
            "resetting env. episode 60.000000, reward total was -20.000000. running mean: -20.790688\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.792781\n",
            "resetting env. episode 62.000000, reward total was -20.000000. running mean: -20.784854\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.787005\n",
            "resetting env. episode 64.000000, reward total was -20.000000. running mean: -20.779135\n",
            "resetting env. episode 65.000000, reward total was -19.000000. running mean: -20.761344\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.763730\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.766093\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.768432\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.770748\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.773040\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -20.765310\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.767657\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.769980\n",
            "resetting env. episode 74.000000, reward total was -17.000000. running mean: -20.732280\n",
            "resetting env. episode 75.000000, reward total was -18.000000. running mean: -20.704958\n",
            "resetting env. episode 76.000000, reward total was -18.000000. running mean: -20.677908\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.681129\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.684318\n",
            "resetting env. episode 79.000000, reward total was -20.000000. running mean: -20.677474\n",
            "resetting env. episode 80.000000, reward total was -20.000000. running mean: -20.670700\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.673993\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.677253\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.680480\n",
            "resetting env. episode 84.000000, reward total was -20.000000. running mean: -20.673675\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.676939\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -20.670169\n",
            "resetting env. episode 87.000000, reward total was -20.000000. running mean: -20.663468\n",
            "resetting env. episode 88.000000, reward total was -20.000000. running mean: -20.656833\n",
            "resetting env. episode 89.000000, reward total was -20.000000. running mean: -20.650265\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.653762\n",
            "resetting env. episode 91.000000, reward total was -20.000000. running mean: -20.647224\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.650752\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.654245\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.657702\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.661125\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.664514\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -20.667869\n",
            "resetting env. episode 98.000000, reward total was -20.000000. running mean: -20.661190\n",
            "resetting env. episode 99.000000, reward total was -20.000000. running mean: -20.654578\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.658032\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.661452\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.664837\n",
            "resetting env. episode 103.000000, reward total was -19.000000. running mean: -20.648189\n",
            "resetting env. episode 104.000000, reward total was -20.000000. running mean: -20.641707\n",
            "resetting env. episode 105.000000, reward total was -19.000000. running mean: -20.625290\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -20.629037\n",
            "resetting env. episode 107.000000, reward total was -20.000000. running mean: -20.622747\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -20.626519\n",
            "resetting env. episode 109.000000, reward total was -20.000000. running mean: -20.620254\n",
            "resetting env. episode 110.000000, reward total was -20.000000. running mean: -20.614052\n",
            "resetting env. episode 111.000000, reward total was -19.000000. running mean: -20.597911\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -20.601932\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -20.595913\n",
            "resetting env. episode 114.000000, reward total was -20.000000. running mean: -20.589954\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.594054\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.598114\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.602132\n",
            "resetting env. episode 118.000000, reward total was -20.000000. running mean: -20.596111\n",
            "resetting env. episode 119.000000, reward total was -20.000000. running mean: -20.590150\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.594248\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.598306\n",
            "resetting env. episode 122.000000, reward total was -20.000000. running mean: -20.592323\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.596400\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.600436\n",
            "resetting env. episode 125.000000, reward total was -19.000000. running mean: -20.584431\n",
            "resetting env. episode 126.000000, reward total was -20.000000. running mean: -20.578587\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.582801\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.586973\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.591103\n",
            "resetting env. episode 130.000000, reward total was -20.000000. running mean: -20.585192\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.589340\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.593447\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.597513\n",
            "resetting env. episode 134.000000, reward total was -20.000000. running mean: -20.591537\n",
            "resetting env. episode 135.000000, reward total was -20.000000. running mean: -20.585622\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.589766\n",
            "resetting env. episode 137.000000, reward total was -19.000000. running mean: -20.573868\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.578130\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.582348\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.586525\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.590659\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.594753\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.598805\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.602817\n",
            "resetting env. episode 145.000000, reward total was -18.000000. running mean: -20.576789\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.581021\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -20.575211\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.579459\n",
            "resetting env. episode 149.000000, reward total was -20.000000. running mean: -20.573664\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.577928\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.582148\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.586327\n",
            "resetting env. episode 153.000000, reward total was -20.000000. running mean: -20.580464\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.584659\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.588812\n",
            "resetting env. episode 156.000000, reward total was -20.000000. running mean: -20.582924\n",
            "resetting env. episode 157.000000, reward total was -18.000000. running mean: -20.557095\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.561524\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.565909\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.570250\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.574547\n",
            "resetting env. episode 162.000000, reward total was -20.000000. running mean: -20.568802\n",
            "resetting env. episode 163.000000, reward total was -19.000000. running mean: -20.553114\n",
            "resetting env. episode 164.000000, reward total was -19.000000. running mean: -20.537583\n",
            "resetting env. episode 165.000000, reward total was -20.000000. running mean: -20.532207\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.536885\n",
            "resetting env. episode 167.000000, reward total was -20.000000. running mean: -20.531516\n",
            "resetting env. episode 168.000000, reward total was -19.000000. running mean: -20.516201\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.521039\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.525828\n",
            "resetting env. episode 171.000000, reward total was -19.000000. running mean: -20.510570\n",
            "resetting env. episode 172.000000, reward total was -19.000000. running mean: -20.495464\n",
            "resetting env. episode 173.000000, reward total was -17.000000. running mean: -20.460510\n",
            "resetting env. episode 174.000000, reward total was -20.000000. running mean: -20.455905\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.461346\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.466732\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.472065\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.477344\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.482571\n",
            "resetting env. episode 180.000000, reward total was -20.000000. running mean: -20.477745\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.482968\n",
            "resetting env. episode 182.000000, reward total was -19.000000. running mean: -20.468138\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.473457\n",
            "resetting env. episode 184.000000, reward total was -19.000000. running mean: -20.458722\n",
            "resetting env. episode 185.000000, reward total was -20.000000. running mean: -20.454135\n",
            "resetting env. episode 186.000000, reward total was -20.000000. running mean: -20.449593\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.455097\n",
            "resetting env. episode 188.000000, reward total was -19.000000. running mean: -20.440546\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.446141\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.451680\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.457163\n",
            "resetting env. episode 192.000000, reward total was -20.000000. running mean: -20.452591\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.458065\n",
            "resetting env. episode 194.000000, reward total was -20.000000. running mean: -20.453485\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.458950\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.464360\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.469717\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.475019\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.480269\n",
            "resetting env. episode 200.000000, reward total was -20.000000. running mean: -20.475467\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.480712\n",
            "resetting env. episode 202.000000, reward total was -20.000000. running mean: -20.475905\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.481146\n",
            "resetting env. episode 204.000000, reward total was -19.000000. running mean: -20.466334\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.471671\n",
            "resetting env. episode 206.000000, reward total was -20.000000. running mean: -20.466954\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.472285\n",
            "resetting env. episode 208.000000, reward total was -19.000000. running mean: -20.457562\n",
            "resetting env. episode 209.000000, reward total was -19.000000. running mean: -20.442986\n",
            "resetting env. episode 210.000000, reward total was -20.000000. running mean: -20.438556\n",
            "resetting env. episode 211.000000, reward total was -17.000000. running mean: -20.404171\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.410129\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.416028\n",
            "resetting env. episode 214.000000, reward total was -18.000000. running mean: -20.391868\n",
            "resetting env. episode 215.000000, reward total was -20.000000. running mean: -20.387949\n",
            "resetting env. episode 216.000000, reward total was -20.000000. running mean: -20.384069\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.390229\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.396326\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.402363\n",
            "resetting env. episode 220.000000, reward total was -20.000000. running mean: -20.398340\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.394356\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.400413\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.406408\n",
            "resetting env. episode 224.000000, reward total was -19.000000. running mean: -20.392344\n",
            "resetting env. episode 225.000000, reward total was -20.000000. running mean: -20.388421\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.394537\n",
            "resetting env. episode 227.000000, reward total was -19.000000. running mean: -20.380591\n",
            "resetting env. episode 228.000000, reward total was -18.000000. running mean: -20.356785\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.363218\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.369585\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.375890\n",
            "resetting env. episode 232.000000, reward total was -19.000000. running mean: -20.362131\n",
            "resetting env. episode 233.000000, reward total was -20.000000. running mean: -20.358509\n",
            "resetting env. episode 234.000000, reward total was -20.000000. running mean: -20.354924\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.361375\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.367761\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.374084\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.380343\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.386539\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.392674\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.398747\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.404760\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.410712\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.416605\n",
            "resetting env. episode 245.000000, reward total was -18.000000. running mean: -20.392439\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.398515\n",
            "resetting env. episode 247.000000, reward total was -20.000000. running mean: -20.394529\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.400584\n",
            "resetting env. episode 249.000000, reward total was -20.000000. running mean: -20.396578\n",
            "resetting env. episode 250.000000, reward total was -19.000000. running mean: -20.382613\n",
            "resetting env. episode 251.000000, reward total was -20.000000. running mean: -20.378786\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.384999\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.391149\n",
            "resetting env. episode 254.000000, reward total was -18.000000. running mean: -20.367237\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.373565\n",
            "resetting env. episode 256.000000, reward total was -20.000000. running mean: -20.369829\n",
            "resetting env. episode 257.000000, reward total was -19.000000. running mean: -20.356131\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.362569\n",
            "resetting env. episode 259.000000, reward total was -19.000000. running mean: -20.348944\n",
            "resetting env. episode 260.000000, reward total was -20.000000. running mean: -20.345454\n",
            "resetting env. episode 261.000000, reward total was -20.000000. running mean: -20.342000\n",
            "resetting env. episode 262.000000, reward total was -20.000000. running mean: -20.338580\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.345194\n",
            "resetting env. episode 264.000000, reward total was -20.000000. running mean: -20.341742\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.348325\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.354841\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.361293\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.367680\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.374003\n",
            "resetting env. episode 270.000000, reward total was -19.000000. running mean: -20.360263\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.366661\n",
            "resetting env. episode 272.000000, reward total was -19.000000. running mean: -20.352994\n",
            "resetting env. episode 273.000000, reward total was -20.000000. running mean: -20.349464\n",
            "resetting env. episode 274.000000, reward total was -20.000000. running mean: -20.345969\n",
            "resetting env. episode 275.000000, reward total was -20.000000. running mean: -20.342510\n",
            "resetting env. episode 276.000000, reward total was -20.000000. running mean: -20.339085\n",
            "resetting env. episode 277.000000, reward total was -20.000000. running mean: -20.335694\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.342337\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.348913\n",
            "resetting env. episode 280.000000, reward total was -20.000000. running mean: -20.345424\n",
            "resetting env. episode 281.000000, reward total was -20.000000. running mean: -20.341970\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.348550\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.355065\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.361514\n",
            "resetting env. episode 285.000000, reward total was -20.000000. running mean: -20.357899\n",
            "resetting env. episode 286.000000, reward total was -20.000000. running mean: -20.354320\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.360777\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.367169\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.373497\n",
            "resetting env. episode 290.000000, reward total was -19.000000. running mean: -20.359762\n",
            "resetting env. episode 291.000000, reward total was -20.000000. running mean: -20.356165\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.362603\n",
            "resetting env. episode 293.000000, reward total was -20.000000. running mean: -20.358977\n",
            "resetting env. episode 294.000000, reward total was -19.000000. running mean: -20.345387\n",
            "resetting env. episode 295.000000, reward total was -19.000000. running mean: -20.331933\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.338614\n",
            "resetting env. episode 297.000000, reward total was -20.000000. running mean: -20.335228\n",
            "resetting env. episode 298.000000, reward total was -18.000000. running mean: -20.311876\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.318757\n",
            "resetting env. episode 300.000000, reward total was -20.000000. running mean: -20.315569\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.322414\n",
            "resetting env. episode 302.000000, reward total was -20.000000. running mean: -20.319190\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.325998\n",
            "resetting env. episode 304.000000, reward total was -20.000000. running mean: -20.322738\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.329510\n",
            "resetting env. episode 306.000000, reward total was -18.000000. running mean: -20.306215\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.313153\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.320022\n",
            "resetting env. episode 309.000000, reward total was -19.000000. running mean: -20.306821\n",
            "resetting env. episode 310.000000, reward total was -19.000000. running mean: -20.293753\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.300816\n",
            "resetting env. episode 312.000000, reward total was -20.000000. running mean: -20.297807\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.304829\n",
            "resetting env. episode 314.000000, reward total was -19.000000. running mean: -20.291781\n",
            "resetting env. episode 315.000000, reward total was -20.000000. running mean: -20.288863\n",
            "resetting env. episode 316.000000, reward total was -20.000000. running mean: -20.285975\n",
            "resetting env. episode 317.000000, reward total was -20.000000. running mean: -20.283115\n",
            "resetting env. episode 318.000000, reward total was -19.000000. running mean: -20.270284\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.277581\n",
            "resetting env. episode 320.000000, reward total was -20.000000. running mean: -20.274805\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.282057\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.289236\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.296344\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.303381\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.310347\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.317243\n",
            "resetting env. episode 327.000000, reward total was -19.000000. running mean: -20.304071\n",
            "resetting env. episode 328.000000, reward total was -19.000000. running mean: -20.291030\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.298120\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.305139\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.312087\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.318966\n",
            "resetting env. episode 333.000000, reward total was -20.000000. running mean: -20.315777\n",
            "resetting env. episode 334.000000, reward total was -19.000000. running mean: -20.302619\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.309593\n",
            "resetting env. episode 336.000000, reward total was -20.000000. running mean: -20.306497\n",
            "resetting env. episode 337.000000, reward total was -20.000000. running mean: -20.303432\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.310398\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.317294\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.324121\n",
            "resetting env. episode 341.000000, reward total was -20.000000. running mean: -20.320880\n",
            "resetting env. episode 342.000000, reward total was -20.000000. running mean: -20.317671\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.324494\n",
            "resetting env. episode 344.000000, reward total was -20.000000. running mean: -20.321249\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.328037\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.334756\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.341409\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.347995\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.354515\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.360969\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -20.357360\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.363786\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.370148\n",
            "resetting env. episode 354.000000, reward total was -20.000000. running mean: -20.366447\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.372782\n",
            "resetting env. episode 356.000000, reward total was -20.000000. running mean: -20.369055\n",
            "resetting env. episode 357.000000, reward total was -20.000000. running mean: -20.365364\n",
            "resetting env. episode 358.000000, reward total was -18.000000. running mean: -20.341710\n",
            "resetting env. episode 359.000000, reward total was -20.000000. running mean: -20.338293\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.344910\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.351461\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -20.347947\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.354467\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.360922\n",
            "resetting env. episode 365.000000, reward total was -20.000000. running mean: -20.357313\n",
            "resetting env. episode 366.000000, reward total was -20.000000. running mean: -20.353740\n",
            "resetting env. episode 367.000000, reward total was -19.000000. running mean: -20.340203\n",
            "resetting env. episode 368.000000, reward total was -19.000000. running mean: -20.326801\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.333533\n",
            "resetting env. episode 370.000000, reward total was -20.000000. running mean: -20.330197\n",
            "resetting env. episode 371.000000, reward total was -19.000000. running mean: -20.316895\n",
            "resetting env. episode 372.000000, reward total was -20.000000. running mean: -20.313726\n",
            "resetting env. episode 373.000000, reward total was -20.000000. running mean: -20.310589\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.317483\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.324308\n",
            "resetting env. episode 376.000000, reward total was -20.000000. running mean: -20.321065\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.327855\n",
            "resetting env. episode 378.000000, reward total was -19.000000. running mean: -20.314576\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.321430\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.328216\n",
            "resetting env. episode 381.000000, reward total was -20.000000. running mean: -20.324934\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.331685\n",
            "resetting env. episode 383.000000, reward total was -18.000000. running mean: -20.308368\n",
            "resetting env. episode 384.000000, reward total was -19.000000. running mean: -20.295284\n",
            "resetting env. episode 385.000000, reward total was -19.000000. running mean: -20.282331\n",
            "resetting env. episode 386.000000, reward total was -20.000000. running mean: -20.279508\n",
            "resetting env. episode 387.000000, reward total was -18.000000. running mean: -20.256713\n",
            "resetting env. episode 388.000000, reward total was -20.000000. running mean: -20.254146\n",
            "resetting env. episode 389.000000, reward total was -17.000000. running mean: -20.221604\n",
            "resetting env. episode 390.000000, reward total was -20.000000. running mean: -20.219388\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.227194\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -20.224922\n",
            "resetting env. episode 393.000000, reward total was -19.000000. running mean: -20.212673\n",
            "resetting env. episode 394.000000, reward total was -20.000000. running mean: -20.210546\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.218441\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.226257\n",
            "resetting env. episode 397.000000, reward total was -19.000000. running mean: -20.213994\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.221854\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.229635\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.237339\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.244966\n",
            "resetting env. episode 402.000000, reward total was -18.000000. running mean: -20.222516\n",
            "resetting env. episode 403.000000, reward total was -20.000000. running mean: -20.220291\n",
            "resetting env. episode 404.000000, reward total was -20.000000. running mean: -20.218088\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.225907\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.233648\n",
            "resetting env. episode 407.000000, reward total was -19.000000. running mean: -20.221312\n",
            "resetting env. episode 408.000000, reward total was -20.000000. running mean: -20.219098\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.226907\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.234638\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -20.232292\n",
            "resetting env. episode 412.000000, reward total was -19.000000. running mean: -20.219969\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.227769\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.235492\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -20.233137\n",
            "resetting env. episode 416.000000, reward total was -19.000000. running mean: -20.220805\n",
            "resetting env. episode 417.000000, reward total was -20.000000. running mean: -20.218597\n",
            "resetting env. episode 418.000000, reward total was -20.000000. running mean: -20.216411\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.224247\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.232005\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.239685\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.247288\n",
            "resetting env. episode 423.000000, reward total was -20.000000. running mean: -20.244815\n",
            "resetting env. episode 424.000000, reward total was -20.000000. running mean: -20.242367\n",
            "resetting env. episode 425.000000, reward total was -19.000000. running mean: -20.229943\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.237644\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.245267\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.252815\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.260287\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.267684\n",
            "resetting env. episode 431.000000, reward total was -19.000000. running mean: -20.255007\n",
            "resetting env. episode 432.000000, reward total was -19.000000. running mean: -20.242457\n",
            "resetting env. episode 433.000000, reward total was -20.000000. running mean: -20.240032\n",
            "resetting env. episode 434.000000, reward total was -20.000000. running mean: -20.237632\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -20.235256\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.242903\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.250474\n",
            "resetting env. episode 438.000000, reward total was -20.000000. running mean: -20.247969\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.255490\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.262935\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -20.260305\n",
            "resetting env. episode 442.000000, reward total was -18.000000. running mean: -20.237702\n",
            "resetting env. episode 443.000000, reward total was -20.000000. running mean: -20.235325\n",
            "resetting env. episode 444.000000, reward total was -20.000000. running mean: -20.232972\n",
            "resetting env. episode 445.000000, reward total was -20.000000. running mean: -20.230642\n",
            "resetting env. episode 446.000000, reward total was -20.000000. running mean: -20.228336\n",
            "resetting env. episode 447.000000, reward total was -19.000000. running mean: -20.216052\n",
            "resetting env. episode 448.000000, reward total was -20.000000. running mean: -20.213892\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.221753\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.229535\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.237240\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.244868\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.252419\n",
            "resetting env. episode 454.000000, reward total was -20.000000. running mean: -20.249895\n",
            "resetting env. episode 455.000000, reward total was -18.000000. running mean: -20.227396\n",
            "resetting env. episode 456.000000, reward total was -19.000000. running mean: -20.215122\n",
            "resetting env. episode 457.000000, reward total was -20.000000. running mean: -20.212971\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.220841\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.228633\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.236346\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.243983\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.251543\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.259028\n",
            "resetting env. episode 464.000000, reward total was -19.000000. running mean: -20.246437\n",
            "resetting env. episode 465.000000, reward total was -18.000000. running mean: -20.223973\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.231733\n",
            "resetting env. episode 467.000000, reward total was -20.000000. running mean: -20.229416\n",
            "resetting env. episode 468.000000, reward total was -20.000000. running mean: -20.227122\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.234850\n",
            "resetting env. episode 470.000000, reward total was -20.000000. running mean: -20.232502\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.240177\n",
            "resetting env. episode 472.000000, reward total was -19.000000. running mean: -20.227775\n",
            "resetting env. episode 473.000000, reward total was -18.000000. running mean: -20.205497\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.213442\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.221308\n",
            "resetting env. episode 476.000000, reward total was -20.000000. running mean: -20.219095\n",
            "resetting env. episode 477.000000, reward total was -20.000000. running mean: -20.216904\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.214735\n",
            "resetting env. episode 479.000000, reward total was -20.000000. running mean: -20.212588\n",
            "resetting env. episode 480.000000, reward total was -19.000000. running mean: -20.200462\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.208457\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.216373\n",
            "resetting env. episode 483.000000, reward total was -19.000000. running mean: -20.204209\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.212167\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.220045\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.227845\n",
            "resetting env. episode 487.000000, reward total was -19.000000. running mean: -20.215566\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.223411\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.231176\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.238865\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.246476\n",
            "resetting env. episode 492.000000, reward total was -17.000000. running mean: -20.214011\n",
            "resetting env. episode 493.000000, reward total was -20.000000. running mean: -20.211871\n",
            "resetting env. episode 494.000000, reward total was -20.000000. running mean: -20.209752\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.217655\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.225478\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.233224\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.240891\n",
            "resetting env. episode 499.000000, reward total was -20.000000. running mean: -20.238482\n",
            "resetting env. episode 500.000000, reward total was -20.000000. running mean: -20.236098\n",
            "resetting env. episode 501.000000, reward total was -21.000000. running mean: -20.243737\n",
            "resetting env. episode 502.000000, reward total was -21.000000. running mean: -20.251299\n",
            "resetting env. episode 503.000000, reward total was -21.000000. running mean: -20.258786\n",
            "resetting env. episode 504.000000, reward total was -20.000000. running mean: -20.256198\n",
            "resetting env. episode 505.000000, reward total was -19.000000. running mean: -20.243636\n",
            "resetting env. episode 506.000000, reward total was -20.000000. running mean: -20.241200\n",
            "resetting env. episode 507.000000, reward total was -20.000000. running mean: -20.238788\n",
            "resetting env. episode 508.000000, reward total was -21.000000. running mean: -20.246400\n",
            "resetting env. episode 509.000000, reward total was -21.000000. running mean: -20.253936\n",
            "resetting env. episode 510.000000, reward total was -20.000000. running mean: -20.251397\n",
            "resetting env. episode 511.000000, reward total was -21.000000. running mean: -20.258883\n",
            "resetting env. episode 512.000000, reward total was -21.000000. running mean: -20.266294\n",
            "resetting env. episode 513.000000, reward total was -21.000000. running mean: -20.273631\n",
            "resetting env. episode 514.000000, reward total was -20.000000. running mean: -20.270895\n",
            "resetting env. episode 515.000000, reward total was -18.000000. running mean: -20.248186\n",
            "resetting env. episode 516.000000, reward total was -20.000000. running mean: -20.245704\n",
            "resetting env. episode 517.000000, reward total was -21.000000. running mean: -20.253247\n",
            "resetting env. episode 518.000000, reward total was -21.000000. running mean: -20.260714\n",
            "resetting env. episode 519.000000, reward total was -20.000000. running mean: -20.258107\n",
            "resetting env. episode 520.000000, reward total was -21.000000. running mean: -20.265526\n",
            "resetting env. episode 521.000000, reward total was -20.000000. running mean: -20.262871\n",
            "resetting env. episode 522.000000, reward total was -21.000000. running mean: -20.270242\n",
            "resetting env. episode 523.000000, reward total was -20.000000. running mean: -20.267540\n",
            "resetting env. episode 524.000000, reward total was -21.000000. running mean: -20.274864\n",
            "resetting env. episode 525.000000, reward total was -20.000000. running mean: -20.272116\n",
            "resetting env. episode 526.000000, reward total was -21.000000. running mean: -20.279395\n",
            "resetting env. episode 527.000000, reward total was -21.000000. running mean: -20.286601\n",
            "resetting env. episode 528.000000, reward total was -20.000000. running mean: -20.283735\n",
            "resetting env. episode 529.000000, reward total was -18.000000. running mean: -20.260897\n",
            "resetting env. episode 530.000000, reward total was -20.000000. running mean: -20.258288\n",
            "resetting env. episode 531.000000, reward total was -20.000000. running mean: -20.255705\n",
            "resetting env. episode 532.000000, reward total was -21.000000. running mean: -20.263148\n",
            "resetting env. episode 533.000000, reward total was -21.000000. running mean: -20.270517\n",
            "resetting env. episode 534.000000, reward total was -21.000000. running mean: -20.277812\n",
            "resetting env. episode 535.000000, reward total was -20.000000. running mean: -20.275034\n",
            "resetting env. episode 536.000000, reward total was -19.000000. running mean: -20.262283\n",
            "resetting env. episode 537.000000, reward total was -20.000000. running mean: -20.259660\n",
            "resetting env. episode 538.000000, reward total was -21.000000. running mean: -20.267064\n",
            "resetting env. episode 539.000000, reward total was -21.000000. running mean: -20.274393\n",
            "resetting env. episode 540.000000, reward total was -21.000000. running mean: -20.281649\n",
            "resetting env. episode 541.000000, reward total was -18.000000. running mean: -20.258833\n",
            "resetting env. episode 542.000000, reward total was -20.000000. running mean: -20.256244\n",
            "resetting env. episode 543.000000, reward total was -21.000000. running mean: -20.263682\n",
            "resetting env. episode 544.000000, reward total was -21.000000. running mean: -20.271045\n",
            "resetting env. episode 545.000000, reward total was -20.000000. running mean: -20.268335\n",
            "resetting env. episode 546.000000, reward total was -21.000000. running mean: -20.275651\n",
            "resetting env. episode 547.000000, reward total was -19.000000. running mean: -20.262895\n",
            "resetting env. episode 548.000000, reward total was -20.000000. running mean: -20.260266\n",
            "resetting env. episode 549.000000, reward total was -20.000000. running mean: -20.257663\n",
            "resetting env. episode 550.000000, reward total was -21.000000. running mean: -20.265087\n",
            "resetting env. episode 551.000000, reward total was -21.000000. running mean: -20.272436\n",
            "resetting env. episode 552.000000, reward total was -21.000000. running mean: -20.279711\n",
            "resetting env. episode 553.000000, reward total was -21.000000. running mean: -20.286914\n",
            "resetting env. episode 554.000000, reward total was -20.000000. running mean: -20.284045\n",
            "resetting env. episode 555.000000, reward total was -21.000000. running mean: -20.291205\n",
            "resetting env. episode 556.000000, reward total was -20.000000. running mean: -20.288293\n",
            "resetting env. episode 557.000000, reward total was -21.000000. running mean: -20.295410\n",
            "resetting env. episode 558.000000, reward total was -20.000000. running mean: -20.292456\n",
            "resetting env. episode 559.000000, reward total was -20.000000. running mean: -20.289531\n",
            "resetting env. episode 560.000000, reward total was -20.000000. running mean: -20.286636\n",
            "resetting env. episode 561.000000, reward total was -20.000000. running mean: -20.283769\n",
            "resetting env. episode 562.000000, reward total was -21.000000. running mean: -20.290932\n",
            "resetting env. episode 563.000000, reward total was -21.000000. running mean: -20.298022\n",
            "resetting env. episode 564.000000, reward total was -21.000000. running mean: -20.305042\n",
            "resetting env. episode 565.000000, reward total was -21.000000. running mean: -20.311992\n",
            "resetting env. episode 566.000000, reward total was -21.000000. running mean: -20.318872\n",
            "resetting env. episode 567.000000, reward total was -19.000000. running mean: -20.305683\n",
            "resetting env. episode 568.000000, reward total was -20.000000. running mean: -20.302626\n",
            "resetting env. episode 569.000000, reward total was -21.000000. running mean: -20.309600\n",
            "resetting env. episode 570.000000, reward total was -21.000000. running mean: -20.316504\n",
            "resetting env. episode 571.000000, reward total was -19.000000. running mean: -20.303339\n",
            "resetting env. episode 572.000000, reward total was -21.000000. running mean: -20.310306\n",
            "resetting env. episode 573.000000, reward total was -20.000000. running mean: -20.307203\n",
            "resetting env. episode 574.000000, reward total was -20.000000. running mean: -20.304131\n",
            "resetting env. episode 575.000000, reward total was -21.000000. running mean: -20.311089\n",
            "resetting env. episode 576.000000, reward total was -20.000000. running mean: -20.307978\n",
            "resetting env. episode 577.000000, reward total was -21.000000. running mean: -20.314899\n",
            "resetting env. episode 578.000000, reward total was -21.000000. running mean: -20.321750\n",
            "resetting env. episode 579.000000, reward total was -20.000000. running mean: -20.318532\n",
            "resetting env. episode 580.000000, reward total was -21.000000. running mean: -20.325347\n",
            "resetting env. episode 581.000000, reward total was -21.000000. running mean: -20.332093\n",
            "resetting env. episode 582.000000, reward total was -20.000000. running mean: -20.328772\n",
            "resetting env. episode 583.000000, reward total was -19.000000. running mean: -20.315485\n",
            "resetting env. episode 584.000000, reward total was -21.000000. running mean: -20.322330\n",
            "resetting env. episode 585.000000, reward total was -21.000000. running mean: -20.329106\n",
            "resetting env. episode 586.000000, reward total was -19.000000. running mean: -20.315815\n",
            "resetting env. episode 587.000000, reward total was -21.000000. running mean: -20.322657\n",
            "resetting env. episode 588.000000, reward total was -21.000000. running mean: -20.329431\n",
            "resetting env. episode 589.000000, reward total was -20.000000. running mean: -20.326136\n",
            "resetting env. episode 590.000000, reward total was -21.000000. running mean: -20.332875\n",
            "resetting env. episode 591.000000, reward total was -20.000000. running mean: -20.329546\n",
            "resetting env. episode 592.000000, reward total was -20.000000. running mean: -20.326251\n",
            "resetting env. episode 593.000000, reward total was -21.000000. running mean: -20.332988\n",
            "resetting env. episode 594.000000, reward total was -21.000000. running mean: -20.339658\n",
            "resetting env. episode 595.000000, reward total was -21.000000. running mean: -20.346262\n",
            "resetting env. episode 596.000000, reward total was -20.000000. running mean: -20.342799\n",
            "resetting env. episode 597.000000, reward total was -21.000000. running mean: -20.349371\n",
            "resetting env. episode 598.000000, reward total was -19.000000. running mean: -20.335878\n",
            "resetting env. episode 599.000000, reward total was -20.000000. running mean: -20.332519\n",
            "resetting env. episode 600.000000, reward total was -21.000000. running mean: -20.339194\n",
            "resetting env. episode 601.000000, reward total was -20.000000. running mean: -20.335802\n",
            "resetting env. episode 602.000000, reward total was -21.000000. running mean: -20.342444\n",
            "resetting env. episode 603.000000, reward total was -21.000000. running mean: -20.349019\n",
            "resetting env. episode 604.000000, reward total was -21.000000. running mean: -20.355529\n",
            "resetting env. episode 605.000000, reward total was -20.000000. running mean: -20.351974\n",
            "resetting env. episode 606.000000, reward total was -19.000000. running mean: -20.338454\n",
            "resetting env. episode 607.000000, reward total was -21.000000. running mean: -20.345069\n",
            "resetting env. episode 608.000000, reward total was -20.000000. running mean: -20.341619\n",
            "resetting env. episode 609.000000, reward total was -19.000000. running mean: -20.328203\n",
            "resetting env. episode 610.000000, reward total was -19.000000. running mean: -20.314920\n",
            "resetting env. episode 611.000000, reward total was -21.000000. running mean: -20.321771\n",
            "resetting env. episode 612.000000, reward total was -21.000000. running mean: -20.328554\n",
            "resetting env. episode 613.000000, reward total was -21.000000. running mean: -20.335268\n",
            "resetting env. episode 614.000000, reward total was -21.000000. running mean: -20.341915\n",
            "resetting env. episode 615.000000, reward total was -21.000000. running mean: -20.348496\n",
            "resetting env. episode 616.000000, reward total was -20.000000. running mean: -20.345011\n",
            "resetting env. episode 617.000000, reward total was -21.000000. running mean: -20.351561\n",
            "resetting env. episode 618.000000, reward total was -20.000000. running mean: -20.348046\n",
            "resetting env. episode 619.000000, reward total was -21.000000. running mean: -20.354565\n",
            "resetting env. episode 620.000000, reward total was -21.000000. running mean: -20.361019\n",
            "resetting env. episode 621.000000, reward total was -19.000000. running mean: -20.347409\n",
            "resetting env. episode 622.000000, reward total was -20.000000. running mean: -20.343935\n",
            "resetting env. episode 623.000000, reward total was -21.000000. running mean: -20.350496\n",
            "resetting env. episode 624.000000, reward total was -20.000000. running mean: -20.346991\n",
            "resetting env. episode 625.000000, reward total was -21.000000. running mean: -20.353521\n",
            "resetting env. episode 626.000000, reward total was -21.000000. running mean: -20.359986\n",
            "resetting env. episode 627.000000, reward total was -21.000000. running mean: -20.366386\n",
            "resetting env. episode 628.000000, reward total was -20.000000. running mean: -20.362722\n",
            "resetting env. episode 629.000000, reward total was -20.000000. running mean: -20.359095\n",
            "resetting env. episode 630.000000, reward total was -21.000000. running mean: -20.365504\n",
            "resetting env. episode 631.000000, reward total was -21.000000. running mean: -20.371849\n",
            "resetting env. episode 632.000000, reward total was -21.000000. running mean: -20.378130\n",
            "resetting env. episode 633.000000, reward total was -20.000000. running mean: -20.374349\n",
            "resetting env. episode 634.000000, reward total was -21.000000. running mean: -20.380605\n",
            "resetting env. episode 635.000000, reward total was -21.000000. running mean: -20.386799\n",
            "resetting env. episode 636.000000, reward total was -21.000000. running mean: -20.392931\n",
            "resetting env. episode 637.000000, reward total was -18.000000. running mean: -20.369002\n",
            "resetting env. episode 638.000000, reward total was -19.000000. running mean: -20.355312\n",
            "resetting env. episode 639.000000, reward total was -21.000000. running mean: -20.361759\n",
            "resetting env. episode 640.000000, reward total was -21.000000. running mean: -20.368141\n",
            "resetting env. episode 641.000000, reward total was -20.000000. running mean: -20.364460\n",
            "resetting env. episode 642.000000, reward total was -20.000000. running mean: -20.360815\n",
            "resetting env. episode 643.000000, reward total was -20.000000. running mean: -20.357207\n",
            "resetting env. episode 644.000000, reward total was -20.000000. running mean: -20.353635\n",
            "resetting env. episode 645.000000, reward total was -21.000000. running mean: -20.360099\n",
            "resetting env. episode 646.000000, reward total was -20.000000. running mean: -20.356498\n",
            "resetting env. episode 647.000000, reward total was -21.000000. running mean: -20.362933\n",
            "resetting env. episode 648.000000, reward total was -20.000000. running mean: -20.359304\n",
            "resetting env. episode 649.000000, reward total was -20.000000. running mean: -20.355710\n",
            "resetting env. episode 650.000000, reward total was -21.000000. running mean: -20.362153\n",
            "resetting env. episode 651.000000, reward total was -20.000000. running mean: -20.358532\n",
            "resetting env. episode 652.000000, reward total was -20.000000. running mean: -20.354947\n",
            "resetting env. episode 653.000000, reward total was -20.000000. running mean: -20.351397\n",
            "resetting env. episode 654.000000, reward total was -21.000000. running mean: -20.357883\n",
            "resetting env. episode 655.000000, reward total was -21.000000. running mean: -20.364304\n",
            "resetting env. episode 656.000000, reward total was -21.000000. running mean: -20.370661\n",
            "resetting env. episode 657.000000, reward total was -21.000000. running mean: -20.376955\n",
            "resetting env. episode 658.000000, reward total was -21.000000. running mean: -20.383185\n",
            "resetting env. episode 659.000000, reward total was -21.000000. running mean: -20.389353\n",
            "resetting env. episode 660.000000, reward total was -21.000000. running mean: -20.395460\n",
            "resetting env. episode 661.000000, reward total was -21.000000. running mean: -20.401505\n",
            "resetting env. episode 662.000000, reward total was -20.000000. running mean: -20.397490\n",
            "resetting env. episode 663.000000, reward total was -20.000000. running mean: -20.393515\n",
            "resetting env. episode 664.000000, reward total was -19.000000. running mean: -20.379580\n",
            "resetting env. episode 665.000000, reward total was -21.000000. running mean: -20.385784\n",
            "resetting env. episode 666.000000, reward total was -21.000000. running mean: -20.391926\n",
            "resetting env. episode 667.000000, reward total was -20.000000. running mean: -20.388007\n",
            "resetting env. episode 668.000000, reward total was -20.000000. running mean: -20.384127\n",
            "resetting env. episode 669.000000, reward total was -20.000000. running mean: -20.380286\n",
            "resetting env. episode 670.000000, reward total was -21.000000. running mean: -20.386483\n",
            "resetting env. episode 671.000000, reward total was -21.000000. running mean: -20.392618\n",
            "resetting env. episode 672.000000, reward total was -20.000000. running mean: -20.388692\n",
            "resetting env. episode 673.000000, reward total was -19.000000. running mean: -20.374805\n",
            "resetting env. episode 674.000000, reward total was -21.000000. running mean: -20.381057\n",
            "resetting env. episode 675.000000, reward total was -20.000000. running mean: -20.377246\n",
            "resetting env. episode 676.000000, reward total was -20.000000. running mean: -20.373474\n",
            "resetting env. episode 677.000000, reward total was -21.000000. running mean: -20.379739\n",
            "resetting env. episode 678.000000, reward total was -17.000000. running mean: -20.345942\n",
            "resetting env. episode 679.000000, reward total was -21.000000. running mean: -20.352482\n",
            "resetting env. episode 680.000000, reward total was -21.000000. running mean: -20.358957\n",
            "resetting env. episode 681.000000, reward total was -21.000000. running mean: -20.365368\n",
            "resetting env. episode 682.000000, reward total was -21.000000. running mean: -20.371714\n",
            "resetting env. episode 683.000000, reward total was -20.000000. running mean: -20.367997\n",
            "resetting env. episode 684.000000, reward total was -21.000000. running mean: -20.374317\n",
            "resetting env. episode 685.000000, reward total was -21.000000. running mean: -20.380574\n",
            "resetting env. episode 686.000000, reward total was -20.000000. running mean: -20.376768\n",
            "resetting env. episode 687.000000, reward total was -20.000000. running mean: -20.373001\n",
            "resetting env. episode 688.000000, reward total was -20.000000. running mean: -20.369271\n",
            "resetting env. episode 689.000000, reward total was -21.000000. running mean: -20.375578\n",
            "resetting env. episode 690.000000, reward total was -20.000000. running mean: -20.371822\n",
            "resetting env. episode 691.000000, reward total was -20.000000. running mean: -20.368104\n",
            "resetting env. episode 692.000000, reward total was -21.000000. running mean: -20.374423\n",
            "resetting env. episode 693.000000, reward total was -19.000000. running mean: -20.360679\n",
            "resetting env. episode 694.000000, reward total was -21.000000. running mean: -20.367072\n",
            "resetting env. episode 695.000000, reward total was -19.000000. running mean: -20.353401\n",
            "resetting env. episode 696.000000, reward total was -21.000000. running mean: -20.359867\n",
            "resetting env. episode 697.000000, reward total was -21.000000. running mean: -20.366268\n",
            "resetting env. episode 698.000000, reward total was -20.000000. running mean: -20.362606\n",
            "resetting env. episode 699.000000, reward total was -21.000000. running mean: -20.368980\n",
            "resetting env. episode 700.000000, reward total was -20.000000. running mean: -20.365290\n",
            "resetting env. episode 701.000000, reward total was -21.000000. running mean: -20.371637\n",
            "resetting env. episode 702.000000, reward total was -20.000000. running mean: -20.367921\n",
            "resetting env. episode 703.000000, reward total was -21.000000. running mean: -20.374241\n",
            "resetting env. episode 704.000000, reward total was -21.000000. running mean: -20.380499\n",
            "resetting env. episode 705.000000, reward total was -21.000000. running mean: -20.386694\n",
            "resetting env. episode 706.000000, reward total was -19.000000. running mean: -20.372827\n",
            "resetting env. episode 707.000000, reward total was -21.000000. running mean: -20.379099\n",
            "resetting env. episode 708.000000, reward total was -20.000000. running mean: -20.375308\n",
            "resetting env. episode 709.000000, reward total was -20.000000. running mean: -20.371555\n",
            "resetting env. episode 710.000000, reward total was -21.000000. running mean: -20.377839\n",
            "resetting env. episode 711.000000, reward total was -21.000000. running mean: -20.384061\n",
            "resetting env. episode 712.000000, reward total was -21.000000. running mean: -20.390220\n",
            "resetting env. episode 713.000000, reward total was -21.000000. running mean: -20.396318\n",
            "resetting env. episode 714.000000, reward total was -20.000000. running mean: -20.392355\n",
            "resetting env. episode 715.000000, reward total was -21.000000. running mean: -20.398431\n",
            "resetting env. episode 716.000000, reward total was -21.000000. running mean: -20.404447\n",
            "resetting env. episode 717.000000, reward total was -21.000000. running mean: -20.410402\n",
            "resetting env. episode 718.000000, reward total was -21.000000. running mean: -20.416298\n",
            "resetting env. episode 719.000000, reward total was -21.000000. running mean: -20.422135\n",
            "resetting env. episode 720.000000, reward total was -21.000000. running mean: -20.427914\n",
            "resetting env. episode 721.000000, reward total was -21.000000. running mean: -20.433635\n",
            "resetting env. episode 722.000000, reward total was -20.000000. running mean: -20.429299\n",
            "resetting env. episode 723.000000, reward total was -20.000000. running mean: -20.425006\n",
            "resetting env. episode 724.000000, reward total was -19.000000. running mean: -20.410756\n",
            "resetting env. episode 725.000000, reward total was -20.000000. running mean: -20.406648\n",
            "resetting env. episode 726.000000, reward total was -20.000000. running mean: -20.402581\n",
            "resetting env. episode 727.000000, reward total was -21.000000. running mean: -20.408556\n",
            "resetting env. episode 728.000000, reward total was -20.000000. running mean: -20.404470\n",
            "resetting env. episode 729.000000, reward total was -20.000000. running mean: -20.400425\n",
            "resetting env. episode 730.000000, reward total was -21.000000. running mean: -20.406421\n",
            "resetting env. episode 731.000000, reward total was -21.000000. running mean: -20.412357\n",
            "resetting env. episode 732.000000, reward total was -20.000000. running mean: -20.408233\n",
            "resetting env. episode 733.000000, reward total was -21.000000. running mean: -20.414151\n",
            "resetting env. episode 734.000000, reward total was -20.000000. running mean: -20.410010\n",
            "resetting env. episode 735.000000, reward total was -21.000000. running mean: -20.415909\n",
            "resetting env. episode 736.000000, reward total was -21.000000. running mean: -20.421750\n",
            "resetting env. episode 737.000000, reward total was -19.000000. running mean: -20.407533\n",
            "resetting env. episode 738.000000, reward total was -21.000000. running mean: -20.413458\n",
            "resetting env. episode 739.000000, reward total was -18.000000. running mean: -20.389323\n",
            "resetting env. episode 740.000000, reward total was -21.000000. running mean: -20.395430\n",
            "resetting env. episode 741.000000, reward total was -21.000000. running mean: -20.401475\n",
            "resetting env. episode 742.000000, reward total was -20.000000. running mean: -20.397461\n",
            "resetting env. episode 743.000000, reward total was -18.000000. running mean: -20.373486\n",
            "resetting env. episode 744.000000, reward total was -21.000000. running mean: -20.379751\n",
            "resetting env. episode 745.000000, reward total was -19.000000. running mean: -20.365954\n",
            "resetting env. episode 746.000000, reward total was -21.000000. running mean: -20.372294\n",
            "resetting env. episode 747.000000, reward total was -21.000000. running mean: -20.378571\n",
            "resetting env. episode 748.000000, reward total was -21.000000. running mean: -20.384785\n",
            "resetting env. episode 749.000000, reward total was -19.000000. running mean: -20.370938\n",
            "resetting env. episode 750.000000, reward total was -21.000000. running mean: -20.377228\n",
            "resetting env. episode 751.000000, reward total was -21.000000. running mean: -20.383456\n",
            "resetting env. episode 752.000000, reward total was -21.000000. running mean: -20.389621\n",
            "resetting env. episode 753.000000, reward total was -21.000000. running mean: -20.395725\n",
            "resetting env. episode 754.000000, reward total was -21.000000. running mean: -20.401768\n",
            "resetting env. episode 755.000000, reward total was -20.000000. running mean: -20.397750\n",
            "resetting env. episode 756.000000, reward total was -21.000000. running mean: -20.403773\n",
            "resetting env. episode 757.000000, reward total was -20.000000. running mean: -20.399735\n",
            "resetting env. episode 758.000000, reward total was -20.000000. running mean: -20.395738\n",
            "resetting env. episode 759.000000, reward total was -21.000000. running mean: -20.401780\n",
            "resetting env. episode 760.000000, reward total was -18.000000. running mean: -20.377763\n",
            "resetting env. episode 761.000000, reward total was -21.000000. running mean: -20.383985\n",
            "resetting env. episode 762.000000, reward total was -18.000000. running mean: -20.360145\n",
            "resetting env. episode 763.000000, reward total was -21.000000. running mean: -20.366544\n",
            "resetting env. episode 764.000000, reward total was -21.000000. running mean: -20.372878\n",
            "resetting env. episode 765.000000, reward total was -20.000000. running mean: -20.369149\n",
            "resetting env. episode 766.000000, reward total was -19.000000. running mean: -20.355458\n",
            "resetting env. episode 767.000000, reward total was -21.000000. running mean: -20.361903\n",
            "resetting env. episode 768.000000, reward total was -21.000000. running mean: -20.368284\n",
            "resetting env. episode 769.000000, reward total was -21.000000. running mean: -20.374601\n",
            "resetting env. episode 770.000000, reward total was -21.000000. running mean: -20.380855\n",
            "resetting env. episode 771.000000, reward total was -21.000000. running mean: -20.387047\n",
            "resetting env. episode 772.000000, reward total was -20.000000. running mean: -20.383176\n",
            "resetting env. episode 773.000000, reward total was -20.000000. running mean: -20.379345\n",
            "resetting env. episode 774.000000, reward total was -20.000000. running mean: -20.375551\n",
            "resetting env. episode 775.000000, reward total was -21.000000. running mean: -20.381796\n",
            "resetting env. episode 776.000000, reward total was -19.000000. running mean: -20.367978\n",
            "resetting env. episode 777.000000, reward total was -20.000000. running mean: -20.364298\n",
            "resetting env. episode 778.000000, reward total was -20.000000. running mean: -20.360655\n",
            "resetting env. episode 779.000000, reward total was -20.000000. running mean: -20.357048\n",
            "resetting env. episode 780.000000, reward total was -21.000000. running mean: -20.363478\n",
            "resetting env. episode 781.000000, reward total was -20.000000. running mean: -20.359843\n",
            "resetting env. episode 782.000000, reward total was -20.000000. running mean: -20.356245\n",
            "resetting env. episode 783.000000, reward total was -20.000000. running mean: -20.352682\n",
            "resetting env. episode 784.000000, reward total was -20.000000. running mean: -20.349155\n",
            "resetting env. episode 785.000000, reward total was -21.000000. running mean: -20.355664\n",
            "resetting env. episode 786.000000, reward total was -21.000000. running mean: -20.362107\n",
            "resetting env. episode 787.000000, reward total was -21.000000. running mean: -20.368486\n",
            "resetting env. episode 788.000000, reward total was -20.000000. running mean: -20.364801\n",
            "resetting env. episode 789.000000, reward total was -19.000000. running mean: -20.351153\n",
            "resetting env. episode 790.000000, reward total was -21.000000. running mean: -20.357642\n",
            "resetting env. episode 791.000000, reward total was -20.000000. running mean: -20.354065\n",
            "resetting env. episode 792.000000, reward total was -20.000000. running mean: -20.350525\n",
            "resetting env. episode 793.000000, reward total was -20.000000. running mean: -20.347019\n",
            "resetting env. episode 794.000000, reward total was -21.000000. running mean: -20.353549\n",
            "resetting env. episode 795.000000, reward total was -21.000000. running mean: -20.360014\n",
            "resetting env. episode 796.000000, reward total was -20.000000. running mean: -20.356414\n",
            "resetting env. episode 797.000000, reward total was -21.000000. running mean: -20.362849\n",
            "resetting env. episode 798.000000, reward total was -21.000000. running mean: -20.369221\n",
            "resetting env. episode 799.000000, reward total was -21.000000. running mean: -20.375529\n",
            "resetting env. episode 800.000000, reward total was -21.000000. running mean: -20.381773\n",
            "resetting env. episode 801.000000, reward total was -20.000000. running mean: -20.377956\n",
            "resetting env. episode 802.000000, reward total was -21.000000. running mean: -20.384176\n",
            "resetting env. episode 803.000000, reward total was -18.000000. running mean: -20.360334\n",
            "resetting env. episode 804.000000, reward total was -21.000000. running mean: -20.366731\n",
            "resetting env. episode 805.000000, reward total was -20.000000. running mean: -20.363064\n",
            "resetting env. episode 806.000000, reward total was -21.000000. running mean: -20.369433\n",
            "resetting env. episode 807.000000, reward total was -21.000000. running mean: -20.375739\n",
            "resetting env. episode 808.000000, reward total was -21.000000. running mean: -20.381981\n",
            "resetting env. episode 809.000000, reward total was -20.000000. running mean: -20.378162\n",
            "resetting env. episode 810.000000, reward total was -21.000000. running mean: -20.384380\n",
            "resetting env. episode 811.000000, reward total was -20.000000. running mean: -20.380536\n",
            "resetting env. episode 812.000000, reward total was -20.000000. running mean: -20.376731\n",
            "resetting env. episode 813.000000, reward total was -20.000000. running mean: -20.372964\n",
            "resetting env. episode 814.000000, reward total was -20.000000. running mean: -20.369234\n",
            "resetting env. episode 815.000000, reward total was -21.000000. running mean: -20.375542\n",
            "resetting env. episode 816.000000, reward total was -20.000000. running mean: -20.371786\n",
            "resetting env. episode 817.000000, reward total was -20.000000. running mean: -20.368068\n",
            "resetting env. episode 818.000000, reward total was -20.000000. running mean: -20.364388\n",
            "resetting env. episode 819.000000, reward total was -20.000000. running mean: -20.360744\n",
            "resetting env. episode 820.000000, reward total was -20.000000. running mean: -20.357136\n",
            "resetting env. episode 821.000000, reward total was -21.000000. running mean: -20.363565\n",
            "resetting env. episode 822.000000, reward total was -21.000000. running mean: -20.369929\n",
            "resetting env. episode 823.000000, reward total was -21.000000. running mean: -20.376230\n",
            "resetting env. episode 824.000000, reward total was -19.000000. running mean: -20.362468\n",
            "resetting env. episode 825.000000, reward total was -21.000000. running mean: -20.368843\n",
            "resetting env. episode 826.000000, reward total was -21.000000. running mean: -20.375155\n",
            "resetting env. episode 827.000000, reward total was -20.000000. running mean: -20.371403\n",
            "resetting env. episode 828.000000, reward total was -21.000000. running mean: -20.377689\n",
            "resetting env. episode 829.000000, reward total was -21.000000. running mean: -20.383912\n",
            "resetting env. episode 830.000000, reward total was -19.000000. running mean: -20.370073\n",
            "resetting env. episode 831.000000, reward total was -21.000000. running mean: -20.376372\n",
            "resetting env. episode 832.000000, reward total was -20.000000. running mean: -20.372609\n",
            "resetting env. episode 833.000000, reward total was -20.000000. running mean: -20.368882\n",
            "resetting env. episode 834.000000, reward total was -20.000000. running mean: -20.365194\n",
            "resetting env. episode 835.000000, reward total was -21.000000. running mean: -20.371542\n",
            "resetting env. episode 836.000000, reward total was -20.000000. running mean: -20.367826\n",
            "resetting env. episode 837.000000, reward total was -21.000000. running mean: -20.374148\n",
            "resetting env. episode 838.000000, reward total was -21.000000. running mean: -20.380407\n",
            "resetting env. episode 839.000000, reward total was -21.000000. running mean: -20.386602\n",
            "resetting env. episode 840.000000, reward total was -21.000000. running mean: -20.392736\n",
            "resetting env. episode 841.000000, reward total was -21.000000. running mean: -20.398809\n",
            "resetting env. episode 842.000000, reward total was -19.000000. running mean: -20.384821\n",
            "resetting env. episode 843.000000, reward total was -20.000000. running mean: -20.380973\n",
            "resetting env. episode 844.000000, reward total was -21.000000. running mean: -20.387163\n",
            "resetting env. episode 845.000000, reward total was -19.000000. running mean: -20.373291\n",
            "resetting env. episode 846.000000, reward total was -21.000000. running mean: -20.379558\n",
            "resetting env. episode 847.000000, reward total was -20.000000. running mean: -20.375763\n",
            "resetting env. episode 848.000000, reward total was -20.000000. running mean: -20.372005\n",
            "resetting env. episode 849.000000, reward total was -20.000000. running mean: -20.368285\n",
            "resetting env. episode 850.000000, reward total was -21.000000. running mean: -20.374602\n",
            "resetting env. episode 851.000000, reward total was -21.000000. running mean: -20.380856\n",
            "resetting env. episode 852.000000, reward total was -20.000000. running mean: -20.377048\n",
            "resetting env. episode 853.000000, reward total was -20.000000. running mean: -20.373277\n",
            "resetting env. episode 854.000000, reward total was -20.000000. running mean: -20.369545\n",
            "resetting env. episode 855.000000, reward total was -21.000000. running mean: -20.375849\n",
            "resetting env. episode 856.000000, reward total was -20.000000. running mean: -20.372091\n",
            "resetting env. episode 857.000000, reward total was -21.000000. running mean: -20.378370\n",
            "resetting env. episode 858.000000, reward total was -20.000000. running mean: -20.374586\n",
            "resetting env. episode 859.000000, reward total was -21.000000. running mean: -20.380840\n",
            "resetting env. episode 860.000000, reward total was -19.000000. running mean: -20.367032\n",
            "resetting env. episode 861.000000, reward total was -19.000000. running mean: -20.353361\n",
            "resetting env. episode 862.000000, reward total was -21.000000. running mean: -20.359828\n",
            "resetting env. episode 863.000000, reward total was -21.000000. running mean: -20.366230\n",
            "resetting env. episode 864.000000, reward total was -21.000000. running mean: -20.372567\n",
            "resetting env. episode 865.000000, reward total was -21.000000. running mean: -20.378842\n",
            "resetting env. episode 866.000000, reward total was -19.000000. running mean: -20.365053\n",
            "resetting env. episode 867.000000, reward total was -20.000000. running mean: -20.361403\n",
            "resetting env. episode 868.000000, reward total was -21.000000. running mean: -20.367789\n",
            "resetting env. episode 869.000000, reward total was -20.000000. running mean: -20.364111\n",
            "resetting env. episode 870.000000, reward total was -21.000000. running mean: -20.370470\n",
            "resetting env. episode 871.000000, reward total was -20.000000. running mean: -20.366765\n",
            "resetting env. episode 872.000000, reward total was -21.000000. running mean: -20.373097\n",
            "resetting env. episode 873.000000, reward total was -20.000000. running mean: -20.369366\n",
            "resetting env. episode 874.000000, reward total was -21.000000. running mean: -20.375673\n",
            "resetting env. episode 875.000000, reward total was -21.000000. running mean: -20.381916\n",
            "resetting env. episode 876.000000, reward total was -21.000000. running mean: -20.388097\n",
            "resetting env. episode 877.000000, reward total was -20.000000. running mean: -20.384216\n",
            "resetting env. episode 878.000000, reward total was -21.000000. running mean: -20.390374\n",
            "resetting env. episode 879.000000, reward total was -21.000000. running mean: -20.396470\n",
            "resetting env. episode 880.000000, reward total was -20.000000. running mean: -20.392505\n",
            "resetting env. episode 881.000000, reward total was -21.000000. running mean: -20.398580\n",
            "resetting env. episode 882.000000, reward total was -19.000000. running mean: -20.384594\n",
            "resetting env. episode 883.000000, reward total was -21.000000. running mean: -20.390748\n",
            "resetting env. episode 884.000000, reward total was -19.000000. running mean: -20.376841\n",
            "resetting env. episode 885.000000, reward total was -21.000000. running mean: -20.383072\n",
            "resetting env. episode 886.000000, reward total was -21.000000. running mean: -20.389242\n",
            "resetting env. episode 887.000000, reward total was -20.000000. running mean: -20.385349\n",
            "resetting env. episode 888.000000, reward total was -21.000000. running mean: -20.391496\n",
            "resetting env. episode 889.000000, reward total was -19.000000. running mean: -20.377581\n",
            "resetting env. episode 890.000000, reward total was -21.000000. running mean: -20.383805\n",
            "resetting env. episode 891.000000, reward total was -20.000000. running mean: -20.379967\n",
            "resetting env. episode 892.000000, reward total was -21.000000. running mean: -20.386167\n",
            "resetting env. episode 893.000000, reward total was -21.000000. running mean: -20.392306\n",
            "resetting env. episode 894.000000, reward total was -21.000000. running mean: -20.398383\n",
            "resetting env. episode 895.000000, reward total was -21.000000. running mean: -20.404399\n",
            "resetting env. episode 896.000000, reward total was -21.000000. running mean: -20.410355\n",
            "resetting env. episode 897.000000, reward total was -21.000000. running mean: -20.416251\n",
            "resetting env. episode 898.000000, reward total was -21.000000. running mean: -20.422089\n",
            "resetting env. episode 899.000000, reward total was -21.000000. running mean: -20.427868\n",
            "resetting env. episode 900.000000, reward total was -17.000000. running mean: -20.393589\n",
            "resetting env. episode 901.000000, reward total was -21.000000. running mean: -20.399653\n",
            "resetting env. episode 902.000000, reward total was -19.000000. running mean: -20.385657\n",
            "resetting env. episode 903.000000, reward total was -20.000000. running mean: -20.381800\n",
            "resetting env. episode 904.000000, reward total was -21.000000. running mean: -20.387982\n",
            "resetting env. episode 905.000000, reward total was -21.000000. running mean: -20.394102\n",
            "resetting env. episode 906.000000, reward total was -20.000000. running mean: -20.390161\n",
            "resetting env. episode 907.000000, reward total was -20.000000. running mean: -20.386260\n",
            "resetting env. episode 908.000000, reward total was -19.000000. running mean: -20.372397\n",
            "resetting env. episode 909.000000, reward total was -21.000000. running mean: -20.378673\n",
            "resetting env. episode 910.000000, reward total was -19.000000. running mean: -20.364886\n",
            "resetting env. episode 911.000000, reward total was -20.000000. running mean: -20.361238\n",
            "resetting env. episode 912.000000, reward total was -21.000000. running mean: -20.367625\n",
            "resetting env. episode 913.000000, reward total was -21.000000. running mean: -20.373949\n",
            "resetting env. episode 914.000000, reward total was -21.000000. running mean: -20.380209\n",
            "resetting env. episode 915.000000, reward total was -20.000000. running mean: -20.376407\n",
            "resetting env. episode 916.000000, reward total was -21.000000. running mean: -20.382643\n",
            "resetting env. episode 917.000000, reward total was -19.000000. running mean: -20.368817\n",
            "resetting env. episode 918.000000, reward total was -20.000000. running mean: -20.365129\n",
            "resetting env. episode 919.000000, reward total was -19.000000. running mean: -20.351477\n",
            "resetting env. episode 920.000000, reward total was -20.000000. running mean: -20.347963\n",
            "resetting env. episode 921.000000, reward total was -20.000000. running mean: -20.344483\n",
            "resetting env. episode 922.000000, reward total was -21.000000. running mean: -20.351038\n",
            "resetting env. episode 923.000000, reward total was -20.000000. running mean: -20.347528\n",
            "resetting env. episode 924.000000, reward total was -21.000000. running mean: -20.354053\n",
            "resetting env. episode 925.000000, reward total was -19.000000. running mean: -20.340512\n",
            "resetting env. episode 926.000000, reward total was -21.000000. running mean: -20.347107\n",
            "resetting env. episode 927.000000, reward total was -20.000000. running mean: -20.343636\n",
            "resetting env. episode 928.000000, reward total was -21.000000. running mean: -20.350199\n",
            "resetting env. episode 929.000000, reward total was -18.000000. running mean: -20.326697\n",
            "resetting env. episode 930.000000, reward total was -21.000000. running mean: -20.333430\n",
            "resetting env. episode 931.000000, reward total was -21.000000. running mean: -20.340096\n",
            "resetting env. episode 932.000000, reward total was -21.000000. running mean: -20.346695\n",
            "resetting env. episode 933.000000, reward total was -21.000000. running mean: -20.353228\n",
            "resetting env. episode 934.000000, reward total was -21.000000. running mean: -20.359696\n",
            "resetting env. episode 935.000000, reward total was -20.000000. running mean: -20.356099\n",
            "resetting env. episode 936.000000, reward total was -19.000000. running mean: -20.342538\n",
            "resetting env. episode 937.000000, reward total was -21.000000. running mean: -20.349113\n",
            "resetting env. episode 938.000000, reward total was -20.000000. running mean: -20.345622\n",
            "resetting env. episode 939.000000, reward total was -19.000000. running mean: -20.332165\n",
            "resetting env. episode 940.000000, reward total was -21.000000. running mean: -20.338844\n",
            "resetting env. episode 941.000000, reward total was -20.000000. running mean: -20.335455\n",
            "resetting env. episode 942.000000, reward total was -20.000000. running mean: -20.332101\n",
            "resetting env. episode 943.000000, reward total was -19.000000. running mean: -20.318780\n",
            "resetting env. episode 944.000000, reward total was -20.000000. running mean: -20.315592\n",
            "resetting env. episode 945.000000, reward total was -19.000000. running mean: -20.302436\n",
            "resetting env. episode 946.000000, reward total was -19.000000. running mean: -20.289412\n",
            "resetting env. episode 947.000000, reward total was -20.000000. running mean: -20.286517\n",
            "resetting env. episode 948.000000, reward total was -20.000000. running mean: -20.283652\n",
            "resetting env. episode 949.000000, reward total was -20.000000. running mean: -20.280816\n",
            "resetting env. episode 950.000000, reward total was -21.000000. running mean: -20.288008\n",
            "resetting env. episode 951.000000, reward total was -20.000000. running mean: -20.285128\n",
            "resetting env. episode 952.000000, reward total was -20.000000. running mean: -20.282276\n",
            "resetting env. episode 953.000000, reward total was -21.000000. running mean: -20.289453\n",
            "resetting env. episode 954.000000, reward total was -20.000000. running mean: -20.286559\n",
            "resetting env. episode 955.000000, reward total was -19.000000. running mean: -20.273693\n",
            "resetting env. episode 956.000000, reward total was -20.000000. running mean: -20.270956\n",
            "resetting env. episode 957.000000, reward total was -21.000000. running mean: -20.278247\n",
            "resetting env. episode 958.000000, reward total was -21.000000. running mean: -20.285464\n",
            "resetting env. episode 959.000000, reward total was -21.000000. running mean: -20.292610\n",
            "resetting env. episode 960.000000, reward total was -21.000000. running mean: -20.299684\n",
            "resetting env. episode 961.000000, reward total was -21.000000. running mean: -20.306687\n",
            "resetting env. episode 962.000000, reward total was -21.000000. running mean: -20.313620\n",
            "resetting env. episode 963.000000, reward total was -21.000000. running mean: -20.320484\n",
            "resetting env. episode 964.000000, reward total was -20.000000. running mean: -20.317279\n",
            "resetting env. episode 965.000000, reward total was -21.000000. running mean: -20.324106\n",
            "resetting env. episode 966.000000, reward total was -21.000000. running mean: -20.330865\n",
            "resetting env. episode 967.000000, reward total was -21.000000. running mean: -20.337556\n",
            "resetting env. episode 968.000000, reward total was -20.000000. running mean: -20.334181\n",
            "resetting env. episode 969.000000, reward total was -19.000000. running mean: -20.320839\n",
            "resetting env. episode 970.000000, reward total was -20.000000. running mean: -20.317631\n",
            "resetting env. episode 971.000000, reward total was -21.000000. running mean: -20.324454\n",
            "resetting env. episode 972.000000, reward total was -19.000000. running mean: -20.311210\n",
            "resetting env. episode 973.000000, reward total was -21.000000. running mean: -20.318098\n",
            "resetting env. episode 974.000000, reward total was -20.000000. running mean: -20.314917\n",
            "resetting env. episode 975.000000, reward total was -19.000000. running mean: -20.301768\n",
            "resetting env. episode 976.000000, reward total was -21.000000. running mean: -20.308750\n",
            "resetting env. episode 977.000000, reward total was -20.000000. running mean: -20.305662\n",
            "resetting env. episode 978.000000, reward total was -21.000000. running mean: -20.312606\n",
            "resetting env. episode 979.000000, reward total was -19.000000. running mean: -20.299480\n",
            "resetting env. episode 980.000000, reward total was -20.000000. running mean: -20.296485\n",
            "resetting env. episode 981.000000, reward total was -21.000000. running mean: -20.303520\n",
            "resetting env. episode 982.000000, reward total was -21.000000. running mean: -20.310485\n",
            "resetting env. episode 983.000000, reward total was -21.000000. running mean: -20.317380\n",
            "resetting env. episode 984.000000, reward total was -20.000000. running mean: -20.314206\n",
            "resetting env. episode 985.000000, reward total was -20.000000. running mean: -20.311064\n",
            "resetting env. episode 986.000000, reward total was -21.000000. running mean: -20.317954\n",
            "resetting env. episode 987.000000, reward total was -19.000000. running mean: -20.304774\n",
            "resetting env. episode 988.000000, reward total was -19.000000. running mean: -20.291726\n",
            "resetting env. episode 989.000000, reward total was -21.000000. running mean: -20.298809\n",
            "resetting env. episode 990.000000, reward total was -21.000000. running mean: -20.305821\n",
            "resetting env. episode 991.000000, reward total was -20.000000. running mean: -20.302763\n",
            "resetting env. episode 992.000000, reward total was -19.000000. running mean: -20.289735\n",
            "resetting env. episode 993.000000, reward total was -21.000000. running mean: -20.296838\n",
            "resetting env. episode 994.000000, reward total was -21.000000. running mean: -20.303869\n",
            "resetting env. episode 995.000000, reward total was -20.000000. running mean: -20.300831\n",
            "resetting env. episode 996.000000, reward total was -21.000000. running mean: -20.307822\n",
            "resetting env. episode 997.000000, reward total was -20.000000. running mean: -20.304744\n",
            "resetting env. episode 998.000000, reward total was -19.000000. running mean: -20.291697\n",
            "resetting env. episode 999.000000, reward total was -21.000000. running mean: -20.298780\n",
            "resetting env. episode 1000.000000, reward total was -21.000000. running mean: -20.305792\n",
            "resetting env. episode 1001.000000, reward total was -20.000000. running mean: -20.302734\n",
            "resetting env. episode 1002.000000, reward total was -20.000000. running mean: -20.299707\n",
            "resetting env. episode 1003.000000, reward total was -21.000000. running mean: -20.306710\n",
            "resetting env. episode 1004.000000, reward total was -18.000000. running mean: -20.283642\n",
            "resetting env. episode 1005.000000, reward total was -21.000000. running mean: -20.290806\n",
            "resetting env. episode 1006.000000, reward total was -19.000000. running mean: -20.277898\n",
            "resetting env. episode 1007.000000, reward total was -21.000000. running mean: -20.285119\n",
            "resetting env. episode 1008.000000, reward total was -18.000000. running mean: -20.262268\n",
            "resetting env. episode 1009.000000, reward total was -21.000000. running mean: -20.269645\n",
            "resetting env. episode 1010.000000, reward total was -20.000000. running mean: -20.266949\n",
            "resetting env. episode 1011.000000, reward total was -21.000000. running mean: -20.274279\n",
            "resetting env. episode 1012.000000, reward total was -21.000000. running mean: -20.281536\n",
            "resetting env. episode 1013.000000, reward total was -20.000000. running mean: -20.278721\n",
            "resetting env. episode 1014.000000, reward total was -21.000000. running mean: -20.285934\n",
            "resetting env. episode 1015.000000, reward total was -21.000000. running mean: -20.293074\n",
            "resetting env. episode 1016.000000, reward total was -20.000000. running mean: -20.290144\n",
            "resetting env. episode 1017.000000, reward total was -20.000000. running mean: -20.287242\n",
            "resetting env. episode 1018.000000, reward total was -21.000000. running mean: -20.294370\n",
            "resetting env. episode 1019.000000, reward total was -20.000000. running mean: -20.291426\n",
            "resetting env. episode 1020.000000, reward total was -21.000000. running mean: -20.298512\n",
            "resetting env. episode 1021.000000, reward total was -20.000000. running mean: -20.295527\n",
            "resetting env. episode 1022.000000, reward total was -21.000000. running mean: -20.302572\n",
            "resetting env. episode 1023.000000, reward total was -21.000000. running mean: -20.309546\n",
            "resetting env. episode 1024.000000, reward total was -21.000000. running mean: -20.316450\n",
            "resetting env. episode 1025.000000, reward total was -21.000000. running mean: -20.323286\n",
            "resetting env. episode 1026.000000, reward total was -21.000000. running mean: -20.330053\n",
            "resetting env. episode 1027.000000, reward total was -21.000000. running mean: -20.336752\n",
            "resetting env. episode 1028.000000, reward total was -20.000000. running mean: -20.333385\n",
            "resetting env. episode 1029.000000, reward total was -21.000000. running mean: -20.340051\n",
            "resetting env. episode 1030.000000, reward total was -20.000000. running mean: -20.336651\n",
            "resetting env. episode 1031.000000, reward total was -21.000000. running mean: -20.343284\n",
            "resetting env. episode 1032.000000, reward total was -21.000000. running mean: -20.349851\n",
            "resetting env. episode 1033.000000, reward total was -21.000000. running mean: -20.356353\n",
            "resetting env. episode 1034.000000, reward total was -20.000000. running mean: -20.352789\n",
            "resetting env. episode 1035.000000, reward total was -20.000000. running mean: -20.349261\n",
            "resetting env. episode 1036.000000, reward total was -20.000000. running mean: -20.345769\n",
            "resetting env. episode 1037.000000, reward total was -21.000000. running mean: -20.352311\n",
            "resetting env. episode 1038.000000, reward total was -19.000000. running mean: -20.338788\n",
            "resetting env. episode 1039.000000, reward total was -21.000000. running mean: -20.345400\n",
            "resetting env. episode 1040.000000, reward total was -21.000000. running mean: -20.351946\n",
            "resetting env. episode 1041.000000, reward total was -21.000000. running mean: -20.358427\n",
            "resetting env. episode 1042.000000, reward total was -20.000000. running mean: -20.354842\n",
            "resetting env. episode 1043.000000, reward total was -21.000000. running mean: -20.361294\n",
            "resetting env. episode 1044.000000, reward total was -20.000000. running mean: -20.357681\n",
            "resetting env. episode 1045.000000, reward total was -20.000000. running mean: -20.354104\n",
            "resetting env. episode 1046.000000, reward total was -19.000000. running mean: -20.340563\n",
            "resetting env. episode 1047.000000, reward total was -21.000000. running mean: -20.347157\n",
            "resetting env. episode 1048.000000, reward total was -20.000000. running mean: -20.343686\n",
            "resetting env. episode 1049.000000, reward total was -21.000000. running mean: -20.350249\n",
            "resetting env. episode 1050.000000, reward total was -21.000000. running mean: -20.356747\n",
            "resetting env. episode 1051.000000, reward total was -21.000000. running mean: -20.363179\n",
            "resetting env. episode 1052.000000, reward total was -20.000000. running mean: -20.359547\n",
            "resetting env. episode 1053.000000, reward total was -20.000000. running mean: -20.355952\n",
            "resetting env. episode 1054.000000, reward total was -21.000000. running mean: -20.362392\n",
            "resetting env. episode 1055.000000, reward total was -21.000000. running mean: -20.368768\n",
            "resetting env. episode 1056.000000, reward total was -20.000000. running mean: -20.365081\n",
            "resetting env. episode 1057.000000, reward total was -21.000000. running mean: -20.371430\n",
            "resetting env. episode 1058.000000, reward total was -19.000000. running mean: -20.357716\n",
            "resetting env. episode 1059.000000, reward total was -21.000000. running mean: -20.364138\n",
            "resetting env. episode 1060.000000, reward total was -21.000000. running mean: -20.370497\n",
            "resetting env. episode 1061.000000, reward total was -18.000000. running mean: -20.346792\n",
            "resetting env. episode 1062.000000, reward total was -21.000000. running mean: -20.353324\n",
            "resetting env. episode 1063.000000, reward total was -20.000000. running mean: -20.349791\n",
            "resetting env. episode 1064.000000, reward total was -19.000000. running mean: -20.336293\n",
            "resetting env. episode 1065.000000, reward total was -21.000000. running mean: -20.342930\n",
            "resetting env. episode 1066.000000, reward total was -20.000000. running mean: -20.339501\n",
            "resetting env. episode 1067.000000, reward total was -20.000000. running mean: -20.336106\n",
            "resetting env. episode 1068.000000, reward total was -20.000000. running mean: -20.332745\n",
            "resetting env. episode 1069.000000, reward total was -20.000000. running mean: -20.329417\n",
            "resetting env. episode 1070.000000, reward total was -21.000000. running mean: -20.336123\n",
            "resetting env. episode 1071.000000, reward total was -21.000000. running mean: -20.342762\n",
            "resetting env. episode 1072.000000, reward total was -21.000000. running mean: -20.349334\n",
            "resetting env. episode 1073.000000, reward total was -21.000000. running mean: -20.355841\n",
            "resetting env. episode 1074.000000, reward total was -21.000000. running mean: -20.362282\n",
            "resetting env. episode 1075.000000, reward total was -21.000000. running mean: -20.368660\n",
            "resetting env. episode 1076.000000, reward total was -20.000000. running mean: -20.364973\n",
            "resetting env. episode 1077.000000, reward total was -18.000000. running mean: -20.341323\n",
            "resetting env. episode 1078.000000, reward total was -21.000000. running mean: -20.347910\n",
            "resetting env. episode 1079.000000, reward total was -20.000000. running mean: -20.344431\n",
            "resetting env. episode 1080.000000, reward total was -21.000000. running mean: -20.350987\n",
            "resetting env. episode 1081.000000, reward total was -21.000000. running mean: -20.357477\n",
            "resetting env. episode 1082.000000, reward total was -21.000000. running mean: -20.363902\n",
            "resetting env. episode 1083.000000, reward total was -21.000000. running mean: -20.370263\n",
            "resetting env. episode 1084.000000, reward total was -21.000000. running mean: -20.376560\n",
            "resetting env. episode 1085.000000, reward total was -20.000000. running mean: -20.372795\n",
            "resetting env. episode 1086.000000, reward total was -21.000000. running mean: -20.379067\n",
            "resetting env. episode 1087.000000, reward total was -20.000000. running mean: -20.375276\n",
            "resetting env. episode 1088.000000, reward total was -21.000000. running mean: -20.381523\n",
            "resetting env. episode 1089.000000, reward total was -20.000000. running mean: -20.377708\n",
            "resetting env. episode 1090.000000, reward total was -20.000000. running mean: -20.373931\n",
            "resetting env. episode 1091.000000, reward total was -21.000000. running mean: -20.380192\n",
            "resetting env. episode 1092.000000, reward total was -20.000000. running mean: -20.376390\n",
            "resetting env. episode 1093.000000, reward total was -20.000000. running mean: -20.372626\n",
            "resetting env. episode 1094.000000, reward total was -21.000000. running mean: -20.378900\n",
            "resetting env. episode 1095.000000, reward total was -21.000000. running mean: -20.385111\n",
            "resetting env. episode 1096.000000, reward total was -20.000000. running mean: -20.381260\n",
            "resetting env. episode 1097.000000, reward total was -21.000000. running mean: -20.387447\n",
            "resetting env. episode 1098.000000, reward total was -19.000000. running mean: -20.373573\n",
            "resetting env. episode 1099.000000, reward total was -21.000000. running mean: -20.379837\n",
            "resetting env. episode 1100.000000, reward total was -21.000000. running mean: -20.386038\n",
            "resetting env. episode 1101.000000, reward total was -21.000000. running mean: -20.392178\n",
            "resetting env. episode 1102.000000, reward total was -20.000000. running mean: -20.388256\n",
            "resetting env. episode 1103.000000, reward total was -21.000000. running mean: -20.394374\n",
            "resetting env. episode 1104.000000, reward total was -18.000000. running mean: -20.370430\n",
            "resetting env. episode 1105.000000, reward total was -21.000000. running mean: -20.376726\n",
            "resetting env. episode 1106.000000, reward total was -21.000000. running mean: -20.382958\n",
            "resetting env. episode 1107.000000, reward total was -21.000000. running mean: -20.389129\n",
            "resetting env. episode 1108.000000, reward total was -20.000000. running mean: -20.385238\n",
            "resetting env. episode 1109.000000, reward total was -21.000000. running mean: -20.391385\n",
            "resetting env. episode 1110.000000, reward total was -20.000000. running mean: -20.387471\n",
            "resetting env. episode 1111.000000, reward total was -19.000000. running mean: -20.373597\n",
            "resetting env. episode 1112.000000, reward total was -21.000000. running mean: -20.379861\n",
            "resetting env. episode 1113.000000, reward total was -18.000000. running mean: -20.356062\n",
            "resetting env. episode 1114.000000, reward total was -19.000000. running mean: -20.342501\n",
            "resetting env. episode 1115.000000, reward total was -20.000000. running mean: -20.339076\n",
            "resetting env. episode 1116.000000, reward total was -20.000000. running mean: -20.335686\n",
            "resetting env. episode 1117.000000, reward total was -21.000000. running mean: -20.342329\n",
            "resetting env. episode 1118.000000, reward total was -20.000000. running mean: -20.338905\n",
            "resetting env. episode 1119.000000, reward total was -21.000000. running mean: -20.345516\n",
            "resetting env. episode 1120.000000, reward total was -21.000000. running mean: -20.352061\n",
            "resetting env. episode 1121.000000, reward total was -19.000000. running mean: -20.338541\n",
            "resetting env. episode 1122.000000, reward total was -18.000000. running mean: -20.315155\n",
            "resetting env. episode 1123.000000, reward total was -20.000000. running mean: -20.312004\n",
            "resetting env. episode 1124.000000, reward total was -21.000000. running mean: -20.318884\n",
            "resetting env. episode 1125.000000, reward total was -21.000000. running mean: -20.325695\n",
            "resetting env. episode 1126.000000, reward total was -21.000000. running mean: -20.332438\n",
            "resetting env. episode 1127.000000, reward total was -21.000000. running mean: -20.339114\n",
            "resetting env. episode 1128.000000, reward total was -20.000000. running mean: -20.335722\n",
            "resetting env. episode 1129.000000, reward total was -21.000000. running mean: -20.342365\n",
            "resetting env. episode 1130.000000, reward total was -18.000000. running mean: -20.318941\n",
            "resetting env. episode 1131.000000, reward total was -21.000000. running mean: -20.325752\n",
            "resetting env. episode 1132.000000, reward total was -20.000000. running mean: -20.322495\n",
            "resetting env. episode 1133.000000, reward total was -21.000000. running mean: -20.329270\n",
            "resetting env. episode 1134.000000, reward total was -21.000000. running mean: -20.335977\n",
            "resetting env. episode 1135.000000, reward total was -20.000000. running mean: -20.332617\n",
            "resetting env. episode 1136.000000, reward total was -18.000000. running mean: -20.309291\n",
            "resetting env. episode 1137.000000, reward total was -21.000000. running mean: -20.316198\n",
            "resetting env. episode 1138.000000, reward total was -20.000000. running mean: -20.313036\n",
            "resetting env. episode 1139.000000, reward total was -21.000000. running mean: -20.319906\n",
            "resetting env. episode 1140.000000, reward total was -19.000000. running mean: -20.306707\n",
            "resetting env. episode 1141.000000, reward total was -20.000000. running mean: -20.303640\n",
            "resetting env. episode 1142.000000, reward total was -19.000000. running mean: -20.290603\n",
            "resetting env. episode 1143.000000, reward total was -19.000000. running mean: -20.277697\n",
            "resetting env. episode 1144.000000, reward total was -20.000000. running mean: -20.274920\n",
            "resetting env. episode 1145.000000, reward total was -20.000000. running mean: -20.272171\n",
            "resetting env. episode 1146.000000, reward total was -20.000000. running mean: -20.269449\n",
            "resetting env. episode 1147.000000, reward total was -19.000000. running mean: -20.256755\n",
            "resetting env. episode 1148.000000, reward total was -20.000000. running mean: -20.254187\n",
            "resetting env. episode 1149.000000, reward total was -21.000000. running mean: -20.261645\n",
            "resetting env. episode 1150.000000, reward total was -20.000000. running mean: -20.259029\n",
            "resetting env. episode 1151.000000, reward total was -20.000000. running mean: -20.256439\n",
            "resetting env. episode 1152.000000, reward total was -20.000000. running mean: -20.253874\n",
            "resetting env. episode 1153.000000, reward total was -21.000000. running mean: -20.261336\n",
            "resetting env. episode 1154.000000, reward total was -21.000000. running mean: -20.268722\n",
            "resetting env. episode 1155.000000, reward total was -20.000000. running mean: -20.266035\n",
            "resetting env. episode 1156.000000, reward total was -21.000000. running mean: -20.273375\n",
            "resetting env. episode 1157.000000, reward total was -18.000000. running mean: -20.250641\n",
            "resetting env. episode 1158.000000, reward total was -21.000000. running mean: -20.258134\n",
            "resetting env. episode 1159.000000, reward total was -19.000000. running mean: -20.245553\n",
            "resetting env. episode 1160.000000, reward total was -21.000000. running mean: -20.253098\n",
            "resetting env. episode 1161.000000, reward total was -19.000000. running mean: -20.240567\n",
            "resetting env. episode 1162.000000, reward total was -21.000000. running mean: -20.248161\n",
            "resetting env. episode 1163.000000, reward total was -20.000000. running mean: -20.245679\n",
            "resetting env. episode 1164.000000, reward total was -21.000000. running mean: -20.253223\n",
            "resetting env. episode 1165.000000, reward total was -17.000000. running mean: -20.220690\n",
            "resetting env. episode 1166.000000, reward total was -20.000000. running mean: -20.218483\n",
            "resetting env. episode 1167.000000, reward total was -21.000000. running mean: -20.226299\n",
            "resetting env. episode 1168.000000, reward total was -21.000000. running mean: -20.234036\n",
            "resetting env. episode 1169.000000, reward total was -20.000000. running mean: -20.231695\n",
            "resetting env. episode 1170.000000, reward total was -20.000000. running mean: -20.229378\n",
            "resetting env. episode 1171.000000, reward total was -21.000000. running mean: -20.237084\n",
            "resetting env. episode 1172.000000, reward total was -19.000000. running mean: -20.224714\n",
            "resetting env. episode 1173.000000, reward total was -20.000000. running mean: -20.222466\n",
            "resetting env. episode 1174.000000, reward total was -20.000000. running mean: -20.220242\n",
            "resetting env. episode 1175.000000, reward total was -20.000000. running mean: -20.218039\n",
            "resetting env. episode 1176.000000, reward total was -21.000000. running mean: -20.225859\n",
            "resetting env. episode 1177.000000, reward total was -21.000000. running mean: -20.233600\n",
            "resetting env. episode 1178.000000, reward total was -21.000000. running mean: -20.241264\n",
            "resetting env. episode 1179.000000, reward total was -20.000000. running mean: -20.238852\n",
            "resetting env. episode 1180.000000, reward total was -21.000000. running mean: -20.246463\n",
            "resetting env. episode 1181.000000, reward total was -21.000000. running mean: -20.253999\n",
            "resetting env. episode 1182.000000, reward total was -19.000000. running mean: -20.241459\n",
            "resetting env. episode 1183.000000, reward total was -20.000000. running mean: -20.239044\n",
            "resetting env. episode 1184.000000, reward total was -19.000000. running mean: -20.226654\n",
            "resetting env. episode 1185.000000, reward total was -19.000000. running mean: -20.214387\n",
            "resetting env. episode 1186.000000, reward total was -20.000000. running mean: -20.212243\n",
            "resetting env. episode 1187.000000, reward total was -19.000000. running mean: -20.200121\n",
            "resetting env. episode 1188.000000, reward total was -21.000000. running mean: -20.208120\n",
            "resetting env. episode 1189.000000, reward total was -21.000000. running mean: -20.216038\n",
            "resetting env. episode 1190.000000, reward total was -19.000000. running mean: -20.203878\n",
            "resetting env. episode 1191.000000, reward total was -19.000000. running mean: -20.191839\n",
            "resetting env. episode 1192.000000, reward total was -20.000000. running mean: -20.189921\n",
            "resetting env. episode 1193.000000, reward total was -17.000000. running mean: -20.158022\n",
            "resetting env. episode 1194.000000, reward total was -20.000000. running mean: -20.156441\n",
            "resetting env. episode 1195.000000, reward total was -21.000000. running mean: -20.164877\n",
            "resetting env. episode 1196.000000, reward total was -21.000000. running mean: -20.173228\n",
            "resetting env. episode 1197.000000, reward total was -21.000000. running mean: -20.181496\n",
            "resetting env. episode 1198.000000, reward total was -20.000000. running mean: -20.179681\n",
            "resetting env. episode 1199.000000, reward total was -21.000000. running mean: -20.187884\n",
            "resetting env. episode 1200.000000, reward total was -21.000000. running mean: -20.196005\n",
            "resetting env. episode 1201.000000, reward total was -19.000000. running mean: -20.184045\n",
            "resetting env. episode 1202.000000, reward total was -20.000000. running mean: -20.182205\n",
            "resetting env. episode 1203.000000, reward total was -21.000000. running mean: -20.190383\n",
            "resetting env. episode 1204.000000, reward total was -20.000000. running mean: -20.188479\n",
            "resetting env. episode 1205.000000, reward total was -19.000000. running mean: -20.176594\n",
            "resetting env. episode 1206.000000, reward total was -19.000000. running mean: -20.164828\n",
            "resetting env. episode 1207.000000, reward total was -21.000000. running mean: -20.173180\n",
            "resetting env. episode 1208.000000, reward total was -19.000000. running mean: -20.161448\n",
            "resetting env. episode 1209.000000, reward total was -20.000000. running mean: -20.159834\n",
            "resetting env. episode 1210.000000, reward total was -21.000000. running mean: -20.168235\n",
            "resetting env. episode 1211.000000, reward total was -19.000000. running mean: -20.156553\n",
            "resetting env. episode 1212.000000, reward total was -19.000000. running mean: -20.144987\n",
            "resetting env. episode 1213.000000, reward total was -21.000000. running mean: -20.153538\n",
            "resetting env. episode 1214.000000, reward total was -20.000000. running mean: -20.152002\n",
            "resetting env. episode 1215.000000, reward total was -19.000000. running mean: -20.140482\n",
            "resetting env. episode 1216.000000, reward total was -20.000000. running mean: -20.139077\n",
            "resetting env. episode 1217.000000, reward total was -21.000000. running mean: -20.147687\n",
            "resetting env. episode 1218.000000, reward total was -19.000000. running mean: -20.136210\n",
            "resetting env. episode 1219.000000, reward total was -19.000000. running mean: -20.124848\n",
            "resetting env. episode 1220.000000, reward total was -21.000000. running mean: -20.133599\n",
            "resetting env. episode 1221.000000, reward total was -20.000000. running mean: -20.132263\n",
            "resetting env. episode 1222.000000, reward total was -19.000000. running mean: -20.120940\n",
            "resetting env. episode 1223.000000, reward total was -21.000000. running mean: -20.129731\n",
            "resetting env. episode 1224.000000, reward total was -19.000000. running mean: -20.118434\n",
            "resetting env. episode 1225.000000, reward total was -20.000000. running mean: -20.117249\n",
            "resetting env. episode 1226.000000, reward total was -20.000000. running mean: -20.116077\n",
            "resetting env. episode 1227.000000, reward total was -20.000000. running mean: -20.114916\n",
            "resetting env. episode 1228.000000, reward total was -18.000000. running mean: -20.093767\n",
            "resetting env. episode 1229.000000, reward total was -21.000000. running mean: -20.102829\n",
            "resetting env. episode 1230.000000, reward total was -20.000000. running mean: -20.101801\n",
            "resetting env. episode 1231.000000, reward total was -18.000000. running mean: -20.080783\n",
            "resetting env. episode 1232.000000, reward total was -20.000000. running mean: -20.079975\n",
            "resetting env. episode 1233.000000, reward total was -19.000000. running mean: -20.069175\n",
            "resetting env. episode 1234.000000, reward total was -21.000000. running mean: -20.078484\n",
            "resetting env. episode 1235.000000, reward total was -21.000000. running mean: -20.087699\n",
            "resetting env. episode 1236.000000, reward total was -21.000000. running mean: -20.096822\n",
            "resetting env. episode 1237.000000, reward total was -20.000000. running mean: -20.095854\n",
            "resetting env. episode 1238.000000, reward total was -21.000000. running mean: -20.104895\n",
            "resetting env. episode 1239.000000, reward total was -21.000000. running mean: -20.113846\n",
            "resetting env. episode 1240.000000, reward total was -21.000000. running mean: -20.122708\n",
            "resetting env. episode 1241.000000, reward total was -20.000000. running mean: -20.121481\n",
            "resetting env. episode 1242.000000, reward total was -20.000000. running mean: -20.120266\n",
            "resetting env. episode 1243.000000, reward total was -19.000000. running mean: -20.109063\n",
            "resetting env. episode 1244.000000, reward total was -21.000000. running mean: -20.117973\n",
            "resetting env. episode 1245.000000, reward total was -21.000000. running mean: -20.126793\n",
            "resetting env. episode 1246.000000, reward total was -20.000000. running mean: -20.125525\n",
            "resetting env. episode 1247.000000, reward total was -21.000000. running mean: -20.134270\n",
            "resetting env. episode 1248.000000, reward total was -20.000000. running mean: -20.132927\n",
            "resetting env. episode 1249.000000, reward total was -20.000000. running mean: -20.131598\n",
            "resetting env. episode 1250.000000, reward total was -20.000000. running mean: -20.130282\n",
            "resetting env. episode 1251.000000, reward total was -20.000000. running mean: -20.128979\n",
            "resetting env. episode 1252.000000, reward total was -20.000000. running mean: -20.127689\n",
            "resetting env. episode 1253.000000, reward total was -20.000000. running mean: -20.126412\n",
            "resetting env. episode 1254.000000, reward total was -19.000000. running mean: -20.115148\n",
            "resetting env. episode 1255.000000, reward total was -21.000000. running mean: -20.123997\n",
            "resetting env. episode 1256.000000, reward total was -21.000000. running mean: -20.132757\n",
            "resetting env. episode 1257.000000, reward total was -20.000000. running mean: -20.131429\n",
            "resetting env. episode 1258.000000, reward total was -19.000000. running mean: -20.120115\n",
            "resetting env. episode 1259.000000, reward total was -20.000000. running mean: -20.118914\n",
            "resetting env. episode 1260.000000, reward total was -21.000000. running mean: -20.127724\n",
            "resetting env. episode 1261.000000, reward total was -19.000000. running mean: -20.116447\n",
            "resetting env. episode 1262.000000, reward total was -21.000000. running mean: -20.125283\n",
            "resetting env. episode 1263.000000, reward total was -19.000000. running mean: -20.114030\n",
            "resetting env. episode 1264.000000, reward total was -20.000000. running mean: -20.112890\n",
            "resetting env. episode 1265.000000, reward total was -20.000000. running mean: -20.111761\n",
            "resetting env. episode 1266.000000, reward total was -21.000000. running mean: -20.120643\n",
            "resetting env. episode 1267.000000, reward total was -21.000000. running mean: -20.129437\n",
            "resetting env. episode 1268.000000, reward total was -19.000000. running mean: -20.118142\n",
            "resetting env. episode 1269.000000, reward total was -20.000000. running mean: -20.116961\n",
            "resetting env. episode 1270.000000, reward total was -20.000000. running mean: -20.115791\n",
            "resetting env. episode 1271.000000, reward total was -20.000000. running mean: -20.114633\n",
            "resetting env. episode 1272.000000, reward total was -19.000000. running mean: -20.103487\n",
            "resetting env. episode 1273.000000, reward total was -21.000000. running mean: -20.112452\n",
            "resetting env. episode 1274.000000, reward total was -18.000000. running mean: -20.091328\n",
            "resetting env. episode 1275.000000, reward total was -20.000000. running mean: -20.090414\n",
            "resetting env. episode 1276.000000, reward total was -21.000000. running mean: -20.099510\n",
            "resetting env. episode 1277.000000, reward total was -21.000000. running mean: -20.108515\n",
            "resetting env. episode 1278.000000, reward total was -20.000000. running mean: -20.107430\n",
            "resetting env. episode 1279.000000, reward total was -20.000000. running mean: -20.106356\n",
            "resetting env. episode 1280.000000, reward total was -19.000000. running mean: -20.095292\n",
            "resetting env. episode 1281.000000, reward total was -21.000000. running mean: -20.104339\n",
            "resetting env. episode 1282.000000, reward total was -21.000000. running mean: -20.113296\n",
            "resetting env. episode 1283.000000, reward total was -18.000000. running mean: -20.092163\n",
            "resetting env. episode 1284.000000, reward total was -19.000000. running mean: -20.081241\n",
            "resetting env. episode 1285.000000, reward total was -21.000000. running mean: -20.090429\n",
            "resetting env. episode 1286.000000, reward total was -20.000000. running mean: -20.089525\n",
            "resetting env. episode 1287.000000, reward total was -21.000000. running mean: -20.098629\n",
            "resetting env. episode 1288.000000, reward total was -20.000000. running mean: -20.097643\n",
            "resetting env. episode 1289.000000, reward total was -20.000000. running mean: -20.096667\n",
            "resetting env. episode 1290.000000, reward total was -20.000000. running mean: -20.095700\n",
            "resetting env. episode 1291.000000, reward total was -20.000000. running mean: -20.094743\n",
            "resetting env. episode 1292.000000, reward total was -21.000000. running mean: -20.103795\n",
            "resetting env. episode 1293.000000, reward total was -21.000000. running mean: -20.112758\n",
            "resetting env. episode 1294.000000, reward total was -21.000000. running mean: -20.121630\n",
            "resetting env. episode 1295.000000, reward total was -20.000000. running mean: -20.120414\n",
            "resetting env. episode 1296.000000, reward total was -21.000000. running mean: -20.129210\n",
            "resetting env. episode 1297.000000, reward total was -19.000000. running mean: -20.117917\n",
            "resetting env. episode 1298.000000, reward total was -19.000000. running mean: -20.106738\n",
            "resetting env. episode 1299.000000, reward total was -21.000000. running mean: -20.115671\n",
            "resetting env. episode 1300.000000, reward total was -20.000000. running mean: -20.114514\n",
            "resetting env. episode 1301.000000, reward total was -21.000000. running mean: -20.123369\n",
            "resetting env. episode 1302.000000, reward total was -21.000000. running mean: -20.132135\n",
            "resetting env. episode 1303.000000, reward total was -17.000000. running mean: -20.100814\n",
            "resetting env. episode 1304.000000, reward total was -19.000000. running mean: -20.089806\n",
            "resetting env. episode 1305.000000, reward total was -19.000000. running mean: -20.078908\n",
            "resetting env. episode 1306.000000, reward total was -19.000000. running mean: -20.068119\n",
            "resetting env. episode 1307.000000, reward total was -21.000000. running mean: -20.077437\n",
            "resetting env. episode 1308.000000, reward total was -19.000000. running mean: -20.066663\n",
            "resetting env. episode 1309.000000, reward total was -19.000000. running mean: -20.055996\n",
            "resetting env. episode 1310.000000, reward total was -19.000000. running mean: -20.045437\n",
            "resetting env. episode 1311.000000, reward total was -21.000000. running mean: -20.054982\n",
            "resetting env. episode 1312.000000, reward total was -18.000000. running mean: -20.034432\n",
            "resetting env. episode 1313.000000, reward total was -21.000000. running mean: -20.044088\n",
            "resetting env. episode 1314.000000, reward total was -21.000000. running mean: -20.053647\n",
            "resetting env. episode 1315.000000, reward total was -21.000000. running mean: -20.063111\n",
            "resetting env. episode 1316.000000, reward total was -20.000000. running mean: -20.062480\n",
            "resetting env. episode 1317.000000, reward total was -21.000000. running mean: -20.071855\n",
            "resetting env. episode 1318.000000, reward total was -21.000000. running mean: -20.081136\n",
            "resetting env. episode 1319.000000, reward total was -20.000000. running mean: -20.080325\n",
            "resetting env. episode 1320.000000, reward total was -21.000000. running mean: -20.089522\n",
            "resetting env. episode 1321.000000, reward total was -20.000000. running mean: -20.088626\n",
            "resetting env. episode 1322.000000, reward total was -21.000000. running mean: -20.097740\n",
            "resetting env. episode 1323.000000, reward total was -20.000000. running mean: -20.096763\n",
            "resetting env. episode 1324.000000, reward total was -21.000000. running mean: -20.105795\n",
            "resetting env. episode 1325.000000, reward total was -21.000000. running mean: -20.114737\n",
            "resetting env. episode 1326.000000, reward total was -20.000000. running mean: -20.113590\n",
            "resetting env. episode 1327.000000, reward total was -21.000000. running mean: -20.122454\n",
            "resetting env. episode 1328.000000, reward total was -20.000000. running mean: -20.121229\n",
            "resetting env. episode 1329.000000, reward total was -21.000000. running mean: -20.130017\n",
            "resetting env. episode 1330.000000, reward total was -21.000000. running mean: -20.138717\n",
            "resetting env. episode 1331.000000, reward total was -20.000000. running mean: -20.137330\n",
            "resetting env. episode 1332.000000, reward total was -21.000000. running mean: -20.145956\n",
            "resetting env. episode 1333.000000, reward total was -21.000000. running mean: -20.154497\n",
            "resetting env. episode 1334.000000, reward total was -20.000000. running mean: -20.152952\n",
            "resetting env. episode 1335.000000, reward total was -19.000000. running mean: -20.141422\n",
            "resetting env. episode 1336.000000, reward total was -21.000000. running mean: -20.150008\n",
            "resetting env. episode 1337.000000, reward total was -19.000000. running mean: -20.138508\n",
            "resetting env. episode 1338.000000, reward total was -20.000000. running mean: -20.137123\n",
            "resetting env. episode 1339.000000, reward total was -21.000000. running mean: -20.145752\n",
            "resetting env. episode 1340.000000, reward total was -20.000000. running mean: -20.144294\n",
            "resetting env. episode 1341.000000, reward total was -21.000000. running mean: -20.152851\n",
            "resetting env. episode 1342.000000, reward total was -20.000000. running mean: -20.151323\n",
            "resetting env. episode 1343.000000, reward total was -21.000000. running mean: -20.159810\n",
            "resetting env. episode 1344.000000, reward total was -19.000000. running mean: -20.148211\n",
            "resetting env. episode 1345.000000, reward total was -21.000000. running mean: -20.156729\n",
            "resetting env. episode 1346.000000, reward total was -21.000000. running mean: -20.165162\n",
            "resetting env. episode 1347.000000, reward total was -19.000000. running mean: -20.153510\n",
            "resetting env. episode 1348.000000, reward total was -21.000000. running mean: -20.161975\n",
            "resetting env. episode 1349.000000, reward total was -19.000000. running mean: -20.150356\n",
            "resetting env. episode 1350.000000, reward total was -21.000000. running mean: -20.158852\n",
            "resetting env. episode 1351.000000, reward total was -20.000000. running mean: -20.157263\n",
            "resetting env. episode 1352.000000, reward total was -21.000000. running mean: -20.165691\n",
            "resetting env. episode 1353.000000, reward total was -19.000000. running mean: -20.154034\n",
            "resetting env. episode 1354.000000, reward total was -21.000000. running mean: -20.162494\n",
            "resetting env. episode 1355.000000, reward total was -19.000000. running mean: -20.150869\n",
            "resetting env. episode 1356.000000, reward total was -21.000000. running mean: -20.159360\n",
            "resetting env. episode 1357.000000, reward total was -20.000000. running mean: -20.157766\n",
            "resetting env. episode 1358.000000, reward total was -20.000000. running mean: -20.156189\n",
            "resetting env. episode 1359.000000, reward total was -19.000000. running mean: -20.144627\n",
            "resetting env. episode 1360.000000, reward total was -20.000000. running mean: -20.143181\n",
            "resetting env. episode 1361.000000, reward total was -15.000000. running mean: -20.091749\n",
            "resetting env. episode 1362.000000, reward total was -20.000000. running mean: -20.090831\n",
            "resetting env. episode 1363.000000, reward total was -21.000000. running mean: -20.099923\n",
            "resetting env. episode 1364.000000, reward total was -19.000000. running mean: -20.088924\n",
            "resetting env. episode 1365.000000, reward total was -19.000000. running mean: -20.078034\n",
            "resetting env. episode 1366.000000, reward total was -20.000000. running mean: -20.077254\n",
            "resetting env. episode 1367.000000, reward total was -21.000000. running mean: -20.086482\n",
            "resetting env. episode 1368.000000, reward total was -19.000000. running mean: -20.075617\n",
            "resetting env. episode 1369.000000, reward total was -20.000000. running mean: -20.074861\n",
            "resetting env. episode 1370.000000, reward total was -18.000000. running mean: -20.054112\n",
            "resetting env. episode 1371.000000, reward total was -19.000000. running mean: -20.043571\n",
            "resetting env. episode 1372.000000, reward total was -21.000000. running mean: -20.053135\n",
            "resetting env. episode 1373.000000, reward total was -20.000000. running mean: -20.052604\n",
            "resetting env. episode 1374.000000, reward total was -19.000000. running mean: -20.042078\n",
            "resetting env. episode 1375.000000, reward total was -21.000000. running mean: -20.051657\n",
            "resetting env. episode 1376.000000, reward total was -21.000000. running mean: -20.061140\n",
            "resetting env. episode 1377.000000, reward total was -21.000000. running mean: -20.070529\n",
            "resetting env. episode 1378.000000, reward total was -19.000000. running mean: -20.059824\n",
            "resetting env. episode 1379.000000, reward total was -19.000000. running mean: -20.049226\n",
            "resetting env. episode 1380.000000, reward total was -21.000000. running mean: -20.058733\n",
            "resetting env. episode 1381.000000, reward total was -19.000000. running mean: -20.048146\n",
            "resetting env. episode 1382.000000, reward total was -21.000000. running mean: -20.057664\n",
            "resetting env. episode 1383.000000, reward total was -20.000000. running mean: -20.057088\n",
            "resetting env. episode 1384.000000, reward total was -21.000000. running mean: -20.066517\n",
            "resetting env. episode 1385.000000, reward total was -21.000000. running mean: -20.075852\n",
            "resetting env. episode 1386.000000, reward total was -21.000000. running mean: -20.085093\n",
            "resetting env. episode 1387.000000, reward total was -21.000000. running mean: -20.094242\n",
            "resetting env. episode 1388.000000, reward total was -21.000000. running mean: -20.103300\n",
            "resetting env. episode 1389.000000, reward total was -20.000000. running mean: -20.102267\n",
            "resetting env. episode 1390.000000, reward total was -21.000000. running mean: -20.111244\n",
            "resetting env. episode 1391.000000, reward total was -20.000000. running mean: -20.110132\n",
            "resetting env. episode 1392.000000, reward total was -18.000000. running mean: -20.089030\n",
            "resetting env. episode 1393.000000, reward total was -19.000000. running mean: -20.078140\n",
            "resetting env. episode 1394.000000, reward total was -20.000000. running mean: -20.077359\n",
            "resetting env. episode 1395.000000, reward total was -20.000000. running mean: -20.076585\n",
            "resetting env. episode 1396.000000, reward total was -21.000000. running mean: -20.085819\n",
            "resetting env. episode 1397.000000, reward total was -21.000000. running mean: -20.094961\n",
            "resetting env. episode 1398.000000, reward total was -20.000000. running mean: -20.094012\n",
            "resetting env. episode 1399.000000, reward total was -20.000000. running mean: -20.093071\n",
            "resetting env. episode 1400.000000, reward total was -20.000000. running mean: -20.092141\n",
            "resetting env. episode 1401.000000, reward total was -21.000000. running mean: -20.101219\n",
            "resetting env. episode 1402.000000, reward total was -21.000000. running mean: -20.110207\n",
            "resetting env. episode 1403.000000, reward total was -19.000000. running mean: -20.099105\n",
            "resetting env. episode 1404.000000, reward total was -19.000000. running mean: -20.088114\n",
            "resetting env. episode 1405.000000, reward total was -21.000000. running mean: -20.097233\n",
            "resetting env. episode 1406.000000, reward total was -21.000000. running mean: -20.106261\n",
            "resetting env. episode 1407.000000, reward total was -21.000000. running mean: -20.115198\n",
            "resetting env. episode 1408.000000, reward total was -21.000000. running mean: -20.124046\n",
            "resetting env. episode 1409.000000, reward total was -21.000000. running mean: -20.132805\n",
            "resetting env. episode 1410.000000, reward total was -20.000000. running mean: -20.131477\n",
            "resetting env. episode 1411.000000, reward total was -21.000000. running mean: -20.140163\n",
            "resetting env. episode 1412.000000, reward total was -21.000000. running mean: -20.148761\n",
            "resetting env. episode 1413.000000, reward total was -20.000000. running mean: -20.147273\n",
            "resetting env. episode 1414.000000, reward total was -21.000000. running mean: -20.155801\n",
            "resetting env. episode 1415.000000, reward total was -21.000000. running mean: -20.164243\n",
            "resetting env. episode 1416.000000, reward total was -21.000000. running mean: -20.172600\n",
            "resetting env. episode 1417.000000, reward total was -19.000000. running mean: -20.160874\n",
            "resetting env. episode 1418.000000, reward total was -21.000000. running mean: -20.169265\n",
            "resetting env. episode 1419.000000, reward total was -20.000000. running mean: -20.167573\n",
            "resetting env. episode 1420.000000, reward total was -21.000000. running mean: -20.175897\n",
            "resetting env. episode 1421.000000, reward total was -19.000000. running mean: -20.164138\n",
            "resetting env. episode 1422.000000, reward total was -21.000000. running mean: -20.172497\n",
            "resetting env. episode 1423.000000, reward total was -18.000000. running mean: -20.150772\n",
            "resetting env. episode 1424.000000, reward total was -21.000000. running mean: -20.159264\n",
            "resetting env. episode 1425.000000, reward total was -21.000000. running mean: -20.167671\n",
            "resetting env. episode 1426.000000, reward total was -19.000000. running mean: -20.155995\n",
            "resetting env. episode 1427.000000, reward total was -20.000000. running mean: -20.154435\n",
            "resetting env. episode 1428.000000, reward total was -18.000000. running mean: -20.132890\n",
            "resetting env. episode 1429.000000, reward total was -20.000000. running mean: -20.131562\n",
            "resetting env. episode 1430.000000, reward total was -20.000000. running mean: -20.130246\n",
            "resetting env. episode 1431.000000, reward total was -20.000000. running mean: -20.128943\n",
            "resetting env. episode 1432.000000, reward total was -19.000000. running mean: -20.117654\n",
            "resetting env. episode 1433.000000, reward total was -21.000000. running mean: -20.126477\n",
            "resetting env. episode 1434.000000, reward total was -20.000000. running mean: -20.125213\n",
            "resetting env. episode 1435.000000, reward total was -21.000000. running mean: -20.133961\n",
            "resetting env. episode 1436.000000, reward total was -21.000000. running mean: -20.142621\n",
            "resetting env. episode 1437.000000, reward total was -20.000000. running mean: -20.141195\n",
            "resetting env. episode 1438.000000, reward total was -19.000000. running mean: -20.129783\n",
            "resetting env. episode 1439.000000, reward total was -20.000000. running mean: -20.128485\n",
            "resetting env. episode 1440.000000, reward total was -20.000000. running mean: -20.127200\n",
            "resetting env. episode 1441.000000, reward total was -21.000000. running mean: -20.135928\n",
            "resetting env. episode 1442.000000, reward total was -20.000000. running mean: -20.134569\n",
            "resetting env. episode 1443.000000, reward total was -18.000000. running mean: -20.113223\n",
            "resetting env. episode 1444.000000, reward total was -19.000000. running mean: -20.102091\n",
            "resetting env. episode 1445.000000, reward total was -20.000000. running mean: -20.101070\n",
            "resetting env. episode 1446.000000, reward total was -21.000000. running mean: -20.110059\n",
            "resetting env. episode 1447.000000, reward total was -20.000000. running mean: -20.108959\n",
            "resetting env. episode 1448.000000, reward total was -19.000000. running mean: -20.097869\n",
            "resetting env. episode 1449.000000, reward total was -21.000000. running mean: -20.106890\n",
            "resetting env. episode 1450.000000, reward total was -20.000000. running mean: -20.105822\n",
            "resetting env. episode 1451.000000, reward total was -21.000000. running mean: -20.114763\n",
            "resetting env. episode 1452.000000, reward total was -21.000000. running mean: -20.123616\n",
            "resetting env. episode 1453.000000, reward total was -21.000000. running mean: -20.132380\n",
            "resetting env. episode 1454.000000, reward total was -18.000000. running mean: -20.111056\n",
            "resetting env. episode 1455.000000, reward total was -21.000000. running mean: -20.119945\n",
            "resetting env. episode 1456.000000, reward total was -20.000000. running mean: -20.118746\n",
            "resetting env. episode 1457.000000, reward total was -20.000000. running mean: -20.117558\n",
            "resetting env. episode 1458.000000, reward total was -20.000000. running mean: -20.116383\n",
            "resetting env. episode 1459.000000, reward total was -21.000000. running mean: -20.125219\n",
            "resetting env. episode 1460.000000, reward total was -21.000000. running mean: -20.133967\n",
            "resetting env. episode 1461.000000, reward total was -21.000000. running mean: -20.142627\n",
            "resetting env. episode 1462.000000, reward total was -20.000000. running mean: -20.141201\n",
            "resetting env. episode 1463.000000, reward total was -21.000000. running mean: -20.149789\n",
            "resetting env. episode 1464.000000, reward total was -19.000000. running mean: -20.138291\n",
            "resetting env. episode 1465.000000, reward total was -20.000000. running mean: -20.136908\n",
            "resetting env. episode 1466.000000, reward total was -21.000000. running mean: -20.145539\n",
            "resetting env. episode 1467.000000, reward total was -21.000000. running mean: -20.154083\n",
            "resetting env. episode 1468.000000, reward total was -20.000000. running mean: -20.152543\n",
            "resetting env. episode 1469.000000, reward total was -19.000000. running mean: -20.141017\n",
            "resetting env. episode 1470.000000, reward total was -21.000000. running mean: -20.149607\n",
            "resetting env. episode 1471.000000, reward total was -21.000000. running mean: -20.158111\n",
            "resetting env. episode 1472.000000, reward total was -20.000000. running mean: -20.156530\n",
            "resetting env. episode 1473.000000, reward total was -17.000000. running mean: -20.124965\n",
            "resetting env. episode 1474.000000, reward total was -19.000000. running mean: -20.113715\n",
            "resetting env. episode 1475.000000, reward total was -20.000000. running mean: -20.112578\n",
            "resetting env. episode 1476.000000, reward total was -20.000000. running mean: -20.111452\n",
            "resetting env. episode 1477.000000, reward total was -20.000000. running mean: -20.110337\n",
            "resetting env. episode 1478.000000, reward total was -21.000000. running mean: -20.119234\n",
            "resetting env. episode 1479.000000, reward total was -18.000000. running mean: -20.098042\n",
            "resetting env. episode 1480.000000, reward total was -21.000000. running mean: -20.107061\n",
            "resetting env. episode 1481.000000, reward total was -20.000000. running mean: -20.105991\n",
            "resetting env. episode 1482.000000, reward total was -21.000000. running mean: -20.114931\n",
            "resetting env. episode 1483.000000, reward total was -20.000000. running mean: -20.113781\n",
            "resetting env. episode 1484.000000, reward total was -20.000000. running mean: -20.112644\n",
            "resetting env. episode 1485.000000, reward total was -20.000000. running mean: -20.111517\n",
            "resetting env. episode 1486.000000, reward total was -21.000000. running mean: -20.120402\n",
            "resetting env. episode 1487.000000, reward total was -21.000000. running mean: -20.129198\n",
            "resetting env. episode 1488.000000, reward total was -16.000000. running mean: -20.087906\n",
            "resetting env. episode 1489.000000, reward total was -21.000000. running mean: -20.097027\n",
            "resetting env. episode 1490.000000, reward total was -21.000000. running mean: -20.106057\n",
            "resetting env. episode 1491.000000, reward total was -19.000000. running mean: -20.094996\n",
            "resetting env. episode 1492.000000, reward total was -21.000000. running mean: -20.104046\n",
            "resetting env. episode 1493.000000, reward total was -21.000000. running mean: -20.113006\n",
            "resetting env. episode 1494.000000, reward total was -20.000000. running mean: -20.111876\n",
            "resetting env. episode 1495.000000, reward total was -20.000000. running mean: -20.110757\n",
            "resetting env. episode 1496.000000, reward total was -20.000000. running mean: -20.109649\n",
            "resetting env. episode 1497.000000, reward total was -21.000000. running mean: -20.118553\n",
            "resetting env. episode 1498.000000, reward total was -20.000000. running mean: -20.117367\n",
            "resetting env. episode 1499.000000, reward total was -20.000000. running mean: -20.116194\n",
            "resetting env. episode 1500.000000, reward total was -20.000000. running mean: -20.115032\n",
            "CPU times: user 1h 31min 48s, sys: 40min 17s, total: 2h 12min 6s\n",
            "Wall time: 1h 8min 26s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "w2NblmwDsL3y",
        "outputId": "ce904f6e-7542-44d4-8122-52bf5ca2cd7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "cell_type": "code",
      "source": [
        "play_game(env, model)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -8.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}